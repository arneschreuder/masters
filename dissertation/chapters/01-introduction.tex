\chapter{Introduction}
\label{chap:introduction}

\begin{quote}
      \textit{The Cephalopod retreats and crawls back up against the glass of the fish tank
            evaluating the situation in front of him. In front of him is a glass jar with
            prey inside. The lid is shut tightly. Never before has he encountered such an
            obstacle in nature, prohibiting him from feeding. He curiously swims closer,
            feeling the shape of the jar with his tentacles. He wraps his tentacles over the
            jar and with a small turn and pop, he releases the lid. The experiment is
            repeated and without hesitation, without failure, he opens the jar, faster, more
            efficiently, every time.}
\end{quote}

\Ac{ML} is one of the most popular fields of research in \ac{AI} studies today.
In recent years, \ac{ML} research has seen some notable achievements in the
academia \cite{ref:lecun:2015, ref:glorot:2010, ref:goodfellow:2014,
      ref:quoc:2017}, as well as the industry at large \cite{ref:silver:2016,
      ref:silver:2017, ref:zoph:2017, ref:lewis:2017}.  \ac{ML} research has grown
tremendously over the past decade with successes like AlphaGo, which set new
standards for \ac{AI} capabilities by beating the world's best Go player, Lee
Sedol, 4-1 \cite{ref:san-hun:2016}.  Furthermore, an improvement on AlphaGo,
called AlphaZero, learned to play Go, \textit{tabula rasa} \footnote{Latin word
      meaning to learn from no prior knowledge.} and managed to beat AlphaGo, 100-1
\cite{ref:silver:2017}.

Historically, \ac{ML} models were built with a specific purpose in mind and
revolved around specialised applications. A notable development was that of Deep
Blue by IBM \cite{ref:keene:1996}, which managed to play chess at a grand master
level. Deep Blue was able to finish 2-4 against the greatest chess player at the
time, Gary Kasparov. Although Deep Blue lost the tournament, this was a major
breakthrough in the field of \ac{AI}. However, Deep Blue was only able to play
chess and could not be utilised for any other purposes \cite{ref:kelley:2010}.
Today, DeepMind's \ac{RL} algorithm is capable of playing all of the original
Atari games \cite{ref:mnih:2013}, but is still very limited to a small set of
problems that it can solve.

Over the past few years, modern hardware capabilities have improved to the
point where workloads in the field of \ac{ML} that were previously computationally
infeasible, are now possible. One such sub-field of \ac{ML} is \acp{ANN}. \acp{ANN} can generally be described as well-organised structures of mathematical computation and are inspired by the biological brain \cite{ref:engelbrecht:2007}. \acp{ANN} are the digital equivalent of how we currently understand the brain to work. \acp{ANN} can be trained, which is the equivalent of ``learnin'' from data. Learning gives rise to the ability to apply some form of decision making. This ability provides a wide-range of applications from healthcare to finance and yields great interest in the field. With the improvement of hardware came an influx of \ac{ML} researchers that focused
their attention on the training of \acp{ANN}.

A popular field of focus for studying \acp{ANN} is the process by which
these models are trained. \acp{FFNN} are specific types of
\acp{ANN} \cite{ref:reed:1999}. The most common way of training \acp{FFNN} is
\index{supervised training}supervised learning. Training of \acp{ANN} is seen as an optimisation problem. The \ac{ANN} maintains a set of parameters, referred to as ``weights'' and ``biases''. A search algorithm known as a \index{heuristic}\textit{heuristic}
\cite{ref:pearl:1984} is used to assign optimal values to the parameters of the \ac{ANN}, such that a specified objective function is minimised.

Although the landscape of what can be solved using \acp{ANN} today
is extensive, there still exists no single model that can be generalised to
solving multiple problem classes, across multiple problem domains. \acs{ANN} training algorithms mostly yield problem specific solutions. This means that a particular approach that works well for one domain of problem class, often does not necessarily work for another. This problem is known as the \ac{NFL} \cite{ref:wolpert:1997}.

Generalisation of \acp{ANN} refer to the capabilities of the network to either perform well on unseen data or to be applied to other problem domains. Various attempts have been made to improve the generalisation capabilities of
\acp{ANN}. Examples of techniques used include weight \index{dropout}dropout
\cite{ref:srivastava:2014}, \index{weight decay}weight decay
\cite{ref:krogh:1992}, \index{meta-learning}meta-learning of learning parameters
such as the \index{learning rate}learning rate and \index{momentum}momentum of
gradient-based heuristics \cite{ref:zeiler:2012, ref:lv:2017, ref:darken:1992},
and removing or postponing the use of outliers in the training data \cite{ref:reeves:1998}. Other techniques focus on improving
generalisation through model design. Design configurations focus on the layout of the neurons and structure of the \acs{ANN} (architecture) and how neurons are connected to interact (topology). Examples of such
techniques include learning of \ac{ANN} architectures through pruning techniques
\cite{ref:cibas:1996, ref:engelbrecht:1996} or constructive techniques
\cite{ref:hassibi:1994, ref:lecun:1990}, adaptive activation functions
\cite{ref:engelbrecht:1995, ref:fletcher:1994}, and quantum methods
\cite{ref:wan:2017, ref:ricks:2004}.

The performance and capabilities of \acp{ANN} is largely influenced by the learning process used. The learning process consists of multiple components. These include the type of underlying optimisation algorithm used, how the model parameters are initialised, the hyper-parameters used and the constraints of learning such as allowed search space and boundaries. Each of these elements influence how much a particular learning technique might focus on a particular solution (exploitation), in comparison to seeking out novel solutions (exploration) during training.

An example of dynamically balancing exploitation and exploration during the
search process is by dynamically adjusting and learning the heuristic
\index{hyper-parameters}\textit{hyper-parameters} as part of the learning
process. This field of study is known as \index{meta-learning}meta-learning
\cite{ref:giraud:2004}. \index{meta-learning}Meta-learning of heuristic
hyper-parameters as applied in the training of \acp{ANN} has shown to yield good
generalisation results \cite{ref:hospedales:2020, ref:vilalta:2002}.

This dynamic nature of the learning process leads towards the
idea that \ac{ML} models could yield better generalisation capabilities if the
learning process and learning mechanism applied are not statically defined, but
rather dynamic and under the control of some mechanism. Furthermore, a learning technique that has at its disposal a more diverse set
of learning behaviours from which it can dynamically select (when the need arises) could yield better generalisation capabilities \cite{ref:huang:2009}.

A more recent suggestion related to the field of \index{meta-learning}meta-learning is to dynamically select and/or adjust the heuristic used throughout the training process. This approach focuses on the hybridisation of learning paradigms. The main concept behind this paradigm is that the learning process is dynamic and differs from problem to problem. A particular learning technique might work well for one problem and not for another. At the same time, a particular learning technique might work well for a particular part of the search landscape, but not for another. By dynamically combining the best of different learning paradigms throughout the learning process, a trade-off can be made between exploration and exploitation as is required.

One such form of hybridisation of learning paradigms is referred to as \index{heterogeneous learning approaches}\textit{heterogeneous learning approaches}. Heterogeneous learning approaches make use of different search behaviours by selecting from a behaviour pool. Heterogeneous approaches have shown to balance the trade-off between exploration and exploitation \cite{ref:nepomuceno:2013}.

A step further in the concept of hybridisation of learning paradigms is that of hybridisation of different heuristics as they are applied to some optimisation problem \cite{ref:burke:2013}. These methods are referred to as \acfp{HH} and focus on finding the best heuristic in \textit{heuristic-space} to solve a specific problem. One such form of \ac{HH} is a
population-based approach that guides the search process by automatically
selecting heuristics from a heuristic-space to be applied to a collection of
different candidate solutions in the solution-space. This collection of
candidate solutions are referred to as a \textit{population} of
\textit{entities}, where each \textit{entity} is a single candidate solution to
the problem being optimised.

Population-based \acp{HH} implement the strategy of multiple heuristics working
together to solve a problem. However, finding the best heuristic to use is
non-trivial and some \textit{selection strategy} must be used to select the best
heuristic. \acp{HH} that implement such a selection strategy are referred to as
\textit{selection} \acp{HH}. The term \textit{selection} refers to the ability
of the \ac{HH} to select the best heuristic from a pool of low-level heuristics.

These selection \acp{HH} can be further categorised based on what information
they use in the selection strategy. One specific type of selection \ac{HH} is
called a \index{multi-method populated-based meta-heuristic}\textit{multi-method
      population-based \index{meta-heuristic}meta-heuristic}
\cite{ref:vanderstockt:2018}. The term \index{multi-method}\textit{multi-method}
refers to the incorporation of different low-level heuristics, with different
search behaviours, into the heuristic-space. The term \textit{population-based}
refers to the utilisation of a population of entities that represent candidate
solutions. Finally, the term \index{meta-heuristic}\textit{meta-heuristics}
refers to the \ac{HH} as a heuristic that does not have any domain knowledge and
only makes use of information from the search process.
\citeauthor{ref:grobler:2015} \cite{ref:grobler:2015} mentioned that \acp{HH}
have been shown to solve a number of problems including bin-packing, examination
timetabling, production scheduling, the traveling salesman problem, vehicle
routing problem and many more.

This dissertation focuses on the development of a \ac{HH} to train \acp{FFNN} in a
supervised learning approach.

\section{Problem Statement}
\label{sec:introduction:problem}

Many different heuristics have been developed and used to train \acp{FFNN}
\cite{ref:gudise:2003, ref:rakitianskaia:2012, ref:montana:1989}. Each of these heuristics have different search behaviours, characteristics, strengths and weaknesses. Finding the best heuristic to train the \ac{FFNN} is required in order to yield optimal results. This
process is often non-trivial and could be a time-consuming exercise.  Consider
that selection of the best heuristic as applied to optimisation problems, such
as training \acp{FFNN}, is very problem dependent \cite{ref:allen:1996,
      ref:drake:2020, ref:pillay:2018}.

Careful, systematic selection is thus required to find and select the best
heuristic to train \acp{FFNN}. In the past, researchers selected the best
heuristic by trail and error. A set of heuristics and carefully selected
hyper-parameters would be implemented, followed by an empirical test to evaluate
the performance of each heuristic for a given problem domain
\cite{ref:pillay:2015}. In this way, researchers were able to determine which
heuristics and which hyper-parameters performed well for different problems.
However, this approach is problematic, because it is time-consuming and tedious.


\section{Motivation}
\label{sec:introduction:motivation}

A process is required to alleviate the burden of having to exhaustively test
each implementation of heuristic and hyper-parameters, for every problem being
optimised.

A modern approach is to use \acp{HH} to automate the process of selecting the
best heuristic when applied to some optimisation problem. The best heuristic
might not be a single heuristic, but rather a hybridisation of heuristics
\cite{ref:pillay:2015}. Therefore, careful consideration is required to include and select from a diverse set of heuristics that is capable of solving the given problem. Importantly, only heuristics that are known to have the potential to solve the
problem are applicable and can be included in the set of heuristics to select from.


In the general context of optimisation, many different types of \acp{HH} have been implemented and applied to many
different problems. Some notable examples include the \index{simulated annealing}simulated annealing-based \ac{HH} by \citeauthor{ref:dowsland:2007} \cite{ref:dowsland:2007}, the \index{tabu-search}tabu-search \ac{HH} of \citeauthor{ref:burke:2010} \cite{ref:burke:2010}, the \index{heterogeneous meta-hyper-heuristic}heterogeneous meta-hyper-heuristic by \citeauthor{ref:grobler:2015} \cite{ref:grobler:2015} and work done by \citeauthor{ref:vanderstockt:2018} \cite{ref:vanderstockt:2018} on the analysis of selection hyper-heuristics for population-based metaheuristics in real-valued dynamic optimization. However, research on the application of \acp{HH} in the context of \acs{FFNN} training is scarce. \citeauthor{ref:nel:2021} \cite{ref:nel:2021} provides the first research in this field, applying a \acs{HH} to \acs{FFNN} training. The shortage of research in this field certainly warrants more exploration.

\acp{FFNN} training is seen as a computational search problem. \Acl{HH}
as defined by Burke et al. \cite{ref:burke:2010} is ``a search method or
learning mechanism for selecting or generating heuristics to solve computational
search problems''. \acp{HH} is a field of research aimed at the automated
selection, combination, adaptation or generation of multiple lower-level
heuristics to efficiently solve computational search problems. This implies that
it should be possible to apply \acp{HH} in the context \acs{FFNN} training.

\citeauthor{ref:grobler:2012} \cite{ref:grobler:2012} mention that the focus of
\acp{HH} is on automating the development of the \textit{learning mechanism}
used to find the best heuristic to obtain an appropriate solution to an optimisation problem. \citeauthor{ref:grobler:2012} explain that \acp{HH} employ a high-level heuristic that focuses on
finding the best low-level heuristic in heuristic-space that could include
\textit{single-method} and \textit{multi-method} optimisation algorithms
(heuristics). \acp{HH} relieve the burden of having to select the best
heuristic to use by trial and error. Furthermore, \acp{HH} can generally be applied to multiple
problems given that the set of low-level heuristics include
heuristics that have the potential to solve the problem at hand
\cite{ref:burke:2010}. Automation of the heuristic selection process and the general application to a wider range of problems are characteristics of \acp{HH} that can be beneficial when applied in the context of \acp{FFNN} training.

Note that training of \acp{FFNN} using \acp{HH} is not to be confused with the
training of \acp{FFNN} using \index{ensemble networks}\textit{ensemble networks}
or \index{query by committees}\textit{query by committees}.
\citeauthor{ref:pappa:2014} \cite{ref:pappa:2014} describe \index{ensemble
      networks}ensemble networks as a combination method in
\index{meta-learning}meta-learning whereby multiple \acp{ANN} are jointly used
to solve a problem. Each member network in the ensemble is trained using a
specified heuristic, and generally this heuristic is applied to the same member
throughout the training process. The results of all of the member networks are
then joined together using some consensus mechanism \cite{ref:zhou:2002}. This mechanism
could be weighted averages, majority voting or weighted voting. \index{ensemble
      networks}Ensemble networks do not search through the heuristic space to find the best training algorithm for each member, while \acp{HH} do.




This research takes a particular interest in developing a high-level heuristic
that makes use of probability theory and Bayesian statistical concepts to guide
the heuristic selection process. This research develops the novel \Acf{BHH}, a
new high-level heuristic that utilises a statistical approach, referred to as \index{Bayesian analysis}\textit{Bayesian
      analysis} which combines prior information with new evidence to the parameters of a selection probability distribution. This selection probability distribution is the mechanism by which the \ac{HH} selects appropriate
heuristics to train \acp{FFNN} during the training process (online learning).


\section{Objectives}
\label{sec:introduction:objectives}

The main objective of this research is focused on developing a novel \Ac{BHH}
selection mechanism in a \ac{HH} framework that can be used to train \acp{FFNN}.
In order to reach this objective, the following sub-objectives are defined:

\begin{itemize}
      \item
            Conduct a literature study on \acp{ANN} in order to provide
            the necessary background information of the \ac{AN}, \acp{FFNN} and the
            training process.

      \item
            Conduct a literature study on different types of heuristics that have been
            used to train \acp{FFNN}. The literature study will provide the necessary
            background information to understand how these different heuristics can
            be used in the set of heuristics to be selected to train the \ac{FFNN}.

      \item
            Conduct a literature study on \index{meta-learning}meta-learning and
            \acp{HH} in order to provide the required background information
            necessary to propose and develop a new high-level heuristic.

      \item
            Conduct a literature study on probability theory and Bayesian statistics
            such as Bayesian inference and Bayesian analysis to provide the necessary
            background information required to understand how Bayesian approaches can be
            used as a learning technique.

      \item
            Develop a novel \Ac{BHH} selection mechanism for a \ac{HH} that makes use
            of Bayesian statistics to guide the \ac{HH} search process while training
            \acp{FFNN} on different problems.

      \item
            Conduct an empirical study to show that the developed \Ac{BHH} can
            effectively be used to train \acp{FFNN}.

      \item Conduct an empirical study to investigate the behavioural characteristics of the \ac{BHH} as it is used to train \acp{FFNN} over a number of different datasets.

      \item
            Conduct an empirical study and critically evaluate the performance of the
            developed \Ac{BHH} compared to individual heuristics in the heuristic-space
            as they are used to train \acp{FFNN} on different problems.

      \item
            Conduct an empirical study that investigates variations of the \Ac{BHH} and the
            effects of design decisions and hyper-parameters on the search process.

      \item
            Provide a statistically sound analysis of the results obtained from the
            empirical study.
\end{itemize}


\section{Contributions}
\label{sec:introduction:contributions}

The results obtained from this research contribute to the field of study in the
following ways:

\begin{itemize}
      \item
            A novel heuristic selection operator is used that focuses on using Bayesian
            statistics to calculate the probability that a heuristic should be selected
            in order to efficiently train \acp{FFNN}. The resulting \ac{HH} is referred
            to as the \Acl{BHH}.

      \item
            The results of the empirical study show  with statistical significance and certainty that the \Ac{BHH} performs generally
            well on multiple problems. It is shown that, for each problem, the \Ac{BHH} performance is comparable to the best low-level heuristics (for the particular problem) included in the heuristic selection pool.

      \item
            The results of the empirical study show that the \Ac{BHH} is able to select
            the best heuristic to train \ac{FFNN} in general, with statistical
            certainty. This relieves researchers from the burden of having to do this
            selection process manually through trial and error.

      \item
            The results of the empirical study show that the \Ac{BHH}, given a diverse
            set of lower-level heuristics, will generally produce good results when
            applied to multiple problems at the same time.

      \item
            Finally, the results of the empirical study show that the \Ac{BHH} is
            capable of utilising \textit{a priori} \footnote{Latin word, meaning ``from
                  what comes before''.} knowledge in which a predefined selection bias is used
            for heuristics that are known to be well suited for certain problems.
\end{itemize}


\section{Dissertation Outline}
\label{sec:introduction:outline}

The remainder of this dissertation is structured as follows:

\begin{itemize}
      \item
            \textbf{Chapter~\ref{chap:anns}} provides a literature study on \acp{ANN}
            and the various components that make up an \acl{AN}. The focus is on the
            training of \acp{FFNN}. It is shown how the training of \acp{FFNN} is
            seen as an optimisation problem.

      \item
            \textbf{Chapter~\ref{chap:heuristics}} provides details on various types of
            heuristics and meta-heuristics that have been used to train \acp{FFNN}. A
            literature study is done to provide details on the search behaviours, application and
            implementation of each heuristic in the context of training \acp{FFNN}.

      \item
            \textbf{Chapter~\ref{chap:hhs}} presents a literature study on the details
            of \acp{HH} and \index{meta-learning}meta-learning in general. A
            discussion follows on the current landscape of \ac{HH} research and a review of different selection approaches for \acp{HH} is conducted. It is shown how \acp{HH} are suitable for \acs{FFNN} training.

      \item
            \textbf{Chapter~\ref{chap:probability}} presents a literature study on
            probability theory. Probability distributions and
            conjugate priors are discussed in detail. The chapter concludes with a
            detailed discussion on Bayesian statistics, specifically focusing on
            Bayesian inference and analysis.

      \item
            \textbf{Chapter~\ref{chap:bhh}} presents the developed \Ac{BHH}. It is shown
            how the \Ac{BHH} is implemented as a selection mechanism in the context of a
            \ac{HH} framework. The \Ac{BHH} is shown to implement a \index{Na\"ive Bayes
                  classifier}Na\"ive Bayes classifier. The probabilistic model that is
            implemented is derived and discussed in detail. Finally, the chapter
            concludes with a detailed discussion on how \index{Bayesian
                  analysis}Bayesian analysis is used to guide the heuristic selection process.
            The update step (training step) for prior probability concentration
            parameters is derived and discussed in detail. The \Ac{BHH} algorithmic
            implementation, variants and suggested application are presented as well.

      \item
            \textbf{Chapter~\ref{chap:methodology}} presents a detailed description of
            the empirical process and the setup of each experiment. It discusses the
            datasets used, the \ac{FFNN} architecture and topology used, heuristics that
            are used, the configuration of hyper-parameters, initialisation techniques
            used, performance measures used and other smaller details of the
            implementation of each experiment.  Finally, discussions follow on how the
            results are analysed. This includes a discussion on the method used to
            determine statistical significance.

      \item
            \textbf{Chapter~\ref{chap:results}} provides and discusses the results of
            the empirical study, in detail. A baseline comparison is done by comparing
            the performance of the \Ac{BHH} to that of all the individual lower-level
            heuristics on all the datasets. These datasets include classification and
            regression problems of various sizes and complexity.  Detailed results are
            presented on the performance of the \Ac{BHH} as a selection mechanism for
            \acp{HH} and a brief comparison is made to other selection mechanisms.
            Finally, results are presented on the effects of the hyper-parameters of the
            \Ac{BHH} on the training process. Discussions follow on how the \Ac{BHH} is
            shown to automate the selection of the best heuristic during the training of
            \acp{FFNN}, as applied to a range of problems, alleviating the burden on
            researchers to apply traditional trial and error approaches. The chapter is
            concluded with a brief argument in favor of the general applicability of the
            \Ac{BHH} to more problem domains.

      \item
            \textbf{Chapter~\ref{chap:conclusion}} summarises the research done in this
            dissertation along with a brief overview of the findings made throughout the research
            process. A review of the research goals are given and suggestions for future
            research are made.
\end{itemize}

This dissertation is accompanied by a full index given at page~\pageref{index} along with the following appendices:

\begin{itemize}
      \item
            \textbf{Appendix~\ref{app:datasets}} provides details on the datasets used
            for the empirical analysis.

      \item
            \textbf{Appendix~\ref{app:statistical_analysis}} provides details on the outcomes of the statistical analyses.

      \item
            \textbf{Appendix~\ref{app:acronyms}} provides a list of the important
            acronyms used or newly defined in the course of this work, as well as their
            associated definitions.

      \item
            \textbf{Appendix~\ref{app:derived_publications}} lists the publications
            derived from this work.

      \item
            \textbf{Appendix~\ref{app:symbols}} lists and defines the mathematical
            symbols used in this work, categorised according to the relevant chapter
            in which they appear.
\end{itemize}

To best view the illustrations, tables and figures presented throughout this
dissertation, it is recommended that the dissertation be viewed in colour.

