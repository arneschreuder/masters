\chapter{Introduction}\label{chap:introduction}

\begin{quotation}
      ``It is better to solve one problem five different ways, than to solve five problems one way.''
\end{quotation}
\begin{flushright}
      - George Polya
\end{flushright}

\Acf{ML} is one of the most popular fields of research in \acf{AI} studies today. In recent years, \acs{ML} research has seen some notable achievements in academia~\cite{ref:lecun:2015, ref:glorot:2010, ref:goodfellow:2014, ref:quoc:2017}, as well as the industry at large~\cite{ref:silver:2016, ref:silver:2017, ref:zoph:2017, ref:lewis:2017}.  \acs{ML} research has grown tremendously over the past decade with successes like AlphaGo, which set new standards for \acs{AI} capabilities by beating the world's best Go player, Lee Sedol, 4-1~\cite{ref:san-hun:2016}.

Over the past few years, modern hardware capabilities have improved to the point where workloads in the field of \acs{ML} that were previously computationally infeasible, are now possible. One such sub-field of \acs{ML} is \acp{ANN}. \acp{ANN} can generally be described as well-organised structures of mathematical computation and are inspired by the biological brain~\cite{ref:engelbrecht:2007}. \acp{ANN} can be trained, which is the equivalent of ``learning'' from data. Learning gives rise to the ability to apply some form of decision making. Decision making as an ability provides a wide-range of applications from healthcare to finance and yields great interest in the field. With the improvement of hardware came an influx of \acs{ML} researchers that focused their attention on training of \acp{ANN}.

\Acfp{FFNN} are specific types of \acp{ANN}~\cite{ref:reed:1999}. A popular field of focus for studying \acp{ANN} is the process by which these models are trained.  The most common way of training \acp{FFNN} is \index{supervised learning}\textit{supervised learning}, which is a training technique that involves exposing the \acp{ANN} with input data and comparing the produced output data to that of predefined target data. Training of \acp{ANN} is seen as an optimisation problem. The \acs{ANN} maintains a set of parameters, referred to as ``weights'' and ``biases''. A search algorithm known as a \index{heuristic}\textit{heuristic}~\cite{ref:pearl:1984} is used to assign optimal values to the parameters of the \acs{ANN}, such that a specified objective function is minimised. The research presented in this disseration focuses on the development of a specific type of heuristic, called a \acf{HH} to train \acp{FFNN} in a supervised learning approach.

This chapter provides the reader with a brief overview of the problem domain and outlines the research problem, objectives and purpose. The remainder of the chapter is presented as follows:

\begin{itemize}
      \item \textbf{Section~\ref{sec:introduction:summary_research_domain}} provides the reader with brief summary of the research domain.

      \item \textbf{Section~\ref{sec:introduction:problem}} outlines the research problem being addressed.

      \item \textbf{Section~\ref{sec:introduction:motivation}} provides a motivation for this research.

      \item \textbf{Section~\ref{sec:introduction:objectives}} presents the reader with the research objectives and outlines the goals of this dissertation.

      \item \textbf{Section~\ref{sec:introduction:contributions}} outlines the contributions of this research to the field.

      \item \textbf{Section~\ref{sec:introduction:outline}} provides a full dissertation outline.
\end{itemize}


\section{Summary of Research Domain}\label{sec:introduction:summary_research_domain}

This section provides the reader with a brief summary of the research field and outlines key concepts that contribute to the topics discussed in this disseration.

Although the landscape of what can be solved using \acp{ANN} today is extensive, there still exists no single model that can be generalised to solving multiple problem classes, across multiple problem domains. \acs{ANN} training algorithms mostly yield problem specific solutions. This means that a particular approach that works well for one domain or problem class, often does not necessarily work for another. This problem is known as the \acf{NFL}~\cite{ref:wolpert:1997}.

The performance and capabilities of \acp{ANN} is largely influenced by the learning process used. The learning process consists of multiple components. These include the type of underlying optimisation algorithm used, how the model parameters are initialised, the hyper-parameters used and the constraints of learning such as allowed search space and boundaries. Each of these elements influence how much a particular learning technique might focus on a particular solution (exploitation), in comparison to seeking out novel solutions (exploration) during training.

Techniques exist to dynamically balance the trade-off between exploration and exploitation during the search process. An example of such a technique is to dynamically adjust and learn the heuristic \index{hyper-parameters}\textit{hyper-parameters} as part of the learning process. This field of study is known as \index{meta-learning}meta-learning~\cite{ref:giraud:2004}. \index{meta-learning}Meta-learning of heuristic hyper-parameters as applied in the training of \acp{ANN} has shown to yield good generalisation results~\cite{ref:hospedales:2020, ref:vilalta:2002}.

The dynamic nature of the learning process leads towards the idea that \acs{ML} models could be trained in a way such that the learning process and learning mechanism applied are not statically defined, but rather dynamic and under the control of some mechanism.

A recent suggestion related to the field of \index{meta-learning}meta-learning is to dynamically select and/or adjust the heuristic used throughout the training process. This approach focuses on the hybridisation of learning paradigms. The main concept behind this paradigm is that the learning process is dynamic and problem specific. A particular learning technique might work well for one problem and not for another. At the same time, a particular learning technique might work well for a particular part of the search landscape, but not for another. By dynamically combining the best of different learning paradigms throughout the learning process, a trade-off can be made between exploration and exploitation as is required.

One such form of hybridisation of learning paradigms is referred to as \textit{heterogeneous learning approaches}. Heterogeneous learning approaches make use of different \textit{search behaviours} by selecting from a behaviour pool. Heterogeneous approaches have shown to balance the trade-off between exploration and exploitation~\cite{ref:nepomuceno:2013}.

A step further in the concept of hybridisation of learning paradigms is that of hybridisation of different \textit{heuristics} as they are applied to some optimisation problem~\cite{ref:burke:2013}. These methods are referred to as \acfp{HH} and focus on finding the best heuristic in \textit{heuristic space} to solve a specific problem. One such form of \acs{HH} is a population-based approach that guides the search process by automatically selecting heuristics from a heuristic-pool to be applied to a collection of different candidate solutions in the solution-space. This collection of candidate solutions are referred to as a \textit{population} of \textit{entities}, where each \textit{entity} is a single candidate solution to the problem being optimised. Population-based \acp{HH} implement a strategy where multiple heuristics work together to solve a problem. This technique requires a mechanism that selects a specific heuristic to be applied to a specific candidate solution.

Finding the best heuristic to use is non-trivial and some \textit{selection strategy} must be used to select the best heuristic. \acp{HH} that implement such a selection strategy are referred to as \textit{selection} \acp{HH}. The term \textit{selection} refers to the ability of the \acs{HH} to select the best heuristic from a pool of low-level heuristics.

One specific type of selection \acs{HH} is called a \index{multi-method populated-based meta-heuristic}\textit{multi-method population-based \index{meta-heuristic}meta-heuristic}~\cite{ref:vanderstockt:2018}. The term \index{multi-method}\textit{multi-method} refers to the incorporation of different low-level heuristics, with different search behaviours, into the heuristic space. The term \textit{population-based} refers to the utilisation of a population of entities that represent candidate solutions. Finally, the term \index{meta-heuristic}\textit{meta-heuristics} refers to the \acs{HH} as a heuristic that does not have any domain knowledge and only makes use of information from the search process.

\citeauthor{ref:grobler:2015}~\cite{ref:grobler:2015} mentioned that \acp{HH} have been shown to solve a number of problems including bin-packing, examination timetabling, production scheduling, the traveling salesman problem, vehicle routing problem and many more.

\section{Problem Statement}\label{sec:introduction:problem}

Many different heuristics have been developed and used to train \acp{FFNN}~\cite{ref:gudise:2003, ref:rakitianskaia:2012, ref:montana:1989}. Each of these heuristics have different search behaviours, characteristics, strengths and weaknesses. Finding the best heuristic to train the \acs{FFNN} is required in order to yield optimal results. This process is often non-trivial and could be a time-consuming exercise.  Consider that selection of the best heuristic as applied to optimisation problems, such as training \acp{FFNN}, is problem specific~\cite{ref:allen:1996, ref:drake:2020, ref:pillay:2018}.

Careful, systematic selection is thus required to find and select the best heuristic to train \acp{FFNN}. In the past, researchers selected the best heuristic by trail and error. A set of heuristics and carefully selected hyper-parameters would be implemented, followed by an empirical test to evaluate the performance of each heuristic for a given problem domain~\cite{ref:pillay:2015}. In this way, researchers were able to determine which heuristics and which hyper-parameters performed well for different problems. However, this approach is problematic, because it is time-consuming and laborious.

\section{Motivation}\label{sec:introduction:motivation}

A process is required to alleviate the burden of having to exhaustively test each implementation of heuristic and hyper-parameters, for every problem being optimised.

In Section~\ref{sec:introduction:problem} above, it is mentioned that finding the best heuristic to train \acp{FFNN} is a timely and tedious process. A modern approach is to use \acp{HH} to automate the process of selecting the best heuristic when applied to some optimisation problem. The best heuristic might not be a single heuristic, but rather a hybridisation of heuristics~\cite{ref:pillay:2015}.

In the general context of optimisation, many different types of \acp{HH} have been implemented and applied to many different problems. Some notable examples include the \index{simulated annealing}simulated annealing-based \acs{HH} by~\citeauthor{ref:dowsland:2007}~\cite{ref:dowsland:2007}, the \index{tabu-search}tabu-search \acs{HH} of~\citeauthor{ref:burke:2010}~\cite{ref:burke:2010}, the \index{heterogeneous meta-hyper-heuristic}heterogeneous meta-hyper-heuristic by~\citeauthor{ref:grobler:2012}~\cite{ref:grobler:2012} and work done by~\citeauthor{ref:vanderstockt:2018}~\cite{ref:vanderstockt:2018} on the analysis of selection hyper-heuristics for population-based \acp{MH} in real-valued dynamic optimization. However, research on the application of \acp{HH} in the context of \acs{FFNN} training is scarce.~\citeauthor{ref:nel:2021}~\cite{ref:nel:2021} provides the first research in this field, applying a \acs{HH} to \acs{FFNN} training.

Training of \acp{FFNN} is seen as a computational search problem. A \acf{HH}, as defined by Burke et al.~\cite{ref:burke:2010}, is ``a search method or learning mechanism for selecting or generating heuristics to solve computational search problems''. \acp{HH} is a field of research aimed at the automated selection, combination, adaptation or generation of multiple lower-level heuristics to efficiently solve computational search problems.

\citeauthor{ref:grobler:2012}~\cite{ref:grobler:2012} mention that the focus of \acp{HH} is on automating the development of the \textit{learning mechanism} used to find the best heuristic to obtain an appropriate solution to an optimisation problem.~\citeauthor{ref:grobler:2012} explain that \acp{HH} employ a high-level heuristic that focuses on finding the best low-level heuristic in heuristic space that could include \textit{single-method} and \textit{multi-method} optimisation algorithms (heuristics). \acp{HH} relieve the burden of having to select the best heuristic to use by trial and error. Furthermore, \acp{HH} can generally be applied to multiple problems given that the set of low-level heuristics include heuristics that have the potential to solve the problem at hand~\cite{ref:burke:2010}. Automation of the heuristic selection process and the general application to a wider range of problems are characteristics of \acp{HH} that can be beneficial when applied in the context of training \acp{FFNN}.

Note that training of \acp{FFNN} using \acp{HH} is not to be confused with the training of \acp{FFNN} using \index{ensemble networks}\textit{ensemble networks} or \index{query by committees}\textit{query by committees}.~\citeauthor{ref:pappa:2014}~\cite{ref:pappa:2014} describe \index{ensemble networks}ensemble networks as a combination method in \index{meta-learning}meta-learning whereby multiple \acp{ANN} are jointly used to solve a problem. Each member network in the ensemble is trained using a specified heuristic, and generally this heuristic is applied to the same member throughout the training process. The results of all of the member networks are then joined together using some consensus mechanism~\cite{ref:zhou:2002}. This mechanism could be weighted averages, majority voting or weighted voting. \index{ensemble networks}Ensemble networks do not search through the heuristic space to find the best training algorithm for each member, while \acp{HH} do.

This research takes a particular interest in developing a selection \acs{HH} that makes use of probability theory and Bayesian statistical concepts to guide the heuristic selection process. This research develops the novel \Acf{BHH}, a new high-level heuristic that utilises a statistical approach, referred to as \index{Bayesian analysis}\textit{Bayesian analysis}, which combines prior information with new evidence to the parameters of a selection probability distribution. This selection probability distribution is the mechanism by which the \acs{HH} selects appropriate heuristics to train \acp{FFNN} during the training process. This process takes place dynamically and during the training process (online learning).


\section{Objectives}\label{sec:introduction:objectives}

The main objective of this research is focused on developing a novel \Acf{BHH} selection mechanism in a \acs{HH} framework that can be used to train \acp{FFNN}. In order to reach this objective, the following sub-objectives are defined.

\begin{itemize}
      \item Conduct a literature study on \acp{ANN} in order to provide the necessary background information of the \acs{AN}, \acp{FFNN} and the training process.

      \item Conduct a literature study on different types of heuristics that have been used to train \acp{FFNN}. The literature study will provide the necessary background information to understand how different heuristics can be used in the set of heuristics to be selected to train the \acs{FFNN}.

      \item Conduct a literature study on \index{meta-learning}meta-learning and \acp{HH} in order to provide the required background information necessary to propose and develop a new high-level heuristic and selection mechanism.

      \item Conduct a literature study on probability theory and Bayesian statistics such as Bayesian inference and Bayesian analysis to provide the necessary background information required to understand how Bayesian approaches can be used as a learning technique.

      \item Develop a novel \Acs{BHH} selection mechanism for a \acs{HH} that makes use of Bayesian statistics to guide the \acs{HH} search process while training \acp{FFNN} on different problems.

      \item Conduct an empirical study to show that the developed \Acs{BHH} can effectively be used to train \acp{FFNN}.

      \item Conduct an empirical study to investigate the behavioural characteristics of the \Acs{BHH} as it is used to train \acp{FFNN} on an example problem.

      \item Conduct an empirical study and critically evaluate the performance of the developed \Acs{BHH} compared to individual heuristics in the heuristic space as they are used to train \acp{FFNN} on a number of different problems.

      \item Conduct an empirical study that investigates variations of the \Acs{BHH} and the effects of design decisions and hyper-parameters on the search process.

      \item Provide a statistical analysis of the results obtained from the empirical study.
\end{itemize}


\section{Contributions}\label{sec:introduction:contributions}

The results obtained from this research contribute to the field of study in the
following ways.

\begin{itemize}
      \item A novel heuristic selection operator is used that focuses on using Bayesian statistics to calculate the probability that a heuristic should be selected in order to efficiently train \acp{FFNN}. The resulting \acs{HH} is referred to as the \acf{BHH}.

      \item The results of the empirical study show with statistical significance and certainty that the \Acs{BHH} performs generally well on multiple problems. It is shown that, for each problem, the \Acs{BHH} performance is comparable to the best low-level heuristics included in the heuristic selection pool.

      \item The results of the empirical study show that the \Acs{BHH} is able to select the best heuristic to train \acp{FFNN} in general. This relieves researchers from the burden of having to do this selection process manually through trial and error.

      \item The results of the empirical study show that the \Acs{BHH}, given a diverse set of lower-level heuristics, will generally produce good results when applied to multiple problems at the same time.

      \item Finally, the results of the empirical study show that the \Acs{BHH} is capable of utilising \textit{a priori} \footnote{Latin word, meaning ``from what comes before''.} knowledge in which a predefined selection bias is used for heuristics that are known to be well suited for certain problems.
\end{itemize}


\section{Dissertation Outline}\label{sec:introduction:outline}

The remainder of this dissertation is structured as follows:

\begin{itemize}
      \item \textbf{Chapter~\ref{chap:anns}} provides a literature study on \acp{ANN} and the various components that make up an \acf{AN}. The focus is on training of \acp{FFNN}. It is shown how training of \acp{FFNN} is seen as an optimisation problem.

      \item \textbf{Chapter~\ref{chap:heuristics}} provides details on various types of heuristics and \acp{MH} that have been used to train \acp{FFNN}. A literature study is done to provide details on the search behaviours, application and implementation of each heuristic in the context of training \acp{FFNN}.

      \item \textbf{Chapter~\ref{chap:hhs}} presents a literature study on the details of \acp{HH} and \index{meta-learning}meta-learning in general. A discussion follows on the current landscape of \acs{HH} research and a review of different selection approaches for \acp{HH} is conducted. It is shown how \acp{HH} are suitable for \acs{FFNN} training.

      \item \textbf{Chapter~\ref{chap:probability}} presents a literature study on probability theory. Probability distributions and conjugate priors are discussed in detail. The chapter concludes with a detailed discussion on Bayesian statistics, specifically focusing on Bayesian inference and analysis.

      \item \textbf{Chapter~\ref{chap:bhh}} presents the developed \Acs{BHH}. It is shown how the \Acs{BHH} is implemented as a selection mechanism in the context of a \acs{HH} framework. The \Acs{BHH} is shown to implement a \index{Na\"ive Bayes classifier}Na\"ive Bayes classifier. The probabilistic model that is implemented is derived and discussed in detail. Finally, the chapter concludes with a detailed discussion on how \index{Bayesian analysis}Bayesian analysis is used to guide the heuristic selection process. The update step (training step) for prior probability concentration parameters is derived and discussed in detail. The \Acs{BHH} algorithmic implementation, variants and suggested application are presented as well.

      \item \textbf{Chapter~\ref{chap:methodology}} presents a detailed description of the empirical process and the setup of each experiment. It discusses the datasets used, the \acs{FFNN} architecture and topology used, heuristics that are used, the configuration of hyper-parameters, initialisation techniques used and performance measures used.  Finally, discussions follow on how the results are analysed. This includes a discussion on the method used to determine statistical significance.

      \item \textbf{Chapter~\ref{chap:results}} provides and discusses the results of the empirical study in detail. A baseline comparison is done by comparing the performance of the \Acs{BHH} to that of all the individual lower-level heuristics on all datasets. These datasets include classification and regression problems of various sizes and complexity.  Detailed results are presented on the performance of the \Acs{BHH} as a selection mechanism for \acp{HH} and a brief comparison is made to other selection mechanisms. Finally, results are presented on the effects of the hyper-parameters of the \Acs{BHH} on the training process. Discussions follow on how the \Acs{BHH} is shown to automate the selection of the best heuristic during the training of \acp{FFNN}, as applied to a range of problems, alleviating the burden on researchers to apply traditional trial and error approaches.

      \item \textbf{Chapter~\ref{chap:conclusion}} summarises the research done in this dissertation along with a brief overview of the findings made throughout the research process. A review of the research goals are given and suggestions for future research are made.
\end{itemize}

This dissertation is accompanied by a full index given at page~\pageref{index} along with the following appendices:

\begin{itemize}
      \item \textbf{Appendix~\ref{app:datasets}} provides details on the datasets used for the empirical analysis.

      \item \textbf{Appendix~\ref{app:statistical_analysis}} provides details on the outcomes of the statistical analyses.

      \item \textbf{Appendix~\ref{app:symbols}} lists and defines the mathematical symbols used in this work, categorised according to the relevant chapter in which they appear.

      \item \textbf{Appendix~\ref{app:acronyms}} provides a list of the important acronyms used or newly defined in the course of this work, as well as their associated definitions.

      \item \textbf{Appendix~\ref{app:derived_publications}} lists the publications derived from this work.
\end{itemize}

To best view the illustrations, tables and figures presented throughout this dissertation, it is recommended that the dissertation be viewed in colour.
