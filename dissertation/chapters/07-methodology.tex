\chapter{Methodology}\label{chap:methodology}

\begin{quotation}
      \noindent ``Observation, reason and experimentation make up what we call the scientific method.''
\end{quotation}
\begin{flushright}
      - Richard P. Feynman
\end{flushright}

\noindent
So far this dissertation has provided background information on \acfp{ANN}, \index{heuristic}heuristics, \acf{HH} and probability theory in Chapters \ref{chap:anns}-\ref{chap:probability}. Chapter \ref{chap:bhh} provided the detail of the implementation of the proposed \acs{BHH} with all its components and hyper-parameters. Scientific experimentation can now be established. Chapter \ref{chap:introduction} identified goals for this dissertation. The goals of this dissertation include an empirical evaluation of the \acs{BHH}, its behaviour and performance, and comparison to state of the art low-level heuristics. This chapter provides the detailed specification of the methodology that is followed for the empirical process. Details are provided on the implementation of experiments, datasets, selection of hyper-parameters and design decisions. The process by which the results are statistically analysed are also presented. The remainder of the chapter is structured as follows.

\begin{itemize}
      \item \textbf{Section \ref{sec:methodology:overview}} provides a brief overview of the overall empirical process.

      \item \textbf{Section \ref{sec:methodology:datasets}} presents the detail around the datasets that are used.

      \item \textbf{Section \ref{sec:methodology:model}} provides the details of the models (\acs{FFNN}s) that are trained.

      \item \textbf{Section \ref{sec:methodology:heuristics}} presents the detail around the different \index{heuristic}heuristics that are used along with their hyper-parameters.

      \item \textbf{Section \ref{sec:methodology:baseline_bhh}} provides the detail of the configuration of the \acs{BHH} baseline.

      \item \textbf{Section \ref{sec:methodology:performance_measures}} sheds light into the performance evaluation measures that are used.

      \item \textbf{Section \ref{sec:methodology:stopping_conditions}} discusses the stopping conditions that are used.

      \item \textbf{Section \ref{sec:methodology:experiments}} presents the different experimental groups that are executed.

      \item \textbf{Section \ref{sec:methodology:implementation}} sheds light on the implementation and execution of the empirical process.

      \item \textbf{Section \ref{sec:methodology:statistical_analysis}} presents the procedures that are followed for the statistical analysis of the results.

      \item \textbf{Section \ref{sec:methodology:summary}} presents a brief summary of the chapter.
\end{itemize}

\section{Overview of Empirical Process}\label{sec:methodology:overview}

The purpose of the empirical process is to conduct a carefully crafted set of experiments that produce data that can be used to reject of verify a hypothesis about the element under investigation. Chapter \ref{chap:introduction} identified a number of empirical tests to execute. These include:

\begin{itemize}
      \item An empirical study to show that the \Acs{BHH} can effectively be used to train \acp{FFNN}.

      \item An empirical study to investigate the behavioural characteristics of the \Acs{BHH} as it is used to train \acp{FFNN} on an example problem.

      \item An empirical study to critically evaluate the performance of the \Acs{BHH} compared to individual low-level heuristics in the heuristic space as they are used to train \acp{FFNN} on a number of different problems.
\end{itemize}

\noindent
These empirical tests represent a set of questions, related to the \acs{BHH}, that need to be answered. The empirical process is designed to answer these questions. The empirical process is structured as follows.

Each empirical tests starts with a question to be answered. A hypothesis is formulated for the outcome of the empirical test. The empirical tests are then designed around the implementation of the components under evaluation. Each empirical test defines the configuration of elements and generally include a set of parameters that are altered between experiments. Each experiment is evaluated by means of a performance measurement. In the context of training \acp{FFNN}, the underlying model is trained across a number of datasets. Each dataset is split into a training and test set. The training set is used to train the model, while the test set is used to evaluate the model's performance on unseen data. Training epochs are split into mini-batch iterations, with each dataset containing a specified mini-batch size. Evaluation takes place at every mini-batch step. Due to the stochastic nature of the experiments, each experiment is repeated over a number of runs to provide sufficient sample for statistical certainty about the outcome of the results. Each run contains a different random seed, such that each run is unique. The evaluation data forms the results of the empirical test. These results are analysed for statistical significance, from which findings and conclusions are then made. The null hypothesis is then either rejected or verified based on these findings.

Details around each of these elements are provided in the following sections and lead with discussions around the datasets that are used.

\section{Datasets}
\label{sec:methodology:datasets}

This section provides the detail around the different datasets that are used throughout the empirical process. These datasets originate from the UCI Machine Learning Repository~\cite{ref:uci:2022}. Datasets are grouped by problem type and include 8 classification and 7 regression datasets. The classification datasets are given in Table \ref{tab:methodology:datasets:classification} and the regression datasets are given in Table \ref{tab:methodology:datasets:regression} below.

\begin{table}[htbp]
      \centering
      \caption{Classification datasets}
      \label{tab:methodology:datasets:classification}%
      \par\bigskip
      \resizebox{\textwidth}{!}{
            \begin{tabular}{ccccccccc}
                  \textbf{dataset} & \textbf{output} & \textbf{types}             & \textbf{attributes} & \textbf{classes} & \textbf{instances} & \textbf{batch} & \textbf{steps} & \textbf{citation}          \\
                  \midrule
                  iris             & multivariate    & real                       & 4                   & 3                & 150                & 16             & 10             & ~\cite{ref:fisher:1936}    \\
                  car              & multivariate    & categorical                & 6                   & 4                & 1728               & 128            & 14             & ~\cite{ref:bohanec:1988}   \\
                  abalone          & multivariate    & categorical, integer, real & 8                   & 28               & 4177               & 256            & 17             & ~\cite{ref:waugh:1995}     \\
                  mushroom         & multivariate    & categorical                & 22                  & 2                & 8214               & 512            & 17             & ~\cite{ref:schlimmer:1987} \\
                  wine quality     & multivariate    & real                       & 12                  & 11               & 4898               & 256            & 20             & ~\cite{ref:cortez:2009}    \\
                  bank             & multivariate    & real                       & 17                  & 2                & 45211              & 512            & 89             & ~\cite{ref:moro:2014}      \\
                  diabetic         & multivariate    & integer                    & 55                  & 3                & 100000             & 1024           & 98             & ~\cite{ref:strack:2014}    \\
                  adult            & multivariate    & categorical, integer       & 14                  & 2                & 48842              & 256            & 191            & ~\cite{ref:kohavi:1996}    \\
            \end{tabular}%
      }
\end{table}%

\begin{table}[htbp]
      \centering
      \caption{Regression datasets}
      \label{tab:methodology:datasets:regression}%
      \par\bigskip
      \resizebox{\textwidth}{!}{
            \begin{tabular}{cccccccc}
                  \textbf{dataset}    & \textbf{output}          & \textbf{types} & \textbf{attributes} & \textbf{instances} & \textbf{batch} & \textbf{steps} & \textbf{citation}         \\
                  \midrule
                  fish toxicity       & multivariate             & real           & 7                   & 908                & 64             & 15             & ~\cite{ref:cassotti:2015} \\
                  housing             & univariate               & real           & 13                  & 506                & 32             & 16             & ~\cite{ref:harrison:1978} \\
                  forest fires        & multivariate             & real           & 13                  & 517                & 32             & 17             & ~\cite{ref:cortez:2007}   \\
                  student performance & multivariate             & integer        & 33                  & 649                & 32             & 21             & ~\cite{ref:cortez:2008}   \\
                  parkinsons          & multivariate             & integer, real  & 26                  & 5875               & 256            & 23             & ~\cite{ref:tsanas:2009}   \\
                  air quality         & multivariate, timeseries & real           & 15                  & 9358               & 256            & 37             & ~\cite{ref:de:2008}       \\
                  bike                & univariate               & integer, real  & 16                  & 17389              & 256            & 68             & ~\cite{ref:fanaee:2014}   \\
            \end{tabular}%
      }
\end{table}%

\noindent
Details on how the datasets where preprocessed and prepared is given in Appendix \ref{app:datasets}. The concept of class balancing is briefly discussed next.

\subsection{Class Balancing}

A number of classification datasets given in Table \ref{tab:methodology:datasets:classification} contain an unbalanced representation of classes. The empirical process defined in this dissertation does not apply mechanisms to cater for class balancing, in order to eliminate as many variables and factors in the empirical process as possible, as it is already a complicated process.  It is therefore suggested that the \ac{BHH} first be studied under the assumption of balanced classes, before applying class balancing techniques. In future research opportunities, class balancing should be utilised.

The next section presents the \acp{FFNN}/models and the configurations that are used in the empirical process.

\section{Models}\label{sec:methodology:model}

All models that are trained in this dissertation follow implementations of shallow \acp{FFNN}, meaning they only have one hidden layer. The architecture of a model is dependent on the dataset being trained on, the type of optimisation problem, the number of input dimensions and the number of output dimensions. Models' initial weights are initialised by means of \index{Glorot uniform sampling}Glorot uniform sampling. The models and their configuration, as it is used for each dataset, is given in Table \ref{tab:methodology:models:configurations} below.


% Table generated by Excel2LaTeX from sheet 'Model Configurations'
\begin{table}[htbp]
      \centering
      \caption{Model configurations}
      \label{tab:methodology:models:configurations}%
      \par\bigskip
      \resizebox{\textwidth}{!}{
            \begin{tabular}{rcccccccc}
                  \textbf{dataset}    & \textbf{inputs} & \textbf{hidden} & \textbf{output} & \textbf{biases} & \textbf{parameters} & \textbf{topology} & \textbf{l1 activation} & \textbf{l2 activation} \\
                  \midrule
                  fish toxicity       & 6               & 3               & 1               & yes             & 25                  & dense             & LReLU ($\alpha = 0.3$) & sigmoid                \\
                  iris                & 4               & 5               & 3               & yes             & 43                  & dense             & LReLU ($\alpha = 0.3$) & softmax                \\
                  air quality         & 12              & 8               & 1               & yes             & 113                 & dense             & LReLU ($\alpha = 0.3$) & sigmoid                \\
                  housing             & 13              & 8               & 1               & yes             & 121                 & dense             & LReLU ($\alpha = 0.3$) & sigmoid                \\
                  wine quality        & 13              & 10              & 7               & yes             & 217                 & dense             & LReLU ($\alpha = 0.3$) & softmax                \\
                  parkinsons          & 21              & 10              & 1               & yes             & 231                 & dense             & LReLU ($\alpha = 0.3$) & sigmoid                \\
                  car                 & 21              & 10              & 4               & yes             & 264                 & dense             & LReLU ($\alpha = 0.3$) & softmax                \\
                  forest fires        & 43              & 16              & 1               & yes             & 721                 & dense             & LReLU ($\alpha = 0.3$) & sigmoid                \\
                  abalone             & 10              & 36              & 28              & yes             & 1432                & dense             & LReLU ($\alpha = 0.3$) & softmax                \\
                  bank                & 51              & 32              & 1               & yes             & 1697                & dense             & LReLU ($\alpha = 0.3$) & softmax                \\
                  bike                & 61              & 32              & 1               & yes             & 2017                & dense             & LReLU ($\alpha = 0.3$) & sigmoid                \\
                  student performance & 99              & 32              & 1               & yes             & 3233                & dense             & LReLU ($\alpha = 0.3$) & sigmoid                \\
                  adult               & 108             & 64              & 1               & yes             & 7041                & dense             & LReLU ($\alpha = 0.3$) & softmax                \\
                  mushroom            & 117             & 64              & 1               & yes             & 7617                & dense             & LReLU ($\alpha = 0.3$) & softmax                \\
                  diabetic            & 2369            & 32              & 3               & yes             & 75939               & dense             & LReLU ($\alpha = 0.3$) & softmax                \\
            \end{tabular}%
      }
\end{table}%

\noindent
For the classification problems presented in Tables \ref{tab:methodology:datasets:classification} and \ref{tab:methodology:models:configurations}, the softmax activation function is added after training and is not included in the model during training. The loss functions, \acf{SparseCatXE} and \acf{BinXE}, that are used, contain a \index{softmax}softmax function.

\index{heuristic}
\section{Heuristics}\label{sec:methodology:heuristics}

This section provides the details of the low-level \index{heuristic}heuristics that are used in the empirical process. Each of these low-level heuristics are implemented as standalone heuristics and are also included in the heuristic pool of the \acs{BHH}. Table \ref{tab:methodology:heuristics} contains a list of all the standalone, low-level \index{heuristic}heuristics that are used as well as their hyper-parameter configuration.

% Table generated by Excel2LaTeX from sheet 'BHH Baseline'
\begin{table}[htbp]
      \centering
      \caption{Low-level \index{heuristic}heuristics and their hyper-parameter configuration.}
      \label{tab:methodology:heuristics}%
      \par\bigskip
      \resizebox{0.7\textwidth}{!}{

            \begin{tabular}{llll}
                  \textbf{heuristic} & \textbf{configuration}    & \textbf{value} & \textbf{citation}          \\
                  \midrule
                  sgd                & learning rate             & 0.1 (0.01)     & ~\cite{ref:sutskever:2013} \\
                  momentum           & learning rate             & 0.1 (0.01)     & ~\cite{ref:sutskever:2013} \\
                                     & momentum                  & 0.9            &                            \\
                  nag                & learning rate             & 0.1 (0.01)     & ~\cite{ref:sutskever:2013} \\
                                     & momentum                  & 0.9            &                            \\
                  adagrad            & learning rate             & 0.1 (0.01)     & ~\cite{ref:duchi:2011}     \\
                                     & epsilon                   & 1E-07          &                            \\
                  rmsprop            & learning rate             & 0.1 (0.01)     & ~\cite{ref:hinton:2012}    \\
                                     & rho                       & 0.95           &                            \\
                                     & epsilon                   & 1E-07          &                            \\
                  adadelta           & learning rate             & 1.0 (0.95)     & ~\cite{ref:zeiler:2012}    \\
                                     & rho                       & 0.95           &                            \\
                                     & epsilon                   & 0.0000001      &                            \\
                  adam               & learning rate             & 0.1 (0.01)     & ~\cite{ref:kingma:2014}    \\
                                     & beta1                     & 0.9            &                            \\
                                     & beta2                     & 0.95           &                            \\
                                     & epsilon                   & 1E-07          &                            \\
                  pso                & population size           & 10             & ~\cite{ref:van:2010}       \\
                                     & learning rate             & 0.1 (0.01)     &                            \\
                                     & inertia weight (w)        & 0.729844       &                            \\
                                     & cognitive control (c1)    & 1.49618        &                            \\
                                     & social control (c2)       & 1.49618        &                            \\
                                     & velocity clip min         & -1.0           &                            \\
                                     & velocity clip max         & 1.0            &                            \\
                  de                 & population size           & 10             & ~\cite{ref:mezura:2006}    \\
                                     & selection strategy        & best           &                            \\
                                     & xo strategy               & exp            &                            \\
                                     & recombination probability & 0.9 (0.1)      &                            \\
                                     & beta                      & 2.0 (0.1)      &                            \\
                  ga                 & population size           & 10             & ~\cite{ref:lambora:2019}   \\
                                     & selection strategy        & rand           &                            \\
                                     & xo strategy               & bin            &                            \\
                                     & mutation rate             & 0.2 (0.05)     &                            \\
            \end{tabular}%
      }
\end{table}%

\noindent
Note that from Table \ref{tab:methodology:heuristics}, values that are configured to make use of a decay schedule are presented with the initial value first and the decay rate in brackets next to it. The number of steps is the total number of mini-batch training steps that are executed for that particular dataset.

Furthermore, it should be noted that a learning rate was added to \acs{PSO} as an attempt to avoid overshooting solutions later in the training process. This parameter does not traditionally form part of the \ac{PSO}, but was found to be useful.

Section \ref{sec:bhh:heuristic:proxies} presented the concept of a mapping of proxied heuristic state update operations. The mapping of proxied heuristic state update operations, implemented by the \acs{BHH} in the empirical process, is given in Figure \ref{fig:methodology:heuristics:proxies}.


\begin{figure}[htbp]
      \centering
      \includegraphics[width=0.95\textwidth]{images/bhh_heuristic_proxies.pdf}
      \caption{Mapping of proxied heuristic state update operations as implemented by the \acs{BHH}}
      \label{fig:methodology:heuristics:proxies}%
\end{figure}


\noindent
In Figure \ref{fig:methodology:heuristics:proxies}, cells containing \textbf{x}, indicate that the associated heuristic implements that particular state parameter explicitly, and cells containing \textbf{o} indicate that the state parameter is implicitly implemented as part of the \acs{BHH} algorithm. The required mapping of proxied heuristic state operations is then implemented as a lookup of the table presented in Figure \ref{fig:methodology:heuristics:proxies}.

The following section provides the details around the \acs{BHH} baseline configuration.

\section{BHH Baseline}\label{sec:methodology:baseline_bhh}

The \acs{BHH} baseline is a name given to a specific configuration of the \ac{BHH} which, during development, has been found to provide a reasonable baseline performance. The baseline configuration is used as the cornerstone configuration from which all other \index{heuristic}heuristics and their configurations are evaluated. The \acs{BHH} baseline is also used for the behavioural study of the \acs{BHH}. The \acs{BHH} baseline configuration is given in Table \ref{tab:methodology:bhh_baseline_configuration}.

% Table generated by Excel2LaTeX from sheet 'BHH Baseline'
\begin{table}[htbp]
      \centering
      \caption{The \acs{BHH} baseline configurations as used in the empirical study.}
      \label{tab:methodology:bhh_baseline_configuration}%
      \par\bigskip
      \resizebox{0.65\textwidth}{!}{
            \begin{tabular}{rrcc}
                  \multicolumn{1}{c}{\textbf{hyper-heuristic}} & \multicolumn{1}{c}{\textbf{variant}} & \textbf{configuration} & \textbf{value} \\
                  \midrule
                  \multicolumn{1}{l}{bhh}                      & \multicolumn{1}{l}{baseline}         & heuristic pool         & all            \\
                                                               &                                      & population             & 5              \\
                                                               &                                      & credit                 & ibest          \\
                                                               &                                      & reselection            & 10             \\
                                                               &                                      & replay                 & 10             \\
                                                               &                                      & reanalysis             & 10             \\
                                                               &                                      & normalise              & false          \\
                                                               &                                      & discounted rewards     & false          \\
            \end{tabular}%
      }
\end{table}%

\noindent
In Table \ref{tab:methodology:bhh_baseline_configuration}, the \index{heuristic}heuristic pool configuration that is used, is referred to as \textit{all}. The \textit{all} configuration refers to a configuration where the \index{heuristicpool}heuristic pool contains all the low-level \index{heuristic}heuristics as presented in Section \ref{sec:methodology:heuristics}, including all gradient-based \index{heuristic}heuristics and \index{meta-heuristic}meta-heuristics.

The following section provides the performance measurements that are implemented.


\section{Performance Measures}\label{sec:methodology:performance_measures}

This section sheds light into the performance measures that are used to evaluate the different experimental runs.

Chapter \ref{chap:anns} provided the reader with a number of techniques that are used to evaluate \ac{FFNN} performance during training. Broadly these techniques measure the output of some loss/cost function and accuracy for classification problems. For the classifications problems, the loss metrics used included \ac{SparseCatXE} and \ac{BinXE}. Their accuracy equivalent metrics are then also used. For regression problems, \ac{RMSE} is used as a loss metric.

As mentioned in the preceding sections, datasets are split into a training set and a test dataset. The training set is used to train the model and the test set is used to test the model. Evaluation takes place at the end of each mini-batch iteration. Loss and accuracy is measured for both training and test datasets and is captured at each mini-batch iteration.

Post processing of the results yield a average rank (by test loss metric) between configurations at each mini-batch step across all runs. This test loss metric and rank then form experimental results that are then statistically analysed.

\section{Stopping Conditions}
\label{sec:methodology:stopping_conditions}

This section sheds light on the stopping conditions that are used.

Early stopping is a technique where training is prematurely stopped due to training saturation. A popular technique for early stopping includes a check to see if the validation/test loss has improved in the last $k$ steps. If the loss has not improved, the training is halted and the last best model weights are used.

It was decided not to use early stopping in this empirical process. Since the \ac{BHH} is a novel \ac{HH}, there are lots of uncertainty around its performance. It is unsure as to how the \ac{BHH} will behave, should overfitting take place. By eliminating early stop, the \ac{BHH} is evaluated until a maximum number of epochs (30) is reached. It is recommended that future research make use of early stopping.

The different experimental groups are given next.







\section{Experiments}
\label{sec:methodology:experiments}

This section presents the experimental groups that form part of the empirical process. There are broadly 3 main groups. These include a behavioural case study, a critical evaluation of the \ac{BHH} baseline's performance compared to other standalone low-level \index{heuristic}heuristics and finally an analysis of different \ac{BHH} variants is done by analysing the effects of different parameter values on the outcomes of the \ac{BHH}. Each of these sections are discussed in more detail below.


\subsection{Behavioural Case Study}
\label{sec:methodology:experiments:case_study}

This experimental group is concerned with objectively studying the behaviour of the \ac{BHH} baseline across a number of runs. This is meant to be an introductory analysis of the \ac{BHH} and includes analysis of the selection mechanism as well as the perturbative nature of the \ac{BHH}. It is important to mention that this experimental group is not meant to be statistically analysed, but to rather provide a detailed visual analysis of a number of hand-picked example runs, to determine if the \ac{BHH} is behaving as expected. It also provides a gentle introduction to the \ac{BHH}'s inner workings. From these observations, it is determined if the \ac{BHH} is learning and that selection is indeed biasing towards better performance. It also provides an opportunity to determine the outcome of the perturbative component of the \ac{BHH} which includes proxied \index{heuristic}heuristic update step operations.

Two different configurations of the \ac{BHH} baseline is trained using the iris dataset~\cite{ref:fisher:1936}. These configurations include a replay window size of 10 (short memory) and a high replay window size of 250 (long memory). Each configuration is repeat 3 times. Finally these configurations are compared to a run that is purely random.

The analysis is primarily done by dissecting the output of various parameters and studying the plots of their values. From these plots, conclusions about the \ac{BHH}'s behaviour is made.

\subsection{Standalone Optimisers}
\label{sec:methodology:experiments:standalone_optimisers}

THis section presents the details with the experimental group that focuses on evaluating the performance of the \ac{BHH} with that of standalone \index{heuristics}/optimisers.

For this experimental group, the heuristics and optimisers as presented in Section \ref{sec:methodology:heuristics} are used, along with their specified parameters. Each of these are then compared to that of the \ac{BHH} baseline configuration which is presented in Section \ref{sec:methodology:baseline_bhh}.

The next section provides the details around different \ac{BHH} variants that are evaluated.

\subsection{BHH Variants}
\label{sec:methodology:experiments:bhh_variants}

This section provides the details of the experimental group that focuses on \ac{BHH} variants and the effect that different hyper-parameter configurations can have on the outcome of the \ac{BHH}.

The different variants and their possible configurations is given in Table \ref{tab:methodology:experiments:bhh_variants} below.

\begin{table}[htbp]
      \centering
      \caption{BHH variants and their configuration}
      \label{tab:methodology:experiments:bhh_variants}%
      \par\bigskip
      \resizebox{0.9\textwidth}{!}{
            \begin{tabular}{rcc}
                  \multicolumn{1}{c}{\textbf{hyper-heuristic}} & \textbf{variant}    & \textbf{values}                   \\
                  \midrule
                  \multicolumn{1}{l}{bhh}                      & heuristic\_pool     & all,gd,mh                         \\
                                                               & population          & 5,10,15,20,25                     \\
                                                               & credit              & ibest,pbest,rbest,gbest,symmetric \\
                                                               & reselection         & 1,5,10,15,20                      \\
                                                               & replay              & 1,5,10,15,20                      \\
                                                               & reanalysis          & 1,5,10,15,20                      \\
                                                               & burn\_in            & 0,5,10,15,20                      \\
                                                               & normalise           & false,true                        \\
                                                               & discounted\_rewards & false,true                        \\
            \end{tabular}%
      }
\end{table}%

Take note that the \index{heuristics}heuristic pool options \textit{gd} and \textit{mh} refers to the \index{heuristics}heuristic pool configurations where the heuristic pools contain only gradient-descent heuristics or only \{index{meta-heuristics}meta-heuristics  respectively.

The next section explains in detail how the results are statistically analysed.

\section{Statistical Analysis}
\label{sec:methodology:statistical_analysis}

This section provides the detail of the process that is used to execute the statistical analysis of the results.

Various experimental groups and the details of the configuration is given in the preceding sections. Each of these experimental groups are statistically analysed on their own. To ensure there is statistically sufficient sample, each experimental group and configuration is trained for 30 epochs, repeated over 30 runs. As it is mentioned above, the results contain performance evaluation data in the form of training and testing, loss and accuracy measurements. Each experimental run is then ranked by these metrics. Each run is executed using a unique seeding value such such that each run is identical in its setup and configuration (apart from seeding values) and independent of the other runs.

Each experimental group goes through a set of steps that are followed during the statistical analysis process. First, the descriptive analysis is done to ensure that each experiment has the required data it needs. This includes 30 runs per configuration, across 15 datasets, across the applicable number of experimental configurations set up by that particular experimental group. The spread of the data is analysed to evaluate for overfit and outliers are identified. The skewness of the results is evaluated per dataset and the \index{Shapiro-Wilk}Shapiro-Wilk test for normality ($\alpha$ = 0.001) is used to determine in the results are normally distributed. The full statistical analysis reports are presented in Appendix \ref{app:statistical_analysis} and provides descriptive plots of the data's distribution. Furthermore, the \index{Levene}Levene's test for equality of variance ($\alpha$ = 0.001) is used.

Dependent on the outcomes of the above statistical tests, the appropriate statistical significance test is then executed. For configurations of independent variables where there are only 2 classes, the Mann-Whitney U\index{Mann-Whitney U} independent samples t-Test ($\alpha$ = 0.001) is executed and for configurations of 3 or more configuration classes, the \ac{ANOVA} statistical test ($\alpha$ = 0.001) is used. The\index{Kruskal-Wallis} Kruskal-Wallis ranked non-parametric test for statistical significance ($\alpha$ = 0.001) is used for cases where data is not normally distributed.

As mentioned earlier in the chapter, results are analysed for statistical significance over all the steps during the training process across all runs. This helps determine if there is statistical difference in the execution of various experimental configurations, throughout the entire training process, but also to cater for overfitting, which is to be studied as well.

Regardless of the statistical test that is used, a post-hoc Tukey honest significant difference test ($\alpha$ = 0.001) is used from which significant ranking is retrieved. Descriptive and critical difference plots are then retrieved from these results to provide visual aid, but all conclusions are made from statistically analysed data as mentioned above.

\section{Implementation and Execution}
\label{sec:methodology:implementation}

This section sheds some light as to the details of the implementation and execution of the empircal process.

All implementation is done from first principles in Python 3.9 using Tensorflow 2.7 and Tensorflow Probability 0.15.0. Most underlying function are reused from the Tensorflow library, however, all heuristics/optimisers are implemented from first principles to fit the \ac{HH} framework that was developed. All source code and data is provided is resources in this dissertation.

It should be noted that this implementation makes heavy use of CPU processing, due to the nature of modeling flattening for the \index{heuristic}heuristics. For this reason, execution is much more timely and costly than with GPU training. The authors hope that as the \ac{BHH} evolves, that better GPU implementations may speed up execution times.

With regards to the computational power that was used the execute this empirical process. All experiments were run on the CHPC's cluster. This included 14 servers each running 24-56 cores and 256GB memory each. The entire empirical process took 6 days.

\section{Summary}
\label{sec:methodology:summary}

This chapter provided the detail around the methodology that is used to execute the empirical process. The datasets, models and heuristics that are used during the empirical process have been presented in detail. A baseline \ac{BHH} has been formulated. The empirical process was defined in terms of a number of different experiments and finally, the process of statistical analysis of the results was provided.

The results and findings of the empirical process are presented in the following chapter.
