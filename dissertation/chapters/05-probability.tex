\chapter{Probability}
\label{chap:probability}

\begin{quotation}
      ``Probability theory is nothing but common sense reduced to calculation.''
\end{quotation}
\begin{flushright}
      - Pierre-Simon Laplace
\end{flushright}

Probability theory and statistics can be traced back to as early as the 18th century. In 1718,~\citeauthor{ref:demoivre:1718}~\cite{ref:demoivre:1718} published the \textit{Doctrine of of Chance}; a book that is widely regarded as the first published book on probability theory. In 1763, Thomas Bayes~\cite{ref:bayes:1763} published an article titled \textit{An Essay towards solving a Problem in the Doctrine of Chances} where the first version of \index{Bayes' theorem}Bayes' theorem was introduced.

Probability theory, statistics and \acf{ML} are transdisciplinary fields with many shared concepts. There are many examples of how probability theory has been incorporated into \acs{ML} research. In 1991,~\citeauthor{ref:denker:1991}~\cite{ref:denker:1991} proposed a way to transform \acf{ANN} outputs to probability distributions. In 1993, \citeauthor{ref:neal:1993}~\cite{ref:neal:1993} developed a \acf{MCMC} sampling algorithm for \acfp{BNN}. These are but a few examples of the role that probability theory has played in \acs{ML} research in the past.

Chapter \ref{chap:hhs} provided the concept of a \acf{HH}. This dissertation aims to develop a selection \acs{HH} that makes use of probability theory to select the best \acs{HH} to solve a given problem. This chapter aims to provide the necessary background information on probability theory and statistics. These are large fields and focus is put on the elements that are required to formulate the proposed \acf{BHH}. The remainder of the chapter is structured as follows:

\begin{itemize}
      \item \textbf{Section \ref{sec:probability:overview}} provides a brief overview of what probability is and how it is used.

      \item \textbf{Section \ref{sec:probability:cond_probability}} presents the concept of conditional probability.

      \item \textbf{Section \ref{sec:probability:multiple_events}} presents the two laws of probability related to the intersection and union of multiple events.

      \item \textbf{Section \ref{sec:probability:bayes_theorem}} introduces \index{Bayes' theorem}Bayes' theorem, the fundamental theorem upon which the \acs{BHH} is built.

      \item \textbf{Section \ref{sec:probability:probability_distributions}} presents the reader with relevant probability distributions.

      \item \textbf{Section \ref{sec:probability:conjugate_priors}} presents the reader with relevant conjugate prior probability distributions.

      \item \textbf{Section \ref{sec:probability:bayesian_statistics}} presents \textit{Bayesian} statistics. Brief discussions follow on the \textit{frequentist} view and \textit{Bayesian} view of probability. Detailed discussions follow on Bayesian optimisation methods such as \index{Bayesian analysis}\textit{Bayesian analysis}.

      \item \textbf{Section \ref{sec:probability:summary}} provides a brief summary of the chapter.
\end{itemize}


\section{Overview of Probability}\label{sec:probability:overview}

In everyday conversation, the term \textit{probability} is a measure of belief in the occurrence of a future event~\cite{ref:wackerly:2014}. Probability is a necessary tool used in many fields including physics, biology, chemistry and computer science. These fields contain many cases that generate observations that cannot be predicted with absolute certainty~\cite{ref:wackerly:2014}. Probability can be be inferred and confirmed through past events. These events are referred to as \textit{random} or \textit{stochastic} events. The probability that a certain event, $A$, might occur is denoted by $P(A)$. Although these random events cannot be predicted with absolute certainty, the relative frequency with which they occur over many trials, is often remarkably stable.

Consider flipping an unbiased, fair coin. The coin has two possible outcomes. It can conclude that each side has a $\frac{1}{2}$ or $50\%$ chance of occurring. In statistics, the decimal probability notation is used, where $0 <= P(A) <= 1$. Suppose the fair coin is thrown 10 times, there is no guarantee of observing $0.5$ heads and $0.5$ tails. There is some probability (although small) that the coin might fall on heads $0/10$ times. The probability of such an event occurring is $0.0009765625$. In the coin flip example, the \acf{CLT} shows that the normalised sum of events tends toward a normal distribution with a mean value of $0.5$, if the number of events observed, $N$, is large~\cite{ref:wackerly:2014}. The larger the value of $N$, the higher the confidence of mean probability and relative frequency of the event. The stable long-term relative frequency by which a random event occurs, provides an intuitive and meaningful measure of belief that a certain event will occur again at some point in the future~\cite{ref:wackerly:2014}.

Probability can also be expressed over multiple random events. Multiple random events can be considered together, dependently or conditionally. The following sections provide insight into conditional and joint probabilities of multiple random events.

\section{Conditional Probability and Independence}\label{sec:probability:cond_probability}

The occurrence of a given random event, $A$, can often be conditional on the occurrence of another event, $B$. In the field of medicine, an example of this is to calculate the probability of a certain diagnosis of a sick patient given his/her symptoms. The aforementioned relationship between events is referred to as the conditional probability between two events. The conditional probability is expressed as $P(A \vert B)$ and is read \textit{the probability of $A$ given $B$}. On the contrary, the \textit{unconditional} probability is the probability of an event, not dependent on any other. The conditional probability of $A$ given $B$ can be expressed as is given in Definition \ref{eq:probability:cond_probability} below~\cite{ref:wackerly:2014}.
\\
\begin{definition}[\textbf{Conditional Probability}]
      \label{eq:probability:cond_probability}
      The conditional probability of an event $A$, subject to the occurrence of event $B$ is expressed as

      \begin{equation}
            \label{eq:probability:overview:conditional}
            P(A \vert B) = \frac{P(A \cap B)}{P(B)}
      \end{equation}
\end{definition}

where $P(B) > 0$. Note that conditional probability does not suggest causation. If an event $A$ has a high probability of occurring after observing event $B$, it does not necessarily mean that $A$ is caused by $B$. Conditional probability simply expresses the dependence amongst events.

It could also be the case that the outcome of observing event $A$ is not affected by the occurrence of $B$. In this case, it is said that events $A$ and $B$ are independent. The independence between two events are expressed in Definition \ref{def:probability:cond_probability:independence} below.
\\
\begin{definition}[\textbf{Independence of Events}]
      \label{def:probability:cond_probability:independence}
      Two events, $A$ and $B$, are said to be independent of each other if, and only if the following criteria hold:

      \begin{itemize}
            \item $P(A \vert B) = P(A)$
            \item $P(B \vert A) = P(B)$
            \item $P(A \cap B) = P(A)P(B)$
      \end{itemize}

      Otherwise, events $A$ and $B$ are said to be \textit{dependent} random events.
\end{definition}

\section{Two Laws of Probability for Multiple Events}\label{sec:probability:multiple_events}

Suppose there are two random events, $A$ and $B$, then one can calculate the probability of the union and intersection of these events. From the aforementioned concept, two laws of probability can be formulated. These are referred to as the \textit{multiplicative} and \textit{additive} laws of probability~\cite{ref:wackerly:2014} and is given below in Theorems \ref{th:probability:multiple_events:multiplicative} and \ref{th:probability:multiple_events:additive} respectively.\\

\begin{theorem}[\textbf{The Multiplicative Law of Probability}]
      \label{th:probability:multiple_events:multiplicative}
      The probability of the intersection of two events, $A$ and $B$, is given as

      \begin{equation}
            \begin{split}
                  P(A \cap B)
                  &= P(A)P(B \vert A) \\
                  &= P(B)P(A \vert B)
            \end{split}
      \end{equation}

      If $A$ and $B$ are independent, then

      \begin{equation}
            P(A \cap B) = P(A)P(B)
      \end{equation}
\end{theorem}

\begin{proof}
      Proof is given from Definition \ref{eq:probability:cond_probability}.
\end{proof}
\vspace*{0.5cm}

\begin{theorem}[\textbf{The Additive Law of Probability}]
      \label{th:probability:multiple_events:additive}
      The probability of the union of two events, $A$ and $B$, is given as

      \begin{equation}
            P(A \cup B) = P(A) + P(B) - P(B \cap A)
      \end{equation}
\end{theorem}

\begin{proof}
      The geometric proof of the additive law of probability is given by the Venn Diagram presented in Figure \ref{fig:probability:multiple_events:additive}. Note that $A \cup B = A \cup ( \bar{A} \cap B)$, where $A$ and $\bar{A} \cap B$ are mutually exclusive events. Furthermore, consider that $B = (\bar{A} \cap B) \cup (A \cap B)$, where $\bar{A} \cap B$ and $A \cap B$ are mutually exclusive. Then $P(A \cup B) = P(A) + P(\bar{A} \cap B)$ and $P(B) = P(\bar{A} \cap B) + P(A \cap B)$. The equality on the right implies that $P(\bar{A} \cap B) = P(B) - P(A \cap B)$. By substituting the expression for $P(\bar{A} \cap B)$ into the expression for $P(A \cup B)$, the resulting expression $P(A \cup B) = P(A) + P(B) - P(A \cap B)$ is obtained.
\end{proof}

\begin{figure}[htbp]
      \centering
      \includegraphics[width=0.7\textwidth]{images/additive_law_of_probability_proof.pdf}
      \caption{A Venn-Diagram showing the proof of the additive law of probability for multiple events.}
      \label{fig:probability:multiple_events:additive}
\end{figure}

\index{Bayes' theorem}
\section{Bayes' Theorem}\label{sec:probability:bayes_theorem}

\index{Bayes' theorem}Bayes' theorem, named after Thomas Bayes, describes the probability of an event, $A$, based on prior knowledge of conditions that might be related to $A$~\cite{ref:zalta:2015}. In order to derive the formal theorem and proof, first consider Definition \ref{def:probability:bayes_theorem:partition} below~\cite{ref:zalta:2015}.
\\
\begin{definition}
      \label{def:probability:bayes_theorem:partition}
      For some positive integer, $K$, let the sets $B_{1}, B_{2}, \dots, B_{K}$ be such that

      \begin{enumerate}
            \item $S = B_{1} \cup B_{2} \cup \dots \cup B_{K}$
            \item $B_{i} \cap B_{j} = \emptyset$, for $i \neq j$
      \end{enumerate}

      Then the collection of sets $\{B_{1}$, $B_{2}$, $\dots$, $B_{K}\}$ is said to be a partition of S, the union of mutually exclusive subsets.
\end{definition}
\vspace*{0.5cm}

\index{Bayes' theorem}
\begin{theorem}[\textbf{Bayes' theorem}]
      \label{th:probability:bayes_theorem:theorem}
      Assume that $\{B_{1}, B_{2}, \dots, B_{K}\}$ is a partition of $S$ such that $P(B_{i}) > 0$, for $i = 1,2, \dots, K$ then\\
      \\
      \begin{equation}
            P(B_{j} \vert A) = \frac{P(A \vert B_{j})P(B_{j})}{\sum_{i=1}^{K} P(A \vert B_{i})P(B_{i})}
      \end{equation}
\end{theorem}

\begin{proof}
      The proof follows from the definition of conditional probability as was presented in Section~\ref{sec:probability:cond_probability}:

      \begin{equation}
            \begin{split}
                  P(B_{j} \vert A)
                  &= \frac{P(A \cap B_{j})}{P(A)}\\
                  &= \frac{P(A \vert B_{j})P(B_{j})}{\sum_{i=1}^{K} P(A \vert B_{i})P(B_{i})}
            \end{split}
      \end{equation}
\end{proof}

One of the many applications of \index{Bayes' theorem}Bayes' theorem is to do statistical inference. \index{Bayes' theorem}Bayes' theorem expresses how a degree of belief, expressed as a probability, should rationally change to account for the availability of related evidence.


\section{Probability Distributions}\label{sec:probability:probability_distributions}

Probability distributions are mathematical functions that give the probabilities of the occurrences of different possible outcomes in the experiment. The theory and equations presented in the following sections where all taken from~\citeauthor{ref:wackerly:2014}~\cite{ref:wackerly:2014}.


\subsection{Beta Probability Distribution}\label{sec:probability:probability_distributions:beta}

The \index{Beta probability distribution}Beta probability distribution is a family of univariate, continuous, probability distributions over some $x$, with support on the interval $[0,1]$~\cite{ref:wackerly:2014}. It is parameterised by two shape parameters $\alpha > 0$, $\alpha \in \mathbb{R}$ and $\beta > 0$, $\beta \in \mathbb{R}$. The \index{Beta probability distribution}Beta probability distribution is denoted as $Beta(\alpha, \beta)$. The \acf{PDF} of the \index{Beta probability distribution}Beta probability distribution is given as

\begin{equation}
      \label{eq:probability:probability_distributions:beta:pdf}
      P(x \vert \alpha, \beta) = f_{Beta}(x; \alpha, \beta) = \frac{1}{B(\alpha, \beta)} x^{\alpha - 1} (1 - x)^{\beta - 1}
\end{equation}

The normalising constant, $B(\alpha, \beta)$, is defined as

\begin{equation}
      \label{eq:probability:probability_distributions:beta:norm_const}
      B(\alpha, \beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)}
\end{equation}

where $\Gamma$ is the Gamma function defined as

\begin{equation}
      \label{eq:probability:probability_distributions:beta:gamma_func}
      \Gamma(n) = ( n - 1)!
\end{equation}

It should be noted that the Gamma function can also be written as

\begin{equation}
      \label{eq:probability:probability_distributions:beta:gamma_func_alt}
      \Gamma(n+1) = n!
\end{equation}

The control parameters $\alpha$ and $\beta$ determine the shape of the distribution. There exists a special case where $\alpha = \beta$. This is referred to as the \textit{symmetric} \index{Beta probability distribution}Beta probability distribution. In the case where $\alpha = \beta = 1$ the distribution is equivalent to the uniform distribution over all points in its support. The \index{Beta probability distribution}Beta probability distribution for various values of $\alpha$ and $\beta$, including the symmetric version is illustrated in Figure \ref{fig:probability:probability_distributions:beta}.

\begin{figure}[htbp]
      \begin{subfigure}{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/beta_various.pdf}
            \caption{\index{Beta probability distribution}Beta probability distribution}
            \label{fig:probability:probability_distributions:beta_normal}
      \end{subfigure}
      \begin{subfigure}{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/beta_cumulative.pdf}
            \caption{Cumulative \index{Beta probability distribution}Beta probability distribution}
            \label{fig:probability:probability_distributions:beta_cumulative}
      \end{subfigure}
      \par\bigskip
      \caption{An illustration of the \index{Beta probability distribution}Beta probability distribution (left)~\cite{ref:beta:2014} as well as the cumulative \index{Beta probability distribution}Beta probability distribution (right)~\cite{ref:cumulativebeta:2014} for various values of $\alpha$ and $\beta$ .}
      \label{fig:probability:probability_distributions:beta}
\end{figure}

The expected value of $x$ is given as

\begin{equation}
      \label{eq:probability:probability_distributions:beta:expected_value}
      E[x] = \frac{\alpha}{\alpha + \beta}
\end{equation}

Similarly, the expected value of the natural logarithm of $x$ is calculated as

\begin{equation}
      \label{eq:probability:probability_distributions:beta:expected_value_ln}
      E[\ln(x)] = \psi({\alpha}) - \psi(\alpha + \beta)
\end{equation}

where $\psi$ is the logarithmic derivative of the Gamma function, called the \textit{Digamma} function. The Digamma function is given as

\begin{equation}
      \label{eq:probability:probability_distributions:beta:digamma}
      \psi(n) = \frac{d}{dn}\ln(\Gamma(n)) = \frac{\Gamma'(n)}{\Gamma(n)}
\end{equation}

Finally, the mode of the distribution is given as

\begin{equation}
      \label{eq:probability:probability_distributions:beta:mode}
      M[x] = E[x] - 1 = \frac{\alpha - 1}{\alpha + \beta - 1}
\end{equation}


\subsection{Dirichlet Probability Distribution}\label{sec:probability:probability_distributions:dirichlet}

The \index{Dirichlet probability distribution}Dirichlet probability distribution is a family of multivariate continuous probability distributions over some $\boldsymbol{x}$ in $K$ dimensions~\cite{ref:wackerly:2014}. The \index{Dirichlet probability distribution}Dirichlet probability distribution is a multivariate generalization of the \index{Beta probability distribution}Beta probability distribution and is thus sometimes referred to by its alternative name, i.e. the multivariate Beta probability distribution. The \index{Dirichlet probability distribution}Dirichlet probability distribution is parameterised by some vector $\boldsymbol{\alpha} = (\alpha_{1}$,  $\dots, \alpha_{K}), \forall_{k=1}^{K} \alpha_{k} > 0, \alpha_{k} \in \mathbb{R}$. The parameters, $\boldsymbol{\alpha}$, are referred to as the concentration parameters. The \index{Dirichlet probability distribution}Dirichlet probability distribution of order $K \geq 2$ with parameters $\boldsymbol{\alpha}$, denoted $Dir(\boldsymbol{\alpha})$, has a \acs{PDF} given as

\begin{equation}
      \label{eq:probability:probability_distributions:dirichlet:pdf}
      P(\boldsymbol{x} \vert \boldsymbol{\alpha}) =  f_{Dir}(\boldsymbol{x}; K, \boldsymbol{\alpha}) = \frac{1}{B(\boldsymbol{\alpha})}  \prod_{k=1}^{K} x_{k}^{\alpha_{k} - 1}
\end{equation}

The normalising constant, $B(\boldsymbol{\alpha})$, is defined as

\begin{equation}
      \label{eq:probability:probability_distributions:dirichlet:norm_cost}
      B(\boldsymbol{\alpha}) = \frac{\prod_{k=1}^{K} \Gamma(\alpha_{k})}{\Gamma(\alpha_{0})}
\end{equation}

where $\alpha_{0}$ is defined as

\begin{equation}
      \label{eq:probability:probability_distributions:dirichlet:alpha_0}
      \alpha_{0} = \sum_{k=1}^{K}\alpha_{k}
\end{equation}

Importantly, the set $\{x_{k}\}_{k=1}^{K}$ belongs to the standard $K-1$ probability simplex $S$, meaning that $x_{K} = 1 - \sum_{k=1}^{K-1}x_{k}$ with support $\forall_{k=1}^{K} x_{k} \in [0,1]$. Under the simplex $S$, this means that the sum over all values of the vector $\boldsymbol{x}$ must be 1. The simplex can thus be rewritten as $\sum_{k=1}^{K}x_{k} = 1$.

Similar to the \index{Beta probability distribution}Beta probability distribution, $\boldsymbol{\alpha}$ determines the shape of the distribution in $K$ dimensions and thus, there also exist a special case, referred to as the \textit{symmetric} distribution when $\forall_{k=1}^{K} \alpha_{k} = c$, where $c$ is some constant. In the case where $c = 1$, the distribution is referred to as a \textit{flat} distribution and yields the uniform distribution over all points in $S$. The \index{Dirichlet probability distribution}Dirichlet probability distribution of order $K = 3$ for various values of $\boldsymbol{\alpha}$, including the symmetric version, is presented in Figure \ref{fig:probability:probability_distributions:dirichlet}.

\begin{figure}[htb]
      \centering
      \includegraphics[width=0.9\textwidth]{images/dirichlet.pdf}
      \caption{The \acfp{PDF} for the \index{Dirichlet probability distribution}Dirichlet probability distribution over the 2-simplex. The concentration parameters, $\boldsymbol{\alpha}$, are varied. The values of the \acs{PDF} are shown by the color maps with contour lines at equal values as indicated in the color bars~\cite{ref:dirichlet:2020}.}
      \label{fig:probability:probability_distributions:dirichlet}
\end{figure}

The expected value of $x_{k}$ is calculated as

\begin{equation}
      \label{eq:probability:probability_distributions:dirichlet:expected_value}
      E[x_{k}] = \frac{\alpha_{k}}{\alpha_{0}}
\end{equation}

Similarly, the expected value of the natural logarithm of $x_{k}$ is calculated as

\begin{equation}
      \label{eq:probability:probability_distributions:dirichlet:expected_value_ln}
      E[\ln(x_{k})] = \psi({\alpha_{k}}) - \psi(\alpha_{0})
\end{equation}

where $\psi$ is the Digamma function as defined in Equation~\eqref{eq:probability:probability_distributions:beta:digamma}. Finally, the mode of the distribution is given as

\begin{equation}
      \label{eq:probability:probability_distributions:dirichlet:mode}
      \begin{split}
            M[x_{k}] &= E[{x_{k}] -K^{-1}} \\
            &=  \frac{\alpha_{k} - 1}{\alpha_{0} - K}
      \end{split}
\end{equation}


\subsection{Bernoulli Probability Distribution}\label{sec:probability:probability_distributions:bernoulli}

The \index{Bernoulli probability distribution}Bernoulli probability distribution is a discrete probability distribution over some random variable $x$ that takes the value of $1$ with probability $\theta$ and $0$ with probability $1-\theta$ ~\cite{ref:wackerly:2014}. The \index{Bernoulli probability distribution}Bernoulli probability distribution is denoted as $Ber(\theta)$ with support $x \in \{0, 1\}$. In probability theory, the \index{Bernoulli probability distribution}Bernoulli probability distribution is often used to explain the possible outcomes of a single experiment that asks a \textit{yes-no} question such as flipping a coin. The outcome of such an experiment is a Boolean value. The \index{Bernoulli probability distribution}Bernoulli probability distribution has a \acs{PMF}, given as

\begin{equation}
      \label{eq:probability:probability_distributions:bernoulli:pmf}
      P(x \vert \theta) = f_{Ber}(x; \theta) =
      \begin{cases}
            \theta     & \text{if}\ x=1 \\
            1 - \theta & \text{if}\ x=0
      \end{cases}
\end{equation}

Equation~\eqref{eq:probability:probability_distributions:bernoulli:pmf} can also be expressed as

\begin{equation}
      \label{eq:probability:probability_distributions:bernoulli:alt}
      f_{Ber}(x; \theta) = \theta^{x}(1-\theta)^{1-x}
\end{equation}

The mean of the \index{Bernoulli probability distribution}Bernoulli probability distribution approaches $\theta$ over many samples according to the \acs{CLT}~\cite{ref:grinstead:1997}. Figure \ref{fig:probability:probability_distributions:bernoulli:coin} illustrates a fair-coin flipping simulation. The mean converges to 0.5 for various sample sizes.

\begin{figure}[htbp]
      \centering
      \begin{subfigure}{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/coin_flip_samples_100.pdf}
            \caption{100 Samples}
            \label{fig:probability:probability_distributions:bernoulli:coin_100}
      \end{subfigure}
      \begin{subfigure}{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/coin_flip_samples_10000.pdf}
            \caption{10000 Samples}
            \label{fig:probability:probability_distributions:bernoulli:coin_10000}
      \end{subfigure}
      \par\bigskip
      \caption{An illustration of the the coin-flip simulation for different sample sizes that show the convergence of the mean as per the \acf{CLT}.}
      \label{fig:probability:probability_distributions:bernoulli:coin}
\end{figure}

The expected value of the distribution is thus given as

\begin{equation}
      \label{eq:probability:probability_distributions:bernoulli:expected_value}
      E[x] = \theta
\end{equation}

The mode of the distribution is given as

\begin{equation}
      \label{eq:probability:probability_distributions:bernoulli:mode}
      M[x] =
      \begin{cases}
            0   & \text{if}\ \theta < 0.5 \\
            0,1 & \text{if}\ \theta = 0.5 \\
            1   & \text{if}\ \theta > 0.5 \\
      \end{cases}
\end{equation}


\subsection{Binomial Probability Distribution}\label{sec:probability:probability_distributions:bin}


The \index{Binomial probability distribution}Binomial probability distribution is a discrete probability distribution over a random variable $x$ taking on a number of successes in $N$ sequential independent experiments that each ask a \textit{yes-no} question~\cite{ref:wackerly:2014}. The probability of a single independent experiment yielding a success is given as $\theta$ and the \index{Binomial probability distribution}Binomial probability distribution is denoted as $Bin(N, \theta)$, with support $x \in \{0, 1, \dots, N\}$.  It should be noted that the \index{Binomial probability distribution}Binomial probability distribution is an extension of the \index{Bernoulli probability distribution}Bernoulli probability distribution over $N$ independent sequential experiments, and thus each experiment also yields some Boolean outcome. When $N=1$, the experiment is referred to as a Bernoulli trial, and the distribution is just a \index{Bernoulli probability distribution}Bernoulli probability distribution. When $N > 1$, the sequence of outcomes is referred to as a Bernoulli process. The \acs{PMF} of the \index{Binomial probability distribution}Binomial probability distribution is given as

\begin{equation}
      \label{eq:probability:probability_distributions:binomial:pmf}
      P(x \vert \theta; N) = f_{Bin}(x; N, \theta) = \binom{N}{x} \theta^{x}(1-\theta)^{1-x}
\end{equation}

Similar to the \index{Bernoulli probability distribution}Bernoulli probability distribution, the mean of the \index{Binomial probability distribution}Binomial probability distribution is $N\theta$ given the \acs{CLT} as was shown in Figure \ref{fig:probability:probability_distributions:bernoulli:coin}. The expected value of the \index{Binomial probability distribution}Binomial probability distribution is thus given as

\begin{equation}
      \label{eq:probability:probability_distributions:binomial:expected_value}
      E[x] = N\theta
\end{equation}

The mode of the distribution is given as

\begin{align}
      \label{eq:probability:probability_distributions:binomial:mode}
      \begin{split}
            M[x] &= E[x] + \theta \\
            &= N\theta  + \theta \\
            &= (N  + 1)\theta
      \end{split}
\end{align}

\subsection{Categorical Probability Distribution}\label{sec:probability:probability_distributions:categorical}

The \index{Categorical probability distribution}Categorical probability distribution is a discrete probability distribution over some random variable, $x$, taking on any one of $K$ possible categories~\cite{ref:wackerly:2014}. There is no ordering to these categories and therefore, for simplicity, each category is assigned a numerical representative value such that $k \in \{1, 2, \dots, K\}$. The probabilities for all outcomes is given by the probability vector $\boldsymbol{\theta} = (\theta_{1}, \dots, \theta_{K})$.  This means that the probability $P(x=k)=\theta_{k}$ has support $x \in \{1, \dots, K\}$. The \index{Categorical probability distribution}Categorical probability distribution, denoted $Cat(\boldsymbol{\theta})$, is a generalization of the \index{Bernoulli probability distribution}Bernoulli probability distribution and is sometimes referred to it by its alternative names, i.e. the generalised \index{Bernoulli probability distribution}Bernoulli probability distribution or the Multinoulli probability distribution. In probability theory, the \index{Categorical probability distribution}Categorical probability distribution is often used to explain the outcome of a single experiment with more than two possible outcomes, such as rolling a six-sided die~\cite{ref:wackerly:2014}. The \acs{PMF} of the \index{Categorical probability distribution}Categorical probability distribution is given as

\begin{equation}
      \label{eq:probability:probability_distributions:categorical:pmf}
      P(x \vert \boldsymbol{\theta}; K) = f_{Cat}(x; K, \boldsymbol{\theta}) = \prod_{k=1}^{K}\theta_{k}^{[x = k]}
\end{equation}

where the term, $[x = k]$, is the \textit{Iversion Bracket}~\cite{ref:iverson:1962}, yielding 1 if $x = k$ and 0 otherwise. Given class $k$, the categorical distribution simply yields $\theta_{k}$ as follows:

\begin{equation}
      \label{eq:probability:probability_distributions:categorical:pmf_k}
      f_{Cat}(x=k; K, \boldsymbol{\theta}) = \theta_{k}
\end{equation}

The random variable $x$ can also be encoded in binary format, yielding a vector $\boldsymbol{x} = (x_{1}, \dots, x_{K})$ of \index{Bernoulli probability distribution}Bernoulli probability distributions such that the support is $\forall_{k=1}^{K} x_{k} \in \{0, 1\}$. Importantly, if the outcome of the random event is of category $k$, then $x_{k} = 1$ and $\forall_{j=1}^{K} x_{j} = 0, j \neq k$ so that the standard $K-1$ probability simplex $S$ still holds. The \acs{PMF} of the \index{Categorical probability distribution}Categorical probability distribution is rewritten as

\begin{equation}
      \label{eq:probability:probability_distributions:categorical:pmf_alt}
      f_{Cat}(\boldsymbol{x}; K, \boldsymbol{\theta}) = \prod_{k=1}^{K}\theta_{k}^{\mathbbm{1}_{1}(x_{k})}
\end{equation}

where $\mathbbm{1}(x_{k})$ is the \textit{indicator function}, yielding 1 if $x_{k} = 1$ and 0 otherwise.

Since there is no order to the underlying categories, the mean of the distribution does not yield any relevant information. The mode of the distribution is given as

\begin{equation}
      \label{eq:probability:probability_distributions:categorical:mode}
      M[x] = \argmax_{k}(\theta_{1}, \dots, \theta_{K})
\end{equation}


\subsection{Multinomial Distribution}
\label{sec:probability:probability_distributions:multinomial}

The \index{Multinomial probability distribution}Multinomial probability distribution is a discrete probability distribution over some random variable $\boldsymbol{x} = (x_{1}, \dots\, x_{K})$ that takes on the counts for each occurrence of $K$ possible classes in $N$ independent trials~\cite{ref:wackerly:2014}. The probabilities for all possible outcomes in a single trial is given by the probability vector, $\boldsymbol{\theta} = (\theta_{1}, \dots, \theta_{K})$. The \index{Multinomial probability distribution}Multinomial probability distribution, denoted $Mul(N, K, \boldsymbol{\theta})$, is thus a generalization of the \index{Binomial probability distribution}Binomial probability distribution to $K$ dimensions. Consider the following special cases:

\begin{itemize}
      \item When $K$ is 2 and $N = 1$, the \index{Multinomial probability distribution}Multinomial probability distribution is the \index{Bernoulli probability distribution}Bernoulli probability distribution.
      \item When $K$ is 2 and $N > 1$, the \index{Multinomial probability distribution}Multinomial probability distribution is the \index{Binomial probability distribution}Binomial probability distribution.
      \item When $K > 2$ and $N = 1$, the \index{Multinomial probability distribution}Multinomial probability distribution is the \index{Categorical probability distribution}Categorical probability distribution.
\end{itemize}

The support for the Multinomial is $\forall_{i=1}^{K} x_{k} \in \{1, \dots, N\}, \sum_{k=1}^{K}x_{k} = N$ and the \acs{PMF} for the \index{Multinomial probability distribution}Multinomial probability distribution is given as

\begin{equation}
      \label{eq:probability:probability_distributions:multinomial:pmf}
      P(\boldsymbol{x} \vert \boldsymbol{\theta}; N; K) = f_{Mul}(\boldsymbol{x}; N, K, \boldsymbol{\theta}) = \frac{N!}{\prod_{k=1}^{K}x_{k}!} \prod_{k=1}^{K}\theta_{k}^{x_{k}}
\end{equation}

Similar to the \index{Categorical probability distribution}Categorical probability distribution, the random variable $x$ can also be encoded in binary format, yielding an $N \times K$ matrix $\boldsymbol{X}$ of \index{Bernoulli probability distribution}Bernoulli probability distributions. The support is then given as $\boldsymbol{X} \in \{0, 1\}^{N \times K}, \forall_{i=1}^{N}\sum_{k=1}^{K} x_{i,k} = 1$ so that the standard $K-1$ probability simplex $S$ still holds for each trial. The \acs{PMF} of the \index{Multinomial probability distribution}Multinomial probability distribution is then rewritten as

\begin{equation}
      \label{eq:probability:probability_distributions:multinomial:pmf_alt}
      \begin{split}
            f_{Mul}(\boldsymbol{X}; N, K, \boldsymbol{\theta}) &= \frac{N!}{\prod_{k=1}(\sum_{i=1}^{N}x_{i, k})!}\prod_{i=1}^{N}\prod_{k=1}^{K}\theta_{k}^{\mathbbm{1}_{1}(x_{i,k})} \\
            &= \frac{N!}{\prod_{k=1}(\sum_{i=1}^{N}x_{i, k})!} \prod_{k=1}^{K}\theta_{k}^{\sum_{i=1}^{N}\mathbbm{1}_{1}(x_{i,k})} \\
            &= \frac{N!}{\prod_{k=1}(\sum_{i=1}^{N}x_{i, k})!} \prod_{k=1}^{K}\theta_{k}^{N_{k}} \\
      \end{split}
\end{equation}

where $N_{k}$ is a summary variable, denoting the number of times a category $k$ occurs over all trials in $N$.

\section{Conjugate Priors}\label{sec:probability:conjugate_priors}

\citeauthor{ref:wackerly:2014}\cite{ref:wackerly:2014} state that conjugate priors are prior probability distributions that result in posterior distributions that are of the same functional form, $\mathcal{A}(v)$, as the prior, but with different parameter values. This section considers the conjugate priors that are used with the Binomial likelihood and Categorical/Multinomial likelihood.

\subsection{Binomial Likelihood}\label{sec:probability:conjugate_priors:binom_likelihood}

The conjugate prior to a \index{Bernoulli probability distribution}Bernoulli probability distribution is the \index{Beta probability distribution}Beta probability distribution
\cite{ref:wackerly:2014}. This is shown by demonstrating that the posterior distribution has the
same functional form, $\mathcal{A}(v)$, as the prior distribution as follows. \\\\
\textbf{Setup}:

\begin{itemize}
      \item Let $I$ be a number of independent, identical  (iid) random events.

      \item Let $\alpha \in \mathbb{R}, \alpha > 0$ and $\beta \in \mathbb{R}, \beta >0$ be the shape parameters to the \index{Beta probability distribution}Beta probability distribution.

      \item Let $\theta$ be the probability of a success. With $\theta | \alpha, \beta \sim Beta(\alpha, \beta)$.

      \item $P(\theta)$ is the prior probability distribution with the functional form $\mathcal{A}(v)$.

      \item Let $\boldsymbol{X} = (x_{1}, \dots, x_{I})$ be the outcomes of $I$ independent, identical random events, each with Boolean outcome. That is $x_{i} | \theta \overset{\text{iid}}{\sim} Ber(\theta)$ and $\mathcal{L}(x_{i} \vert \theta)$ is the Bernoulli likelihood.

      \item Let $\boldsymbol{\mathcal{D}}$ denote all the prior data of $\boldsymbol{X}$, parameterised by $\alpha$, $\beta$.

      \item Let $N_{1} = \sum_{i=1}^{I} \mathbbm{1}(x_{i} = 1)$ and $N_{0} = \sum_{i=1}^{I} \mathbbm{1}(x_{i} = 0)$.
\end{itemize}

The Bernoulli likelihood is given as

\begin{equation}
      \label{eq:probability:conjugate_priors:binom_likelihood:likelihood}
      \begin{split}
            \mathcal{L}(\boldsymbol{\mathcal{D}}) &=  P(\boldsymbol{\mathcal{D}} | \theta) \\
            &\propto \theta^{N_{1}}(1-\theta)^{N_{0}}
      \end{split}
\end{equation}

By \index{Bayes' theorem}Bayes' theorem, the posterior distribution with given prior data $\boldsymbol{\mathcal{D}}$ is given as

\begin{equation}
      \begin{split}
            \label{eq:probability:conjugate_priors:binom_likelihood:posterior}
            P(\theta \vert \boldsymbol{\mathcal{D}}) &= \frac{P(\boldsymbol{\mathcal{D}} | \theta) P(\theta)}{P(\boldsymbol{\mathcal{D}})}
      \end{split}
\end{equation}

Since the denominator sums to $1$, the denominator and constants for the Bernoulli likelihood and the Beta prior can be removed by expressing the posterior as proportional to the likelihood times the prior as follows:

\begin{equation}
      \label{eq:probability:conjugate_priors:binom_likelihood:posterior_propto}
      \begin{split}
            P(\theta | \boldsymbol{\mathcal{D}}) &\propto \left[\theta^{N_{1}}(1-\theta)^{N_{0}}\right] \left[\theta^{\alpha - 1} (1 - \theta)^{\beta - 1}\right] \\
            &\propto \theta^{(N_{1} + \alpha) - 1}(1-\theta)^{(N_{0} + \beta) - 1} \\
            &\propto Beta(N_{1} + \alpha, N_{0} + \beta)
      \end{split}
\end{equation}

The posterior distribution has the same functional form, $\mathcal{A}(v)$, as the prior, but with updated prior parameters $\alpha' = N_{1} + \alpha$ and $\beta' = N_{0} + \beta$. This shows that the \index{Beta probability distribution}Beta probability distribution is the conjugate prior used with the Bernoulli likelihood.


\subsection{Categorical and Multinomial Likelihood}
\label{sec:probability:conjugate_priors:cat_mult_likelihood}

The conjugate prior to a \index{Categorical probability distribution}Categorical and \index{Multinomial probability distribution}Multinomial probability distribution is the \index{Dirichlet probability distribution}Dirichlet probability distribution~\cite{ref:wackerly:2014}. The proof of the conjugate prior for the \index{Bernoulli probability distribution}Bernoulli probability distribution is similar to the proof presented in Section \ref{sec:probability:conjugate_priors:binom_likelihood}. This means that the posterior distribution must have the same functional form, $\mathcal{A}(v)$, as the prior distribution. The proof is shown as follows. \\\\
\textbf{Setup}:

\begin{itemize}
      \item Let $I$ be a number of independent, identical (iid) random events.

      \item Let $K$ be a number of possible outcomes for each event, with $K \geq 2$.

      \item Let $\boldsymbol{\alpha} = (\alpha_{1}, \dots, \alpha_{K}), \forall_{k=1}^{K} \alpha_{k} \in \mathbb{R}, \alpha_{k} > 0$ be the concentration parameters to the \index{Dirichlet probability distribution}Dirichlet probability distribution.

      \item Let $\boldsymbol{\theta} = (\theta_{1}, \dots, \theta_{K}), \forall_{k=1}^{K} \theta_{k} \in (0,1), \sum_{k}^{K} \theta_{k} = 1$ be the probability of each class in $K$ and $\boldsymbol{\theta}$ belongs to the standard $K-1$ probability simplex $S$. With $\boldsymbol{\theta} | \boldsymbol{\alpha} \sim Dir(K, \boldsymbol{\alpha})$.

      \item $P(\boldsymbol{\theta})$ is the prior probability distribution with the functional form $\mathcal{A}(v)$.

      \item Let $\boldsymbol{X} = (\boldsymbol{x_{1}}, \dots, \boldsymbol{x_{I}})$ be the outcomes of $I$ independent, identical random events, each with $K$ possible outcomes. That is $\boldsymbol{x_{i}} | \boldsymbol{\theta} \overset{\text{iid}}{\sim} Cat(\boldsymbol{\theta})$ and $\mathcal{L}(\boldsymbol{x_{i}} \vert \boldsymbol{\theta})$ is the Categorical likelihood.

      \item Let $\boldsymbol{\mathcal{D}}$ denote all the prior data of $\boldsymbol{X}$, parameterised by $\boldsymbol{\alpha}$.

      \item Let $\boldsymbol{N} = (N_{1}, \dots, N_{K}), N_{k} = \sum_{i=1}^{I} \mathbbm{1}(x_{i,k} = 1)$, denote the counts for each occurrence of a class $k$.
\end{itemize}

The likelihood of the Categorical and \index{Multinomial probability distribution}Multinomial probability distributions is given as

\begin{equation}
      \label{eq:probability:conjugate_priors:mult_likelihood:likelihood}
      \begin{split}
            \mathcal{L}(\boldsymbol{\mathcal{D}}) &=  P(\boldsymbol{\mathcal{D}} | \boldsymbol{\theta}) \\
            &\propto \prod_{k=1}^{K} \theta_{k}^{N_{k}}
      \end{split}
\end{equation}

By \index{Bayes' theorem}Bayes' theorem, the posterior distribution with given prior data $\boldsymbol{\mathcal{D}}$ is given as

\begin{equation}
      \label{eq:probability:conjugate_priors:mult_likelihood:posterior}
      \begin{split}
            P(\boldsymbol{\theta} \vert \boldsymbol{\mathcal{D}}) &= \frac{P(\boldsymbol{\mathcal{D}} \vert \boldsymbol{\theta}) P(\boldsymbol{\theta)}}{P(\boldsymbol{\mathcal{D}})}
      \end{split}
\end{equation}

Since the denominator sums to $1$, the denominator and constants for the \index{Dirichlet probability distribution}Dirichlet prior can be removed by expressing the posterior as proportional to the likelihood times the prior as follows:

\begin{equation}
      \label{eq:probability:conjugate_priors:mult_likelihood:posterior_propto}
      \begin{split}
            P(\boldsymbol{\theta} \vert \boldsymbol{\mathcal{D}}) &\propto \prod_{k=1}^{K} \theta_{k}^{N_{k}} \prod_{k=1}^{K} \theta_{k}^{\alpha_{k} - 1}\\
            &\propto \prod_{k=1}^{K} \theta_{k}^{(N_{k} + \alpha_{k}) - 1} \\
            &\propto Dir(K, \boldsymbol{N} + \boldsymbol{\alpha})
      \end{split}
\end{equation}

The posterior distribution has the same form, $\mathcal{A}(v)$, as the prior, but with updated prior parameters $\boldsymbol{\alpha'} = \boldsymbol{N} + \boldsymbol{\alpha}$. This shows that the \index{Dirichlet probability distribution}Dirichlet probability distribution is the conjugate prior used with the Categorical/Multinomial likelihood.

\section{Bayesian Statistics}
\label{sec:probability:bayesian_statistics}

This section provides detailed discussions on various aspects related to Bayesian statistics and probability. Brief discussion follow on the frequentist and the Bayesian approach to statistics and probability. Finally, the concept of Bayesian analysis is presented in detail.

\subsection{Frequentist vs. Bayesian Statistics}
\label{sec:probability:bayesian_statistics:frequentist_vs_bayesian}

In general, there are two main views to probability and statistics. These include the \textit{frequentist} and the \textit{Bayesian} view of statistics. Naturally, Bayesian statistics is based on \index{Bayes' theorem}Bayes' theorem as was presented in Section \ref{sec:probability:bayes_theorem}. Bayesian statistics describe the probability of an event in terms of some belief, based on previous knowledge of the event and the conditions under which the event happened~\cite{ref:hackenberger:2019}. To introduce the concept of Bayesian inference and Bayesian analysis, the differences between the frequentist and the Bayesian view of statistics need to be presented.

Bayesian statistics out-date the frequentist approach, but lacked interest in the early days, partly because of the limited applications where the conjugate priors where known~\cite{ref:hackenberger:2019}. More recent advancements in mathematical methods popularised the Bayesian approach again. A notable contribution to this switch was the development of the \acf{MCMC} algorithm in the 1950s. This family of algorithms allowed for the construction of random sampling algorithms from a probability distribution, which allows for the calculation of Bayesian hierarchical models~\cite{ref:hackenberger:2019}. Soon after followed one of the earliest papers that use Bayesian statistics in the field of medicine in 1982~\cite{ref:ashby:2006}.

The difference between the frequentist approach and the Bayesian approach can be illustrated using an example.~\citeauthor{ref:hackenberger:2019}~\cite{ref:hackenberger:2019} suggests an experiment that investigates whether the gender ratio in some hypothetical mice population is $1:1$. Two experiments were designed. In the first experiment, mice are randomly selected until the first male is chosen. The result in this experiment will then be the total number of mice chosen by gender. For the second experiment, exactly seven mice are randomly selected. The result of the second experiment would be the number of males and females in a sample of seven. Suppose the outcome of the experiment was $FFFFFFM$, where $F$ represents a female and $M$ represents a male. If the experimental design is not known ahead of time, the result is useless. Consider the $p$-value for each of these experiments. The $p$-value is the probability of obtaining results at least as extreme as the observed results of a statistical hypothesis test~\cite{ref:beers:2022}. For the first experiment, the $p$-value is $0.031$ and for the second experiment, the $p$-value is $0.227$. Using a confidence level of $\alpha = 0.05$, opposite outcomes could be concluded for these two experiments when it comes to rejecting the null hypothesis, despite using the same data. The reason for the difference in outcomes, is due to the difference in their null distributions, which represent the probability distribution of the test statistic when the null hypothesis is true. The first approach uses a geometrical approach, and the second used a binomial approach as illustrated in Figure \ref{fig:probability:bayesian_statistics:mouse_experiment_outcome}.

\begin{figure}[htb]
      \centering
      \includegraphics[width=0.6\textwidth]{images/mouse_experiment_outcome.pdf}
      \caption{The experimental outcomes for the mice-population experiments as was taken from~\cite{ref:hackenberger:2019} .}
      \label{fig:probability:bayesian_statistics:mouse_experiment_outcome}
\end{figure}

If Bayesian statistics is used, the experimental design that was chosen does not matter. In Bayesian statistics it is common to use a \index{Beta probability distribution}Beta probability distribution as a prior distribution. If the prior distribution is sampled from $Beta(3,3)$, then using Bayesian analysis, the posterior distribution, according to the outcomes of this experiment, would yield $Beta(9,4)$.

~\citeauthor{ref:hackenberger:2019}~\cite{ref:hackenberger:2019} mentions that the $Beta$ probability distribution can be seen as a probability distribution of the occurrence of specific parameters. From the information that is now known about the $Beta$ probability distribution, it is possible to calculate the probability that the gender ratio in this mice population is not $1:1$, with the $Beta$ probability distribution as a prior, yielding a $p$-value of $0.92$. This means that there is a probability of $92\%$ that the mice population is not $1:1$, regardless of experimental design. An illustration of this is given in Figure \ref{fig:probability:bayesian_statistics:mouse_distributions}.

\begin{figure}[htbp]
      \centering
      \includegraphics[width=0.6\textwidth]{images/mouse_experiment_distributions.pdf}
      \caption{An illustration of the prior and posterior probability distributions for the outcomes of the mice-population experiment, using a $Beta$ prior, as was taken from~\cite{ref:hackenberger:2019}.}
      \label{fig:probability:bayesian_statistics:mouse_distributions}
\end{figure}

Figure \ref{fig:probability:bayesian_statistics:mouse_distributions} shows how the posterior distribution differs from the prior distribution.

\subsection{Bayesian Analysis}
\label{sec:probability:bayesian_statistics:bayesian_analysis}

Bayesian analysis is the process by which prior beliefs are updated as a result of observing new data/evidence. Similar to the approaches followed above to explain Bayesian statistics, a proposal is made to explain Bayesian analysis by means of an example taken from~\cite{ref:wackerly:2014}. Let $\boldsymbol{Y} = (Y_{1}, Y_{2}, \dots, Y_{N})$ denote the random variable that is observed over a sample size of $N$. Then the likelihood of the sample is given as $\mathcal{L}(y_{1}, y_{2}, \dots, y_{n} \vert \theta)$. In the discrete case, the likelihood function is defined to be the joint probability, $P(Y_{1} = y_{1}, Y_{2} = y_{2}, \dots, Y_{N} = y_{n})$, and for the continuous case, yields the joint density of $Y_{1}, Y_{2}, \dots, Y_{N}$, evaluated at $y_{1}, y_{2}, \dots, y_{n}$. The Bayesian view models the parameter $\theta$ as a random variable with a probability distribution, referred to as the prior distribution of $\theta$. The symbol $\theta$, is included in the notation of $\mathcal{L}$ as an argument to illustrate that this function is dependent, explicitly, on the value of $\theta$. Importantly, the prior distribution is specified before any data is collected and represents the theoretical prior knowledge about $\theta$. Assume that the parameter $\theta$ has a continuous distribution with density $g(\theta)$ that has no unknown parameters. Considering the likelihood of the data and the prior on $\theta$, then the joint likelihood of $Y_{1}, Y_{2}, \dots, Y_{N}, \theta$ is given as

\begin{equation}
      \label{eq:probability:bayesian_statistic:bayesian_analysis:joint_likelihood}
      f(y_{1}, y_{2}, \dots, y_{n}, \theta) = \mathcal{L}(y_{1}, y_{2}, \dots, y_{n} \vert \theta)g(\theta)
\end{equation}

The marginal density or mass function of $Y_{1}, Y_{2}, \dots, Y_{N}$ is given as

\begin{equation}
      \label{eq:probability:bayesian_statistic:bayesian_analysis:marginal_density}
      m(y_{1}, y_{2}, \dots, y_{n}) = \int_{-\infty}^{\infty} \mathcal{L}(y_{1}, y_{2}, \dots, y_{n} \vert \theta)g(\theta)d\theta
\end{equation}

Finally, the posterior density of $\theta \vert y_{1}, y_{2}, \dots, y_{n}$ denoted by $g^{*}(\theta \vert y_{1}, y_{2}, \dots, y_{n})$, according to \index{Bayes' theorem}Bayes' theorem, is given as

\begin{equation}
      \label{eq:probability:bayesian_statistic:bayesian_analysis:posterior_density}
      g^{*}(\theta \vert y_{1}, y_{2}, \dots, y_{n}) = \frac{\mathcal{L}(y_{1}, y_{2}, \dots, y_{n} \vert \theta)g(\theta)}{\int_{-\infty}^{\infty} \mathcal{L}(y_{1}, y_{2}, \dots, y_{n} \vert \theta)g(\theta)d\theta}
\end{equation}

\citeauthor{ref:wackerly:2014}\cite{ref:wackerly:2014} mention that the posterior density summarises all the pertinent information about the parameter $\theta$ by making use of the information contained in the prior for $\theta$, as well as the information in the observed data/evidence.

Consider now how Bayesian analysis can be used to update the priors on $\theta$ based on newly observed data. As with the example above, the discussion below is taken from~\cite{ref:wackerly:2014}. Let $Y_{1}, Y_{2}, \dots, Y_{N}$ denote a random sample from a \index{Bernoulli probability distribution}Bernoulli probability distribution, where $P(Y_{i} = 1) = \theta$ and $P(Y_{i} = 0) = 1 - \theta$ and the prior distribution for $\theta$ is $Beta(\alpha, \beta)$. The posterior distribution for $\theta$ can be formulated as follows. The Bernoulli probability function is written as

\begin{equation}
      \label{eq:probability:bayesian_statistic:bayesian_analysis:step_1}
      P(y_{i} \vert \theta) = \theta^{y_{i}}(1 - \theta)^{1-y_{i}}
\end{equation}

where $y_{i} = \{0,1\}$ and $0 < \theta < 1$. The likelihood $\mathcal{L}(y_{1}, y_{2}, \dots, y_{n} \vert \theta)$ is presented as follows:

\begin{equation}
      \label{eq:probability:bayesian_statistic:bayesian_analysis:step_2}
      \begin{split}
            \mathcal{L}(y_{1}, y_{2}, \dots, y_{n} \vert \theta)
            &= P(y_{1}, y_{2}, \dots, y_{n} \vert \theta)\\
            &= \theta^{y_{1}}(1-\theta)^{1 - y_{1}} \times \theta^{y_{2}}(1-\theta)^{1 - y_{2}} \times \dots \times \theta^{y_{n}}(1-\theta)^{1 - y_{n}}\\
            &= \theta^{\sum y_{i}}(1-\theta)^{n-\sum y_{i}}
      \end{split}
\end{equation}

Then the joint likelihood of $Y_{1}, Y_{2}, \dots, Y_{N}, \theta$ from Equation~\eqref{eq:probability:bayesian_statistic:bayesian_analysis:joint_likelihood} is formulated as

\begin{equation}
      \label{eq:probability:bayesian_statistic:bayesian_analysis:step_3}
      \begin{split}
            f(y_{1}, y_{2}, \dots, y_{n}, \theta)
            &= \mathcal{L}(y_{1}, y_{2}, \dots, y_{n} \vert \theta)g(\theta)\\
            &= \theta^{\sum y_{i}}(1-\theta)^{n-\sum y_{i}} \times \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta^{\alpha - 1}(1 - \theta)^{\beta  - 1}\\
            &= \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta^{\sum y_{i} + \alpha - 1}(1-\theta)^{n - \sum y_{i} + \beta - 1}
      \end{split}
\end{equation}

The marginal density of $Y_{1}, Y_{2}, \dots Y_{N}$ is then given as

\begin{equation}
      \label{eq:probability:bayesian_statistic:bayesian_analysis:step_4}
      \begin{split}
            m(y_{1}, y_{2}, \dots, y_{n})
            &= \int_{0}^{1}\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta^{\sum y_{i} + \alpha - 1}(1-\theta)^{n - \sum y_{i} + \beta - 1}d\theta\\
            &= \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\frac{\Gamma(\sum y_{i} + \alpha)\Gamma(n - \sum y_{i} + \beta)}{\Gamma(n + \alpha + \beta)}
      \end{split}
\end{equation}

Finally, the posterior density of $\theta$, denoted by $g^{*}(\theta \vert y_{1}, y_{2}, \dots, y_{n})$, is given as

\begin{equation}
      \label{eq:probability:bayesian_statistic:bayesian_analysis:step_5}
      \begin{split}
            g^{*}(\theta \vert y_{1}, y_{2}, \dots, y_{n})
            &= \frac{\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta^{\sum y_{i} + \alpha - 1}(1-\theta)^{n - \sum y_{i} + \beta - 1}}{\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\frac{\Gamma(\sum y_{i} + \alpha)\Gamma(n - \sum y_{i} + \beta)}{\Gamma(n + \alpha + \beta)}}\\
            &= \frac{\Gamma(n + \alpha + \beta)}{\Gamma(\sum y_{i} + \alpha)\Gamma(n - \sum y_{i} + \beta)}\theta^{\sum y_{i} + \alpha - 1}(1-\theta)^{n - \sum y_{i} + \beta - 1}
      \end{split}
\end{equation}

The posterior density has the same functional form, $\mathcal{A}(v)$, as the prior, yielding the update on prior parameters as $\alpha' = \sum y_{i} + \alpha$ and $\beta' = n - \sum y_{i} + \beta$.


\section{Summary}\label{sec:probability:summary}

This chapter provided all the necessary background information on probability theory and statistics. The origins of statistical analysis were discussed and the differences between the frequentist and Bayesian view of statistics were discussed in detail. Probability distributions and proofs of conjugate priors were provided with detailed mathematical descriptions. Finally, a detailed description of Bayesian analysis was provided with detailed mathematical descriptions.

This chapter concludes all the relevant background information that is needed to formulate the \acs{BHH}. The proposed \acs{BHH} is presented in the following chapter.