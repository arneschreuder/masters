\chapter{Backpropagation}\label{app:backpropagation}

This appendix provides a full breakdown of the \acs{BP} algorithm as is provided by \citeauthor{ref:engelbrecht:2007} \cite{ref:engelbrecht:2007}.

In order to present the example for \ac{BP} as applied to \acp{FFNN}, a number of assumptions are made. These include:

\begin{itemize}
      \item The \ac{SSE} is used as the objective function.
      \item The sigmoid activation function is used in both the input and output layers.
      \item Biases are included through augmented vectors as was presented in Chapter \ref{chap:anns}
      \item \Acp{SU} are used in the input and output layers.
      \item Stochastic learning is used where a small subset of the training patterns, called a \textit{mini-batch}, is presented at a time (\ac{SGD} is discussed in further detail later in Section \ref{sec:heuristics:gd:sgd}).
\end{itemize}

Then, for each training pattern $z_{p}$ the output of the objective function is given in Equation \ref{eq:heuristics:gd:bp:sse} below.

\begin{equation}
      \label{eq:heuristics:gd:bp:sse}
      \epsilon_{p} = \frac{1}{2}\left(\frac{\sum^{K}_{k=1}(t_{k,p} - o_{k,p})^{2}}{K}\right)
\end{equation}

In Equation \ref{eq:heuristics:gd:bp:sse}, $K$ represents the number of output units and $t_{k,p}$ and $o_{k,p}$ are respectively the target and output values of the $k$-th output unit.

The rest of the derivations are presented only for a single pattern. The subscript $p$ is thus omitted for convenience. The output of each layer is given as

\begin{equation}
      \label{eq:heuristics:gd:bp:output_1}
      o_{k} = f_{o_{k}}(net_{o_{k}}) = \frac{1}{1 + e^{-net_{o_{k}}}}
\end{equation}

and

\begin{equation}
      \label{eq:heuristics:gd:bp:output_2}
      y_{k} = f_{y_{k}}(net_{y_{k}}) = \frac{1}{1 + e^{-net_{y_{k}}}}
\end{equation}

The weights of the \ac{FFNN} are then updated according to Equations \ref{eq:heuristics:gd:bp:output_hto} and \ref{eq:heuristics:gd:bp:output_ith} below.

\begin{equation}
      \label{eq:heuristics:gd:bp:output_hto}
      w_{kj}(t) \hspace{3mm} += \hspace{3mm} \Delta w_{kj}(t) + \alpha \Delta w_{kj}(t - 1)
\end{equation}

\begin{equation}
      \label{eq:heuristics:gd:bp:output_ith}
      v_{ji}(t) \hspace{3mm} += \hspace{3mm} \Delta v_{ji}(t) + \alpha \Delta v_{ji}(t - 1)
\end{equation}

In Equations \ref{eq:heuristics:gd:bp:output_hto} and \ref{eq:heuristics:gd:bp:output_ith} $\alpha$ represents the \textit{momentum} and is discussed in more detail later in Section \ref{sec:heuristics:gd:momentum}. The remainder of the derivations are focused on calculating $\Delta w_{kj}(t)$ and $\Delta v_{ji}(t)$. As such, the reference to time $(t)$ is omitted for convenience.

From Equation \ref{eq:heuristics:gd:bp:output_hto} it follows that

\begin{equation}
      \label{eq:heuristics:gd:bp:partial_output}
      \frac{\partial o_{k}}{\partial net_{o_{k}}} = \frac{\partial f_{o_{k}}}{\partial net_{o_{k}}} = (1 - o_{k})o_{k} = f'_{o_{k}}
\end{equation}

and

\begin{equation}
      \label{eq:heuristics:gd:bp:partial_hidden}
      \frac{\partial net_{o_{k}}}{\partial w_{kj}} = \frac{\partial}{\partial w_{jk}}\left( \sum^{J+1}_{j=1}w_{kj}y_{j}\right) = y_{j}
\end{equation}

In Equation \ref{eq:heuristics:gd:bp:partial_output} $f'_{o_{k}}$ represents the derivative of the corresponding activation function. From Equations \ref{eq:heuristics:gd:bp:partial_output} and \ref{eq:heuristics:gd:bp:partial_hidden}, using the chain-rule, it follows that


\begin{equation}
      \label{eq:heuristics:gd:bp:partial_output_hidden}
      \begin{split}
            \frac{\partial o_{k}}{\partial w_{kj}}
            &= \frac{\partial o_{k}}{\partial net_{o_{k}}}\frac{\partial net_{o_{k}}}{\partial w_{kj}}\\
            &= (1 - o_{k})o_{k}y_{j}\\
            &= f'_{o_{k}}y_{j}
      \end{split}
\end{equation}

From Equation \ref{eq:heuristics:gd:bp:sse} it follows that

\begin{equation}
      \label{eq:heuristics:gd:bp:partial_error}
      \frac{\partial \epsilon}{\partial o_{k}} = \frac{\partial}{\partial o_{k}}\left( \frac{1}{2} \sum^{K}_{k=1}(t_{k} - o_{k})^{2} \right) = -(t_{k} - o_{k})
\end{equation}

Define the error signal that needs to be back-propagated as $\delta_{o_{k}} = \frac{\partial \epsilon}{\partial net_{o_{k}}}$. Then, from Equations \ref{eq:heuristics:gd:bp:partial_output} and \ref{eq:heuristics:gd:bp:partial_error} it follows that


\begin{equation}
      \label{eq:heuristics:gd:bp:delta_output}
      \begin{split}
            \delta_{o_{k}}
            &= \frac{\partial \epsilon}{\partial net_{o_{k}}}\\
            &= \frac{\partial \epsilon}{\partial o_{k}}\frac{\partial o_{k}}{\partial net_{o_{k}}}\\
            &= -(t_{k} - o_{k})(1 - o_{k})o_{k}\\
            &= -(t_{k} - o_{k})f'_{o_{k}}
      \end{split}
\end{equation}

The changes in the hidden-to-output weights are then computed from Equations \ref{eq:heuristics:gd:bp:partial_output_hidden}, \ref{eq:heuristics:gd:bp:partial_error} and \ref{eq:heuristics:gd:bp:delta_output} as follows.

\begin{equation}
      \label{eq:heuristics:gd:bp:delta_hidden_to_output}
      \begin{split}
            \Delta w_{kj}
            &= \eta \left( - \frac{\partial \epsilon}{\partial w_{kj}} \right)\\
            &= -\eta \frac{\partial \epsilon}{\partial o_{k}}\frac{\partial o_{k}}{\partial w_{kj}}\\
            &= -\eta \delta_{o_{k}}y_{j}
      \end{split}
\end{equation}

Continuing on with the input-to-hidden weights as follows

\begin{equation}
      \label{eq:heuristics:gd:bp:partial_input_to_hidden_part_1}
      \begin{split}
            \frac{\partial y_{j}}{\partial net_{y_{j}}}
            &= \frac{\partial f_{y_{j}}}{\partial net_{y_{j}}}\\
            &= (1 - y_{j})y_{j}\\
            &= f'_{y_{j}}
      \end{split}
\end{equation}

and

\begin{equation}
      \label{eq:heuristics:gd:bp:partial_input_to_hidden_part_2}
      \begin{split}
            \frac{\partial net_{y_{j}}}{\partial v_{ji}}
            &= \frac{\partial}{\partial v_{ji}}\left( \sum^{I+1}_{i=1}v_{ji}z_{i}\right)\\
            &= z_{i}
      \end{split}
\end{equation}


From Equations \ref{eq:heuristics:gd:bp:partial_input_to_hidden_part_1} and \ref{eq:heuristics:gd:bp:partial_input_to_hidden_part_2} it follows that

\begin{equation}
      \label{eq:heuristics:gd:bp:partial_input_to_hidden_part_3}
      \begin{split}
            \frac{\partial y_{j}}{\partial v_{ji}}
            &= \frac{\partial y_{j}}{\partial net_{y_{j}}}\frac{\partial net_{y_{j}}}{\partial v_{ji}}\\
            &= (1 - y_{j})y_{j}z_{i}\\
            &= f'_{y_{j}}z_{i}
      \end{split}
\end{equation}

and

\begin{equation}
      \label{eq:heuristics:gd:bp:partial_input_to_hidden_part_4}
      \begin{split}
            \frac{\partial net_{o_{k}}}{\partial y_{j}}
            &= \frac{\partial}{\partial y_{j}}\left( \sum^{J+1}_{j=1}w_{kj}y_{j}\right)\\
            &= w_{kj}
      \end{split}
\end{equation}

From Equations \ref{eq:heuristics:gd:bp:delta_hidden_to_output} and \ref{eq:heuristics:gd:bp:partial_input_to_hidden_part_4} it follows that

\begin{equation}
      \label{eq:heuristics:gd:bp:partial_input_to_hidden_part_5}
      \begin{split}
            \frac{\partial \epsilon}{\partial y_{j}}
            &= \frac{\partial}{\partial y_{j}}\left( \frac{1}{2} \sum^{K}_{k=1} (t_{k} - o_{k})^{2} \right)\\
            &= \sum^{K}_{k=1}\frac{}{} \frac{\partial \epsilon}{\partial o_{k}} \frac{\partial o_{k}}{\partial net_{o_{k}}} \frac{\partial net_{o_{k}}}{\partial y_{j}}\\
            &= \sum^{K}_{k=1} \frac{\partial \epsilon}{\partial net_{o_{k}}} \frac{\partial net_{o_{k}}}{\partial y_{j}}\\
            &= \sum^{K}_{k=1} \delta_{o_{k}}w_{kj}
      \end{split}
\end{equation}

Define the hidden layer error which needs to be back-propagated from Equations \ref{eq:heuristics:gd:bp:partial_input_to_hidden_part_1} and \ref{eq:heuristics:gd:bp:partial_input_to_hidden_part_5} as

\begin{equation}
      \label{eq:heuristics:gd:bp:delta_hidden}
      \begin{split}
            \delta_{y_{j}}
            &= \frac{\partial \epsilon}{\partial net_{y_{j}}}\\
            &= \frac{\partial \epsilon}{\partial y_{j}} \frac{\partial y_{j}}{\partial net_{y_{j}}}\\
            &= \sum^{K}_{k=1}\delta_{o_{k}}w_{kj}f'_{y_{j}}
      \end{split}
\end{equation}

Finally, the changes to the input-to-hidden weights can then be calculated from Equations \ref{eq:heuristics:gd:bp:partial_input_to_hidden_part_3}, \ref{eq:heuristics:gd:bp:partial_input_to_hidden_part_5} and \ref{eq:heuristics:gd:bp:delta_hidden} as follows

\begin{equation}
      \label{eq:heuristics:gd:bp:delta_input_to_hidden}
      \begin{split}
            \Delta v_{ji}
            &= \eta \left( - \frac{\partial \epsilon}{\partial v_{ji}} \right)\\
            &= -\eta \frac{\partial \epsilon}{\partial y_{j}} \frac{\partial y_{j}}{\partial v_{ji}}\\
            &= -\eta \delta_{y_{j}}z_{i}
      \end{split}
\end{equation}

If the \ac{FFNN} include direct weights from the input to output layer, the following additional weight updates are required.

\begin{equation}
      \label{eq:heuristics:gd:bp:delta_input_to_output}
      \begin{split}
            \Delta u_{ki}
            &= \eta \left( - \frac{\partial \epsilon}{\partial u_{ki}} \right)\\
            &= -\eta \frac{\partial \epsilon}{\partial o_{k}} \frac{\partial o_{k}}{\partial u_{ki}}\\
            &= -\eta \delta_{o_{k}}z_{i}
      \end{split}
\end{equation}

In Equation \ref{eq:heuristics:gd:bp:delta_input_to_output} above, $u_{ki}$ is a weight from the $i$-th input unit to the $k$-th output unit.

The derivations above assumed stochastic learning, however, if batch learning is applied, weight updates are given in Equations \ref{eq:heuristics:gd:bp:delta_hidden_to_output_batch_learning} and \ref{eq:heuristics:gd:bp:delta_input_to_hidden_batch_learning} below

\begin{equation}
      \label{eq:heuristics:gd:bp:delta_hidden_to_output_batch_learning}
      \begin{split}
            \Delta w_{kj}(t) = \sum^{P_{T}}_{p=1} \Delta w_{kj,p}(t)
      \end{split}
\end{equation}

\begin{equation}
      \label{eq:heuristics:gd:bp:delta_input_to_hidden_batch_learning}
      \begin{split}
            \Delta v_{kj}(t) = \sum^{P_{T}}_{p=1} \Delta v_{kj,p}(t)
      \end{split}
\end{equation}

where $\Delta w_{kj}(t)$ and $\Delta v_{kj}(t)$ are weight updates for individual patterns $p$ and $P_{T}$ is the total number of patterns in the training set.

Equations \ref{eq:heuristics:gd:bp:sse} through \ref{eq:heuristics:gd:bp:delta_input_to_hidden} provided the reader with the detailed derivations of the \ac{BP} weight-update components as it resolves from the error signal. The following sections provide the reader with variants of the \ac{BP} algorithm as presented in this section.
