## TODO

- [x] there is a mistake with discounted rewards, since step 0 is furthest back. Thus, we need to take len - step
- [x] fw.credits.PBest
- [x] fw.credits.GBest
- [x] fw.credits.RBest
- [x] multiple credits together -> sum (bhh)
- [x] fw.credits.Symmetric
- [x] there is a mistake with multiple credits, should not recount observations of heuristics and entities, only sum credit.
- [x] loss change average to sum -> This is a problem with SGD -> Fixed, reduction was wrong.
- [x] log-sum-exp -> Fixed by using log, no need for log-sum-exp
- [x] split sgd, momentum and nag
- [x] learning rate schedules
- [x] readd learning rate for PSO
- [ ] adagrad, sgd, momentum do not all update all params. Need to find way to update missing
- [x] SGD
- [x] Momentum
- [x] NAG
- [x] ADAGRAD
- [ ] ADADELTA
- [ ] ADAMAX
- [ ] RMSPROP
- [ ] ADAM
- [ ] NADAM
- [ ] GA
- [ ] DE
- [ ] CMA-ES
- [x] PSO
- [ ] QuantumPSO or Heterogeneous PSO?
- [ ] evaluate all at step 0, allowing an evaluation, before any steps. All losses should start at ln(1/classes) then.
- [ ] counts -> should start with alpha, priors parameters -> initialisers, see bhh l.58
- [ ] Comment credit source files
- [ ] Comment bhh source files
- [ ] change steps/epochs -> proportion of training time
- [ ] tensorboard selection probabilities
- [ ] tensorboard selections
- [ ] credits from performance log is extremely inefficient
- [ ] flatten and reshape of model params is extremely inefficient
- [ ] bhh hyper-params on schedule? -> do after Adam with LR decay See: tf.keras.optimizers.schedules.LearningRateSchedule
- [ ] more datasets
- [ ] optimisation
- [ ] training, test, validation
- [ ] main + experiment -> cli
- [ ] ensure all heuristics are always selected, despite model
- [ ] ensure all heuristics prerequisite data are met
- [ ] prepare experiments
- [ ] test scripts
- [ ] access hpc
- [ ] run experiments
- [ ] gather data
- [ ] do statistical analysis
- [ ] prepare results
