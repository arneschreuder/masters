\section{Introduction}
\label{sec:introduction}


The performance and capabilities of \acp{ANN} is largely influenced by the learning process used. As such, a popular field of focus for studying \acp{FFNN} is the process by which these models are trained. The learning process consists of multiple components. Each of these elements influence how much a particular learning technique might focus on a particular solution (exploitation), in comparison to seeking out novel solutions (exploration) during training.

Techniques exist to dynamically balance the trade-off between exploration and exploitation during the search process. An example of such a technique is to dynamically adjust and learn the heuristic \index{hyper-parameters}\textit{hyper-parameters} as part of the learning process. This field of study is known as \index{meta-learning}meta-learning~\cite{ref:giraud:2004}.

A recent suggestion related to the field of \index{meta-learning}meta-learning is to dynamically select and/or adjust the heuristic used throughout the training process. This approach focuses on the hybridisation of learning paradigms.

One such form of hybridisation of learning paradigms is referred to as \textit{heterogeneous learning approaches}. Heterogeneous learning approaches make use of different \textit{search behaviours} by selecting from a behaviour pool. Heterogeneous approaches have shown to balance the trade-off between exploration and exploitation~\cite{ref:nepomuceno:2013}.

A step further in the concept of hybridisation of learning paradigms is that of hybridisation of different \textit{heuristics} as they are applied to some optimisation problem~\cite{ref:burke:2013}. These methods are referred to as \acfp{HH} and focus on finding the best heuristic in \textit{heuristic space} to solve a specific problem.

One such form of \acs{HH} is a population-based approach that guides the search process by automatically selecting heuristics from a heuristic-pool to be applied to a collection of different candidate solutions in the solution-space. This collection of candidate solutions are referred to as a \textit{population} of \textit{entities}, where each \textit{entity} is a single candidate solution to the problem being optimised. Population-based \acp{HH} implement a strategy where multiple heuristics work together to solve a problem. This technique requires a mechanism that selects a specific heuristic to be applied to a specific candidate solution.

Finding the best heuristic to use is non-trivial and some \textit{selection strategy} must be used to select the best heuristic. \acp{HH} that implement such a selection strategy are referred to as \textit{selection} \acp{HH}. The term \textit{selection} refers to the ability of the \acs{HH} to select the best heuristic from a pool of low-level heuristics.

\subsection{Problem Statement}\label{subsec:introduction:problem}

Many different heuristics have been developed and used to train \acp{FFNN}~\cite{ref:gudise:2003, ref:rakitianskaia:2012, ref:montana:1989}. Each of these heuristics have different search behaviours, characteristics, strengths and weaknesses. Finding the best heuristic to train the \acs{FFNN} is required in order to yield optimal results. This process is often non-trivial and could be a time-consuming exercise.  Consider that selection of the best heuristic as applied to optimisation problems, such as training \acp{FFNN}, is problem specific~\cite{ref:allen:1996, ref:drake:2020, ref:pillay:2018}.

Careful, systematic selection is thus required to find and select the best heuristic to train \acp{FFNN}. In the past, researchers selected the best heuristic by trial and error. A set of heuristics and carefully selected hyper-parameters would be implemented, followed by an empirical test to evaluate the performance of each heuristic for a given problem domain~\cite{ref:pillay:2015}. In this way, researchers were able to determine which heuristics and which hyper-parameters performed well for different problems. However, this approach is problematic, because it is time-consuming and laborious.

\subsection{Motivation}\label{subsec:introduction:motivation}

It was previously mentioned that finding the best heuristic to train \acp{FFNN} is a timely and tedious process. A modern approach is to use \acp{HH} to automate the process of selecting the best heuristic when applied to some optimisation problem. The best heuristic might not be a single heuristic, but rather a hybridisation of heuristics~\cite{ref:pillay:2015}.

In the general context of optimisation, many different types of \acp{HH} have been implemented and applied to many different problems. Some notable examples include the \index{simulated annealing}simulated annealing-based \acs{HH} by~\citeauthor{ref:dowsland:2007}~\cite{ref:dowsland:2007}, the \index{tabu-search}tabu-search \acs{HH} of~\citeauthor{ref:burke:2010}~\cite{ref:burke:2010}, the \index{heterogeneous meta-hyper-heuristic}heterogeneous meta-hyper-heuristic by~\citeauthor{ref:grobler:2012}~\cite{ref:grobler:2012} and work done by~\citeauthor{ref:vanderstockt:2018}~\cite{ref:vanderstockt:2018} on the analysis of selection hyper-heuristics for population-based \acp{MH} in real-valued dynamic optimization. However, research on the application of \acp{HH} in the context of \acs{FFNN} training is scarce.~\citeauthor{ref:nel:2021}~\cite{ref:nel:2021} provides the first research in this field, applying a \acs{HH} to \acs{FFNN} training.

This research takes a particular interest in developing a selection \acs{HH} that makes use of probability theory and Bayesian statistical concepts to guide the heuristic selection process. This research develops the novel \Acf{BHH}, a new high-level heuristic that utilises a statistical approach, referred to as \index{Bayesian analysis}\textit{Bayesian analysis}, which combines prior information with new evidence to the parameters of a selection probability distribution. This selection probability distribution is the mechanism by which the \acs{HH} selects appropriate heuristics to train \acp{FFNN} during the training process. This process takes place dynamically and during the training process (online learning).

\subsection{Outline}\label{subsec:introduction:outline}

The remainder of this article is structured as follows: Section~\ref{sec:anns} provides a literature study on \acp{ANN}. The focus is on training of \acp{FFNN}. Section~\ref{sec:heuristics} provides details on various types of heuristics and \acp{MH} that have been used to train \acp{FFNN}. Section~\ref{sec:hhs} presents a literature study on the details of \acp{HH} and \index{meta-learning}meta-learning in general. Section~\ref{sec:probability} presents a literature study on probability theory. Section~\ref{sec:bhh} presents the developed \Acs{BHH}. Section~\ref{sec:methodology} presents a detailed description of the empirical process and the setup of each experiment. Section~\ref{sec:results} provides and discusses the results of the empirical study in detail. Section~\ref{sec:conclusion} summarises the research done in this dissertation along with a brief overview of the findings made throughout the research process.
