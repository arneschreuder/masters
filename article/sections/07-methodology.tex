\section{Methodology}
\label{sec:methodology}

This section provides the details of the implementation of the empirical process. Two experimental groups are defined. These include an empirical study to investigate the behavioural characteristics of the \Acs{BHH} as it is used to train \acp{FFNN} on an example problem, as well as an empirical study to critically evaluate the performance of the \Acs{BHH} compared to individual low-level heuristics as they are used to train \acp{FFNN} on a number of different problems.

In the context of training \acp{FFNN}, the underlying model is trained across a number of datasets. Each dataset is split into a training set comprising of 80\% of the data, and a test set comprising of 20\% of the data. Each experiment is repeated over 30 independent runs. Each run contains a different random seed, such that each run is unique. The evaluation data forms the results of these empirical tests. These results are analysed for statistical significance, from which findings and conclusions are then made.

\subsection{Datasets}\label{sec:methodology:datasets}

All the datasets used in the empirical process originate from the UCI Machine Learning Repository~\cite{ref:uci:2022}. Datasets are grouped by problem type and include seven classification and seven regression datasets. The details around the datasets used can be found in Appendix \ref{app:appendix_datasets}. A number of classification datasets contain an unbalanced representation of classes. The empirical process defined in this work does not apply mechanisms to cater for class balancing, in order to eliminate as many variables and factors in the empirical process as possible.

\subsection{Models}\label{sec:methodology:model}

All models that are trained in the empirical process follow implementations of shallow \acp{FFNN}, meaning that they only have one hidden layer. The number of hidden units used were determined empirically. Weights are initialised by means of Glorot uniform sampling~\cite{ref:glorot:2010}. The models and their configuration, as it is used for each dataset, are given in Appendix \ref{app:appendix_models}. \acf{SparseCatXE} and \acf{BinXE} loss functions are used for classification problems, while \acf{MSE} is used for regression problems.


\subsection{Heuristics}\label{sec:methodology:heuristics}

Each of these low-level heuristics are implemented as standalone heuristics and are also included in the heuristic pool of the \acs{BHH}. Appendix \ref{app:appendix_heuristics} contains a list of all the standalone, low-level heuristics that are used as well as their hyper-parameter configurations.

Note from Table \ref{tab:methodology:heuristics} that values that are configured to make use of a decay schedule are presented with the initial value first and the decay rate in brackets next to it. The number of steps is the total number of mini-batch training steps that are executed for that particular dataset.

Furthermore, it should be noted that a learning rate was added to \acs{PSO} as an attempt to avoid overshooting solutions later in the training process. A learning rate parameter does not traditionally form part of \acs{PSO}, but was found to be useful.

Section \ref{sec:bhh:heuristic:proxies} presented the concept of a mapping of proxied heuristic state update operations. The mapping of proxied heuristic state update operations implemented by the \acs{BHH} in the empirical process is given in Figure \ref{fig:methodology:heuristics:proxies}.

In Figure \ref{fig:methodology:heuristics:proxies}, cells containing \textbf{x} indicate that the associated heuristic implements that particular state parameter explicitly, and cells containing \textbf{o} indicate that the state parameter is implicitly implemented as part of the \acs{BHH} algorithm. The required mapping of proxied heuristic state operations is then implemented as a lookup of the table presented in Figure \ref{fig:methodology:heuristics:proxies}.

\subsection{BHH Baseline}\label{sec:methodology:baseline_bhh}

The \acs{BHH} baseline is a name given to a specific configuration of the \acs{BHH} which, during development, has been found to provide a reasonable baseline performance. The baseline configuration is used as the cornerstone configuration from which all other heuristics and their configurations are evaluated. The \acs{BHH} baseline is also used for the behavioural study of the \acs{BHH}. The \acs{BHH} baseline configuration is given in Table \ref{tab:methodology:bhh_baseline_configuration}.

% Table generated by Excel2LaTeX from sheet 'BHH Baseline'
\begin{table}[htb]
	\centering
	\caption{The \acs{BHH} baseline configuration as it is used in the empirical study.}
	\label{tab:methodology:bhh_baseline_configuration}%
	\par\bigskip
	\resizebox{0.65\textwidth}{!}{
		\begin{tabular}{rrcc}
			\multicolumn{1}{c}{\textbf{hyper-heuristic}} & \multicolumn{1}{c}{\textbf{variant}} & \textbf{configuration} & \textbf{value} \\
			\midrule
			\multicolumn{1}{l}{bhh}                      & \multicolumn{1}{l}{baseline}         & heuristic pool         & all            \\
			                                             &                                      & population             & 5              \\
			                                             &                                      & credit                 & ibest          \\
			                                             &                                      & burn in                & 0              \\
			                                             &                                      & reselection            & 10             \\
			                                             &                                      & replay                 & 10             \\
			                                             &                                      & reanalysis             & 10             \\
			                                             &                                      & normalise              & false          \\
			                                             &                                      & discounted rewards     & false          \\
		\end{tabular}%
	}
\end{table}%

In Table \ref{tab:methodology:bhh_baseline_configuration}, the heuristic pool configuration that is used, is referred to as \textit{all}. The \textit{all} configuration refers to a configuration where the heuristic pool contains all the low-level heuristics as presented in Section \ref{sec:methodology:heuristics}, including all gradient-based heuristics and \acp{MH}

\subsection{Performance Measures}\label{sec:methodology:performance_measures}

In this dissertation, \acs{BinXE} is used for classification problems with two classes and \acs{SparseCatXE} is used for classification problems with more than two classes. For the classification problems, accuracy is also measured. Accuracy refers to the proportion of correct classifications. For regression problems, the \acs{RMSE} is used as a performance metric.

The test set is used as a validation set during training. All performance metrics are measured for the training set as well as the test/validation set during training. As such, a divergence in performance metrics between the training and test/validation set is used to evaluate for overfitting during the training process.

After training has been completed, the \textit{average rank}, based on test loss, for all configurations, is calculated at each mini-batch step. A total of 30 independent runs with different random seed configurations are executed for each empirical test. The test loss metric and average rank are then statistically analysed across all runs, and are used to derive findings and conclusions.

\subsection{Stopping Conditions}\label{sec:methodology:stopping_conditions}

For this dissertation, the \textit{maximum epoch} stopping condition is used, where a fixed, maximum number of training steps and epochs are executed and training is not halted until the maximum number of epochs have been reached. \acp{FFNN} are trained for a maximum of 30 epochs. Early stopping is not implemented for the empirical process in this dissertation.


\subsection{Experiments}
\label{sec:methodology:experiments}

This section presents the experimental groups that are executed in the empirical process. Three main experimental groups are formulated. Each of these are discussed in more detail in the following sections.


\subsubsection{Behavioural Case Study}
\label{sec:methodology:experiments:case_study}

The behavioural case study analyses the behaviour of the \acs{BHH} baseline configuration as it relates to an example problem dataset, across 30 independent runs. The example problem dataset is the iris dataset, presented in Table \ref{tab:methodology:datasets:classification}. The behavioural case study is meant to provide an introductory analysis of the inner workings of the \acs{BHH} and includes analysis of the selection mechanism, as well as the perturbative nature of the \acs{BHH}. The behavioural case study is used to determine if the \acs{BHH} is learning and that selection is indeed biasing towards better performance. These observations also provide an opportunity to observe the outcome of the perturbative nature of the \acs{BHH}, which includes proxied heuristic update step operations.

The behavioural case study provides an implementation of the \acs{BHH} baseline which, by default, has a replay window size of 10. The baseline configuration is provided and compared to an implementation of a \acs{BHH} configuration where the replay window size is large (250), as well as an implementation of the \acs{BHH} using the \textit{symmetric} credit assignment strategy. The large replay window size configuration is used to show the behaviour of the \acs{BHH} when it has access to a large number of performance log samples, which increases the statistical certainty of the learning outcome. The \acs{BHH} configuration that uses the \textit{symmetric} credit assignment strategy is used to illustrate the behaviour of the \acs{BHH} where no performance bias takes place and thus, no learning occurs.

\subsubsection{Standalone Heuristics}
\label{sec:methodology:experiments:standalone_optimisers}

For the standalone heuristics experimental group, the heuristics, as presented in Section \ref{sec:methodology:heuristics}, are used along with their specified hyper-parameters. Each of these standalone low-level heuristics are compared to that of the \acs{BHH} baseline configuration, across all datasets.

The intent of the standalone heuristics experimental group is to determine if the \acs{BHH} baseline configuration can generalise to multiple problems in comparison to individual low-level heuristics.

Additional to the \acs{BHH} baseline configuration, two more \acs{BHH} configurations are included. These include \acs{BHH} configurations where the heuristic pool only makes use of gradient-based heuristics, and a configuration where the heuristic pool only makes use of \acp{MH}. The intent behind the inclusion of these configurations is to determine the effectiveness of multi-method approaches in the heuristic pool as it applies to training \acp{FFNN}.

\subsection{Statistical Analysis}
\label{sec:methodology:statistical_analysis}

The results from each of these experimental groups are statistically analysed. Eeach experiment and configuration is trained for 30 epochs and is repeated over 30 runs, for each of the fourteen datasets. The experimental results contain performance evaluation data for the training and testing datasets, and includes loss and accuracy measurements. Statistical analysis is executed on the results from the testing datasets. An average rank is calculated across all 30 runs, for all experimental groups and configurations, at each step, for every epoch of training. Each run is executed using a unique random seed, such that each run is identical in its setup and configuration, but unique between runs.

The evaluation of outcome is based on the aggregated statistical results as mentioned above. A descriptive analysis is executed on the spread of the data. The results are analysed and checked for overfitting and outliers are identified. The skewness of the results is evaluated per dataset and the Shapiro-Wilk test for normality~\cite{ref:shapiro:1965} ($\alpha$ = 0.001) is used to determine if the results are normally distributed. Furthermore, the Levene's test for equality of variance~\cite{ref:levene:1961} ($\alpha$ = 0.001) is used. The output of the full statistical analysis is presented in Appendix \ref{app:statistical_analysis}.

Dependent on the outcomes of the above statistical tests, the appropriate statistical significance test is then executed. For experiments where there are only two configurations, the Mann-Whitney U independent samples t-Test~\cite{ref:mann:1947} ($\alpha$ = 0.001) is executed. For experiments with three or more configurations, the \acs{ANOVA} statistical test~\cite{ref:fisher:1921} ($\alpha$ = 0.001) is used. The Kruskal-Wallis ranked non-parametric test~\cite{ref:kruskal:1952} for statistical significance ($\alpha$ = 0.001) is used for cases where data is not normally distributed.

Regardless of the statistical test that is used, a post-hoc Tukey honest significant difference test~\cite{ref:tukey:1949} ($\alpha$ = 0.001) is used from which significant ranking is retrieved. Descriptive and critical difference plots are then retrieved from these results to provide visual aid.

\subsection{Implementation and Execution}\label{sec:methodology:implementation}

All implementations are done from first principles in Python 3.9 using Tensorflow 2.7 and Tensorflow Probability 0.15.0. Most underlying mathematical functions are reused from the Tensorflow library, however, all heuristics are implemented from first principles to fit the \acs{HH} framework that was developed. The source code and data for this dissertation is provided and made public at \url{https://github.com/arneschreuder/masters}.

It should be noted that this implementation makes heavy use of CPU processing, due to flattening of the \acs{FFNN} weights by the heuristics. For this reason, execution is much more timely and costly than with GPU training.

All experiments were run on the Centre for High Performance Computing (CHPC) cluster. A total of 14 different server were used, each running 24 to 56 cores and 256GB of memory. The entire empirical process took six days.
