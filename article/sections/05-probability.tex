\section{Probability}
\label{sec:probability}

In general, there are two main views to probability and statistics. These include the \textit{frequentist} and the \textit{Bayesian} view of statistics. Naturally, Bayesian statistics is based on Bayes' theorem, which is given as

\begin{equation}
	\label{eq:bayes}
	P(A \vert B) = \frac{P(B \vert A)P(A)}{P(B)}
\end{equation}

Bayesian statistics describe the probability of an event in terms of some belief, based on previous knowledge of the event and the conditions under which the event happened~\cite{ref:hackenberger:2019}. Bayes' theorem expresses how a degree of belief, expressed as a probability, should rationally change to account for the availability of related evidence.

One of the many applications of Bayes' theorem is to do statistical inference. Like \acp{FFNN}, Bayesian models need to be \textit{trained}, a process known as \textit{Bayesian analysis}. Bayesian analysis is the process by which prior beliefs are updated as a result of observing new data/evidence.

Bayesian analysis utilises the the concept of conjugate priors. \citeauthor{ref:wackerly:2014}\cite{ref:wackerly:2014} state that conjugate priors are prior probability distributions that result in posterior distributions that are of the same functional form, $\mathcal{A}(v)$, as the prior, but with different parameter values. The conjugate prior to a Bernoulli probability distribution is the Beta probability distribution The conjugate prior to a Categorical and Multinomial probability distribution is the Dirichlet probability distribution~\cite{ref:wackerly:2014}.
