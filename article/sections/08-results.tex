\section{Results}
\label{sec:results}

This section provides the results of the experimental groups that have been conducted. Detailed discussions follow on the outcomes of each experiment. Discussions are accompanied by figures and plots to help provide visual aid for discussions. Where possible, a number of improvements to the \acs{BHH} are identified and recommended as it relates to the outcomes of the results.

\subsection{Overview}\label{sec:results:overview}

This section provides a brief discussion on the general outcome of the empirical process as a whole and identifies some key aspects to be kept in mind when interpreting the results of the experiments.

Firstly, the \acs{BHH} applies a form of online learning. As such, the \acs{BHH} applies the learning mechanism during training in a single run of the training process. The training process is not repeated iteratively as is the case with some \acp{HH}.

Most of the training progress is observed to occur within the first five epochs. As a result, the \acs{BHH} should apply most learning at the early stages of the training process. After five epochs, the training of most of the underlying \acp{FFNN} converges and little performance gain is observed after that point. Since this empirical process does not apply early stopping of the training process, the \acs{BHH} will continue to explore the heuristic space beyond the five epoch mark.

The \acs{BHH} does not implement a type of move-acceptance strategy where the application of a heuristic to an entity is only accepted if it leads to a better solution. In some cases, the \acs{BHH} then selects heuristics that yield sub-optimal results, but is shown to mostly return to optimal solutions over a number of steps.

Given the stochastic nature of the heuristic selection mechanism, sufficient samples of the performance of each heuristics-entity combination in the performance log is required for the \acs{BHH} to learn. This requirement is further strengthened by the Bayesian nature of the probabilistic model implemented by the \acs{BHH}. The probabilistic model implements \textit{probability distributions of heuristic selection probabilities} and as such, insufficient samples in the performance log could render a form of random search.

By default, the \acs{BHH} baseline configuration has a reanalysis interval of 10, and a replay window size of 10, which is a small window to learn from. Despite the small reanalysis interval and the small replay window size, it should be observed that the \acs{BHH} exploits small performance biases regardless and does find small performance gains throughout.

\subsection{Behavioural Case Study}\label{sec:results:case_study}

This section provides the results for the behavioural case study on the \acs{BHH}. The first point of interest is to determine if the \acs{BHH} is able to successfully train the underlying \acs{FFNN}.

\begin{figure}[H]
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{case_study/metrics/figures/train/train_loss.pdf}
		\caption{Train log loss}
		\label{fig:results:case_study:metrics:train_loss}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{case_study/metrics/figures/test/test_loss.pdf}
		\caption{Test log loss}
		\label{fig:results:case_study:metrics:test_loss}
	\end{subfigure}
	\par\bigskip
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{case_study/metrics/figures/train/train_accuracy.pdf}
		\caption{Train log accuracy}
		\label{fig:results:case_study:metrics:train_accuracy}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{case_study/metrics/figures/test/test_accuracy.pdf}
		\caption{Test log accuracy}
		\label{fig:results:case_study:metrics:test_accuracy}
	\end{subfigure}
	\par\bigskip
	\caption{The average train and test loss and accuracy plots over 30 epochs, obtained from 30 runs of the case study on the behaviour of the \acs{BHH} on the iris dataset, illustrated in log scale.}
	\label{fig:results:case_study:metrics}
\end{figure}

Figures \ref{fig:results:case_study:metrics:train_loss} to \ref{fig:results:case_study:metrics:test_accuracy} provide illustrations of the train and test loss and accuracy plots of the \acs{BHH} over 30 epochs, obtained from 30 runs of the case study on the behaviour of the \acs{BHH} on the iris dataset, illustrated in log scale.

The first logical observation that can be made is that the \acs{BHH} was indeed able to successfully train the underlying \acs{FFNN}, observed by the convergence of the training process, yielding good results. Figures \ref{fig:results:case_study:metrics:train_accuracy} and \ref{fig:results:case_study:metrics:test_accuracy} show that the trained \acs{FFNN} achieved an accuracy of almost 100\%.

To illustrate the learning process undergone by the \acs{BHH}, further investigation is required. This can be achieved by analysing the outcomes of the change in concentration parameters and the probabilitiy distributions implemented by the \acs{BHH}. Figures \ref{fig:results:case_study:alphas:6} to \ref{fig:results:case_study:p_HgEC:0:6} provide illustrations of the change in concentration parameter, $\boldsymbol{\alpha}$, and the effect is has on the probability distributions of heuristic selection probabilities, denoted $P(\boldsymbol{\theta} \vert \boldsymbol{\alpha})$, the prior heuristic selection probability distribution, denoted $P(\boldsymbol{H} \vert \boldsymbol{\theta})$, and the posterior heuristic selection probability distribution, denoted $P(\boldsymbol{H} \vert \boldsymbol{E}, \boldsymbol{C}; \boldsymbol{\theta}, \boldsymbol{\phi}, \boldsymbol{\psi})$. These illustrations represent an example heuristic, \acs{Adam}, and an example entity, identified by indices 6 and 0 respectively in the heuristic and entity pools. The illustrations are obtained from 30 runs of the case study on the behaviour of the \acs{BHH} on the iris dataset, illustrated in log scale. Similar illustrations for the rest of the concentration parameters and probability distributions are left out for brevity as they contain similar illustrations. The following sections provide interpretations of each of these concepts.

\begin{figure}[H]
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{case_study/params/figures/alphas/alpha[6].pdf}
		\caption{$\alpha_{6}$ - \acs{Adam}}
		\label{fig:results:case_study:alphas:6}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{case_study/params/figures/thetas/theta[6].pdf}
		\caption{P($\theta_{6} \vert \alpha_{6})$ - \acs{Adam}}
		\label{fig:results:case_study:thetas:6}
	\end{subfigure}
	\par\bigskip
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{case_study/params/figures/p_H/p_H[6].pdf}
		\caption{P($h_{6} \vert \theta_{6})$ - \acs{Adam}}
		\label{fig:results:case_study:p_H:6}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{case_study/params/figures/p_HgEC/p_HgEC[0][6].pdf}
		\caption{$P(h_{6} \vert e_{0}, c_{1})$ - \acs{Adam}}
		\label{fig:results:case_study:p_HgEC:0:6}
	\end{subfigure}
	\par\bigskip
	\caption{Illustrations of the change in concentration parameter and probability distributions for an example heuristic, \acs{Adam} (index 6), and an example entity (index 0), in the heuristic and entity pools.}
	\label{fig:results:case_study}
\end{figure}

\subsubsection{Alphas}\label{sec:results:case_study:alphas}

The concentration parameter, $\boldsymbol{\alpha}$, parameterises the Dirichlet probability distribution, denoted $P(\boldsymbol{\theta} \vert \boldsymbol{\alpha})$. The probability distribution, $P(\boldsymbol{\theta} \vert \boldsymbol{\alpha})$, is used to sample prior \textit{heuristic selection probabilities}.

The first logical observation to be made from Figure \ref{fig:results:case_study:alphas:6} is the step-wise, increasing nature of $\boldsymbol{\alpha}$ for the bhh\_replay\_250 configuration, illustrated in \textcolor{green}{green}. Since the replay window size is sufficiently large to contain the performance log for all the steps executed in the training process, the \acs{BHH} does not forget past performances of low-level heuristics at all. The value for $\boldsymbol{\alpha}$ is never reset to its initial value of 1.0, and thus, $\boldsymbol{\alpha}$ continues to increase throughout the training process.

The next observations that can be made are for the bhh\_baseline configuration, illustrated in \textcolor{red}{red}, and the bhh\_credit\_symmetric configuration in \textcolor{blue}{blue}. Both these configurations see $\boldsymbol{\alpha}$ being reset to its initial value of 1.0 as regular intervals. This interval is defined by the \textit{reanalysis interval} hyper-parameter. The reanalysis interval dictates the frequency at which Bayesian analysis is conducted on the performance log, maintained by the \acs{BHH}. Bayesian analysis is used to update $\boldsymbol{\alpha}$ only at the reanalysis interval. As a result, small plateaus appear where $\boldsymbol{\alpha}$ does not change.

It is important to mention that a change in $\boldsymbol{\alpha}$ between reanalysis windows does not yet necessarily indicate that the \acs{BHH} is learning. The concentration parameter $\boldsymbol{\alpha}$ tracks the \textit{occurrence} of the random event of observing heuristics (denoted $\boldsymbol{{H}}$). Heuristics can be observed as a result of good performance, by which the \acs{BHH} then learns to frequently reselect these heuristics again, or just by chance through the stochastic nature of probabilistic sampling as implemented by the \acs{BHH}. Further investigation is required to illustrate the learning mechanism of the \acs{BHH}.

\subsubsection{Probability Distribution of Heuristic Selection Probabilities}\label{sec:results:case_study:probabilities_of_probabilities}

The probability distribution of heuristic selection probabilities is denoted by $P(\boldsymbol{\theta} \vert \boldsymbol{\alpha})$, where $\boldsymbol{\theta} \sim Dir(\boldsymbol{\alpha, K})$ are the sampled heuristic selection probabilities. From the illustration presented in Figure \ref{fig:results:case_study:thetas:6}, a clearer picture of the learning process of the \acs{BHH} is formed.

The first important observation to make is for the bhh\_replay\_250 configuration, presented in \textcolor{green}{green}. As a reminder, ten low-level heuristics are included in the heuristic pool, yielding an expected mean heuristics selection probability of 0.1, for each heuristic, by the frequentist view of probabilistic modelling. Towards the end of the training process, the heuristic selection probability converges back to the symmetrical, uniform probability distribution, yielding a heuristic selection probability of 0.1, for all heuristics. This can be explained as follows: most of the training progress is made in the early stages of the training process, and training converges towards the end of the training process. Since training converges, all heuristics, no matter their past performances, fail to yield better solutions towards the end of the training process. As a result of training convergence, heuristics then fail to meet the performance criteria and credit allocations by means of the credit assignment strategy.

Both the bhh\_baseline (\textcolor{red}{red}) and the bhh\_replay\_250 (\textcolor{green}{green}) configurations make use of the \textit{ibest} credit assignment strategy. The \textit{ibest} credit assignment strategy allocates credit to the heuristic that yields the best performance for the current iteration/step and thus, towards the end of the training process, any random heuristic can yield the best iteration performance. However, since the bhh\_baseline is configured with a small replay window size of 10, and a reanalysis interval of 10, the concentration parameter, $\boldsymbol{\alpha}$, is reset to its default value of 1.0 more often than the bhh\_replay\_250 configuration, resulting in a probability distribution that is broader, and thus, explaining the larger variance of $\boldsymbol{\theta}$ throughout.

It should be noted that, despite using the same random seed across configurations for the same run number, the behavioural changes between the configurations start to show after about 30 steps. As a reminder, the bhh\_credit\_symmetric (\textcolor{blue}{blue}) configuration, does not bias towards best performing heuristics. Where the bhh\_baseline and bhh\_replay\_250 configurations then diverge from the behaviour of the bhh\_credit\_symmetric configuration is proof of the effect of performance bias.

Furthermore, it can be observed for small windows, at various steps for multiple runs, that the variance of $\boldsymbol{\theta}$, for the bhh\_baseline configuration and the bhh\_credit\_symmetric configuration, does not yield means that are equal to the expected heuristic selection probabilities of 0.1. This is proof that the \acs{BHH} does not just implement a form of random search, despite having small reanalysis interval and replay window size configurations. This is also true for the bhh\_credit\_symmetric configuration, as the bhh\_credit\_symmetric configuration biases towards heuristics that happen to be sampled, despite not biasing towards good performance.

Finally, the bhh\_baseline configuration and the bhh\_credit\_symmetric configuration both yield similar volatile behaviour, much more so than with the bhh\_replay\_250 configuration. This can be attributed to a very small reanalysis window combined with a small replay window size of 10, that contains very few samples to learn from. A small reanalysis interval and a small replay window size allows for more exploration of the heuristic space, but can also yield greater variance of the heuristic selection probabilities. Once again, any differences then in the behaviour of the bhh\_baseline compared to the bhh\_credit\_symmetric configurations is proof of small performance exploitations and biases by the bhh\_baseline configuration.

\subsubsection{Prior Heuristic Selection Probabilities}\label{sec:results:case_study:priors}

The prior heuristic selection probability distribution is denoted $P(\boldsymbol{H} \vert \boldsymbol{\theta})$. As such, $P(\boldsymbol{H} \vert \boldsymbol{\theta}) = \boldsymbol{\theta}$ and heuristics are initially sampled such that $\boldsymbol{H} \sim Cat(\boldsymbol{\theta})$. Figure \ref{fig:results:case_study:p_H:6} provide an illustrations of the prior heuristic selection probabilities for \acs{Adam} throughout the training process.

The main observation to make from these figures is that they are much less volatile and noisy than the figures presented for the distribution of heuristic selection probabilities, $\boldsymbol{\theta}$, presented in the previous section. This is because of the \textit{reselection} interval hyper-parameter. The reselection interval hyper-parameter is implemented as a way to control the frequency by which new heuristics are selected and allocated to each entity. Since the default \textit{reselection} interval is set to 10, the heuristic selection probabilities are only resampled at intervals of 10. Figure \ref{fig:results:case_study:p_H:6} then simply yields a rough approximation of the illustration provided in Figure \ref{fig:results:case_study:thetas:6}. As such, the same observations and conclusions that are made in Section \ref{sec:results:case_study:probabilities_of_probabilities} also apply in this section.

Finally, it should be mentioned that at each step, the goal of the \acs{BHH} is to update these prior ``beliefs'' based on newly observed evidence of heuristic performances. Since these prior heuristic selection probabilities change over time, it can be concluded that the change in prior heuristic selection probabilities is a result of the learning mechanism of the \acs{BHH}. Furthermore, the prior heuristic selection probability distribution provides an opportunity to utilise prior knowledge by some expert before training starts, by injecting heuristic selection biases, by means of the initial values for the concentration parameter, $\boldsymbol{\alpha}$.

\subsubsection{Posterior Heuristic Selection Probabilities}\label{sec:results:case_study:posteriors}

The posterior heuristic selection probability distribution is defined as $P(\boldsymbol{H} \vert \boldsymbol{E}, \boldsymbol{C}; \boldsymbol{\theta}, \boldsymbol{\phi}, \boldsymbol{\psi})$. Figure \ref{fig:results:case_study:p_HgEC:0:6} provides an illustration of the calculated posterior heuristic selection probabilities implemented by the \acs{BHH}.

The main observation to make from Figure \ref{fig:results:case_study:p_HgEC:0:6} is that the implemented posterior heuristic selection distribution does not yield normalised probabilities, but rather yield unnormalised \textit{logits}, which are used to parameterise a Categorical probability distribution from which new heuristic selections are sampled. The reasons for the aforementioned is because the probabilistic model is evaluated proportionally as was discussed in Section \ref{sec:bhh}. Furthermore, the log-sum-exp trick is used in order to maintain numerical stability, yielding \textit{logits} instead of probabilities.

Another observation to make is for the bhh\_replay\_250 configuration $(\textcolor{green}{green})$. Figure \ref{fig:results:case_study:p_HgEC:0:6} show that the posterior heuristic selection probabilities converge to the expected heuristic selection probabilities later in the training stages. The aforementioned suggests that the \acs{BHH} is not able to find further performance biases and cannot exploit better solutions. At this point, the \acs{BHH} starts to explore more as the concentration parameters are reanalysed more uniformly, resolving more and more to a random search in attempt to find better solutions.

Furthermore, the posterior heuristic selection probability distribution is conditional on the occurrence of a specific entity that the potential heuristic will be applied to, as well as a specific performance criterion enforced by a specific credit assignment strategy. This means that heuristic selection is specific to each entity. A particular heuristic might be good for one entity, but not for another. This is a strong characteristic of the \acs{BHH}, as it learns to apply the correct heuristic to the correct entity at the correct time in the training process.

Finally, the posterior heuristic selection probabilities are much less volatile than their prior heuristic selection probability equivalents. This is a result of the information that is added in the performance log by tracking the entity and credit allocation as well.

It can be concluded that the \acs{BHH} is able to successfully train the underlying \acs{FFNN} for the case study on the iris dataset. Furthermore it can be concluded that the learning mechanism implemented by the \acs{BHH} is able to exploit minor performance biases, and thus the \acs{BHH} is able to correctly allocate the correct heuristic to the correct entity at the correct time in the training process.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
% BHH vs. Low-Level Heuristics
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\subsection{BHH vs. Low-Level Heuristics}\label{sec:results:standalone}

This section provides the empirical results for the experimental group that compares the performance of the \acs{BHH} to the performance of the individual, standalone, low-level heuristics. Three variants of the \acs{BHH} is included in the experiment, including the \acs{BHH} baseline configuration with a heuristic pool that contains all the low-level heuristics (denoted bhh\_all), the \acs{BHH} configuration with a heuristic pool that contains only gradient-based heuristics (denoted bhh\_gd), and finally, the \acs{BHH} configuration with a heuristics pool that contains only \acp{MH} (denoted bhh\_mh).

Tables \ref{tab:results:standalone:metrics:rank_a} and \ref{tab:results:standalone:metrics:rank_b} provides the empirical results in ranked format. The performance rank is calculated as the average rank produced by each heuristic, across all datasets, for all independent runs and all epochs. The average rank across all epochs produces a view on the performance of the heuristics as it relates to the entire training process. Finally, a normalised average rank is provided for the overall performance of all heuristics at the bottom of the table. The normalised average rank is calculated as a discrete normalisation of the average rank achieved across all datasets, for all independent runs and epochs.

Tables \ref{tab:results:standalone:metrics:rank_a} and \ref{tab:results:standalone:metrics:rank_b} shows that the bhh\_gd configuration produced the best results of the \acs{BHH} variants and managed to perform well, producing generally good results across all datasets. The bhh\_gd configuration managed to produce results that are comparable to the top three heuristics for each dataset, while the bhh\_all and bhh\_mh produced average results compared to all the heuristics.

\begin{table}[htb]
	\centering
	\caption{Empirical results showing normalised average rank and statistics for the top six low-level heuristics and three heuristic pool variants of the \acs{BHH} baseline configuration, across multiple datasets, for all independent runs and epochs.}
	\label{tab:results:standalone:metrics:rank_a}%
	\par\bigskip
	\resizebox{\textwidth}{!}{
		\begin{tabular}{r|ccc|c|c|c|}
			                              & \multicolumn{6}{c|}{\textbf{BHH vs. Low-Level Heuristics - Average Rank (Part A)}}                                                                                                                                                                                                                                                                                                   \\
			\midrule
			\textbf{dataset}              & \textbf{adagrad}                                                                   & \textbf{adam}                                           & \textbf{rmsprop}                                        & \textbf{bhh\_gd}                                        & \textbf{nag}                                            & \textbf{bhh\_all}                                       \\
			\midrule
			\textbf{abalone}              & \cellcolor[rgb]{ .388,  .745,  .482}2,2215 ($\pm$1,591)                            & \cellcolor[rgb]{ .416,  .753,  .482}2,3989 ($\pm$1,887) & \cellcolor[rgb]{ .78,  .859,  .502}4,6172 ($\pm$2,65)   & \cellcolor[rgb]{ .796,  .863,  .506}4,7032 ($\pm$2,108) & \cellcolor[rgb]{ .725,  .839,  .498}4,2731 ($\pm$1,542) & \cellcolor[rgb]{ 1,  .922,  .518}5,9376 ($\pm$2,399)    \\
			\textbf{air\_quality}         & \cellcolor[rgb]{ .427,  .757,  .482}3,6409 ($\pm$2,259)                            & \cellcolor[rgb]{ .804,  .863,  .506}5,4312 ($\pm$2,62)  & \cellcolor[rgb]{ .388,  .745,  .482}3,4452 ($\pm$2,57)  & \cellcolor[rgb]{ .729,  .843,  .502}5,0817 ($\pm$2,762) & \cellcolor[rgb]{ .467,  .765,  .486}3,8194 ($\pm$2,229) & \cellcolor[rgb]{ 1,  .894,  .514}6,686 ($\pm$3,061)     \\
			\textbf{bank}                 & \cellcolor[rgb]{ .455,  .765,  .486}2,5495 ($\pm$1,598)                            & \cellcolor[rgb]{ .388,  .745,  .482}2,0796 ($\pm$1,587) & \cellcolor[rgb]{ .588,  .8,  .49}3,4645 ($\pm$2,209)    & \cellcolor[rgb]{ .8,  .863,  .506}4,8828 ($\pm$1,702)   & \cellcolor[rgb]{ .71,  .835,  .498}4,2871 ($\pm$1,732)  & \cellcolor[rgb]{ 1,  .922,  .518}6,2419 ($\pm$2,157)    \\
			\textbf{bike}                 & \cellcolor[rgb]{ .388,  .745,  .482}1,7204 ($\pm$1,384)                            & \cellcolor[rgb]{ .643,  .816,  .494}3,6925 ($\pm$4,004) & \cellcolor[rgb]{ .973,  .914,  .514}6,2624 ($\pm$4,58)  & \cellcolor[rgb]{ .663,  .824,  .498}3,8441 ($\pm$1,398) & \cellcolor[rgb]{ 1,  .922,  .518}6,4516 ($\pm$1,02)     & \cellcolor[rgb]{ .71,  .835,  .498}4,2151 ($\pm$1,361)  \\
			\textbf{car}                  & \cellcolor[rgb]{ .702,  .835,  .498}4,7634 ($\pm$0,938)                            & \cellcolor[rgb]{ .388,  .745,  .482}1,6226 ($\pm$1,405) & \cellcolor[rgb]{ .455,  .765,  .486}2,3269 ($\pm$1,409) & \cellcolor[rgb]{ .561,  .792,  .49}3,3473 ($\pm$1,35)   & \cellcolor[rgb]{ .831,  .871,  .506}6,0785 ($\pm$0,799) & \cellcolor[rgb]{ .58,  .8,  .49}3,5624 ($\pm$1,315)     \\
			\textbf{diabetic}             & \cellcolor[rgb]{ .506,  .776,  .486}2,7796 ($\pm$1,659)                            & \cellcolor[rgb]{ 1,  .886,  .514}7,1484 ($\pm$2,227)    & \cellcolor[rgb]{ 1,  .922,  .518}6,7376 ($\pm$2,577)    & \cellcolor[rgb]{ .812,  .867,  .506}5,2269 ($\pm$2,186) & \cellcolor[rgb]{ .388,  .745,  .482}1,8118 ($\pm$1,413) & \cellcolor[rgb]{ .988,  .69,  .475}9,3968 ($\pm$3,022)  \\
			\textbf{fish\_toxicity}       & \cellcolor[rgb]{ .533,  .784,  .49}4,2645 ($\pm$2,614)                             & \cellcolor[rgb]{ .388,  .745,  .482}3,6022 ($\pm$2,445) & \cellcolor[rgb]{ .388,  .745,  .482}3,5946 ($\pm$2,329) & \cellcolor[rgb]{ .784,  .859,  .502}5,4118 ($\pm$2,665) & \cellcolor[rgb]{ .89,  .89,  .51}5,8914 ($\pm$2,629)    & \cellcolor[rgb]{ .875,  .886,  .51}5,829 ($\pm$2,856)   \\
			\textbf{forest\_fires}        & \cellcolor[rgb]{ .769,  .855,  .502}5,1559 ($\pm$2,922)                            & \cellcolor[rgb]{ .388,  .745,  .482}4,2688 ($\pm$2,984) & \cellcolor[rgb]{ .718,  .839,  .498}5,0355 ($\pm$3,143) & \cellcolor[rgb]{ .569,  .796,  .49}4,6935 ($\pm$2,759)  & \cellcolor[rgb]{ 1,  .922,  .518}5,6882 ($\pm$2,215)    & \cellcolor[rgb]{ .91,  .894,  .51}5,4839 ($\pm$3,107)   \\
			\textbf{housing}              & \cellcolor[rgb]{ .404,  .749,  .482}3,4484 ($\pm$2,025)                            & \cellcolor[rgb]{ .388,  .745,  .482}3,3344 ($\pm$1,819) & \cellcolor[rgb]{ .439,  .757,  .482}3,6946 ($\pm$2,166) & \cellcolor[rgb]{ .553,  .792,  .49}4,4742 ($\pm$2,312)  & \cellcolor[rgb]{ .584,  .8,  .49}4,6839 ($\pm$2,658)    & \cellcolor[rgb]{ .537,  .788,  .49}4,3763 ($\pm$2,438)  \\
			\textbf{iris}                 & \cellcolor[rgb]{ .965,  .91,  .514}6,3946 ($\pm$1,6)                               & \cellcolor[rgb]{ .525,  .784,  .49}3,5839 ($\pm$2,511)  & \cellcolor[rgb]{ .388,  .745,  .482}2,6968 ($\pm$1,912) & \cellcolor[rgb]{ .706,  .835,  .498}4,7473 ($\pm$2,275) & \cellcolor[rgb]{ .522,  .78,  .486}3,5548 ($\pm$2,125)  & \cellcolor[rgb]{ .78,  .859,  .502}5,2204 ($\pm$3,041)  \\
			\textbf{mushroom}             & \cellcolor[rgb]{ .71,  .835,  .498}4,4656 ($\pm$1,053)                             & \cellcolor[rgb]{ .388,  .745,  .482}2,1344 ($\pm$1,883) & \cellcolor[rgb]{ .431,  .757,  .482}2,4656 ($\pm$1,359) & \cellcolor[rgb]{ .569,  .796,  .49}3,4484 ($\pm$1,602)  & \cellcolor[rgb]{ .969,  .91,  .514}6,3323 ($\pm$0,891)  & \cellcolor[rgb]{ .6,  .804,  .494}3,6688 ($\pm$2,469)   \\
			\textbf{parkinsons}           & \cellcolor[rgb]{ .412,  .749,  .482}2,4677 ($\pm$1,497)                            & \cellcolor[rgb]{ .388,  .745,  .482}2,2333 ($\pm$1,742) & \cellcolor[rgb]{ .541,  .788,  .49}3,5656 ($\pm$2,492)  & \cellcolor[rgb]{ .655,  .82,  .494}4,572 ($\pm$1,934)   & \cellcolor[rgb]{ 1,  .922,  .518}7,5355 ($\pm$1,44)     & \cellcolor[rgb]{ .635,  .816,  .494}4,3839 ($\pm$1,861) \\
			\textbf{student\_performance} & \cellcolor[rgb]{ .388,  .745,  .482}2,5634 ($\pm$1,912)                            & \cellcolor[rgb]{ .98,  .506,  .439}11,3978 ($\pm$2,178) & \cellcolor[rgb]{ .973,  .412,  .42}12,4312 ($\pm$1,34)  & \cellcolor[rgb]{ .843,  .875,  .506}5,6624 ($\pm$3,57)  & \cellcolor[rgb]{ .478,  .769,  .486}3,1935 ($\pm$2,12)  & \cellcolor[rgb]{ .875,  .882,  .51}5,8634 ($\pm$3,159)  \\
			\textbf{wine\_quality}        & \cellcolor[rgb]{ .569,  .796,  .49}3,2806 ($\pm$1,931)                             & \cellcolor[rgb]{ .388,  .745,  .482}2,1118 ($\pm$1,666) & \cellcolor[rgb]{ .624,  .812,  .494}3,6301 ($\pm$1,731) & \cellcolor[rgb]{ .808,  .863,  .506}4,7882 ($\pm$2,105) & \cellcolor[rgb]{ .706,  .835,  .498}4,1505 ($\pm$1,916) & \cellcolor[rgb]{ .871,  .882,  .51}5,1925 ($\pm$1,951)  \\
			\midrule
			\textbf{avg rank}             & \cellcolor[rgb]{ .388,  .745,  .482}3,5512 ($\pm$2,25)                             & \cellcolor[rgb]{ .471,  .769,  .486}3,9314 ($\pm$3,423) & \cellcolor[rgb]{ .616,  .808,  .494}4,5691 ($\pm$3,517) & \cellcolor[rgb]{ .631,  .812,  .494}4,6346 ($\pm$2,364) & \cellcolor[rgb]{ .675,  .827,  .498}4,8394 ($\pm$2,384) & \cellcolor[rgb]{ .808,  .867,  .506}5,4327 ($\pm$2,9)   \\
			\midrule
			\textbf{normalised avg rank}  & \cellcolor[rgb]{ .388,  .745,  .482}1                                              & \cellcolor[rgb]{ .49,  .773,  .486}2                    & \cellcolor[rgb]{ .592,  .804,  .494}3                   & \cellcolor[rgb]{ .694,  .831,  .498}4                   & \cellcolor[rgb]{ .796,  .863,  .506}5                   & \cellcolor[rgb]{ .898,  .89,  .51}6                     \\
			\cmidrule{5-5}\cmidrule{7-7}\end{tabular}%
	}
\end{table}%


\begin{table}[htb]
	\centering
	\caption{Empirical results showing normalised average rank and statistics for the bottom six low-level heuristics and three heuristic pool variants of the \acs{BHH} baseline configuration, across multiple datasets, for all independent runs and epochs.}
	\label{tab:results:standalone:metrics:rank_b}%
	\par\bigskip
	\resizebox{\textwidth}{!}{
		\begin{tabular}{r|c|c|ccccc|}
			                              & \multicolumn{7}{c}{\textbf{BHH vs. Low-Level Heuristics - Average Rank (Part B)}}                                                                                                                                                                                                                                                                                                                                                                  \\
			\midrule
			\textbf{dataset}              & \textbf{adadelta}                                                                 & \textbf{bhh\_mh}                                        & \textbf{ga}                                              & \textbf{pso}                                             & \textbf{sgd}                                             & \textbf{momentum}                                        & \textbf{de}                                              \\
			\midrule
			\textbf{abalone}              & \cellcolor[rgb]{ .894,  .89,  .51}5,3129 ($\pm$1,478)                             & \cellcolor[rgb]{ .992,  .749,  .486}8,1882 ($\pm$1,195) & \cellcolor[rgb]{ .98,  .525,  .443}11,1108 ($\pm$1,102)  & \cellcolor[rgb]{ .98,  .514,  .439}11,2559 ($\pm$1,826)  & \cellcolor[rgb]{ .992,  .718,  .478}8,628 ($\pm$1,019)   & \cellcolor[rgb]{ .984,  .624,  .463}9,8151 ($\pm$1,16)   & \cellcolor[rgb]{ .973,  .412,  .42}12,5376 ($\pm$1,329)  \\
			\textbf{air\_quality}         & \cellcolor[rgb]{ .765,  .851,  .502}5,2441 ($\pm$3,162)                           & \cellcolor[rgb]{ 1,  .922,  .518}6,357 ($\pm$2,303)     & \cellcolor[rgb]{ .996,  .784,  .494}7,8204 ($\pm$2,265)  & \cellcolor[rgb]{ .984,  .588,  .455}9,9151 ($\pm$2,288)  & \cellcolor[rgb]{ .98,  .518,  .443}10,6613 ($\pm$1,606)  & \cellcolor[rgb]{ .973,  .412,  .42}11,7559 ($\pm$1,473)  & \cellcolor[rgb]{ .976,  .471,  .431}11,1419 ($\pm$2,236) \\
			\textbf{bank}                 & \cellcolor[rgb]{ .914,  .894,  .51}5,672 ($\pm$1,241)                             & \cellcolor[rgb]{ .988,  .639,  .463}9,7495 ($\pm$1,048) & \cellcolor[rgb]{ .98,  .537,  .447}10,9817 ($\pm$1,216)  & \cellcolor[rgb]{ .976,  .467,  .431}11,8376 ($\pm$1,464) & \cellcolor[rgb]{ .992,  .757,  .486}8,2774 ($\pm$1,03)   & \cellcolor[rgb]{ .992,  .741,  .486}8,4774 ($\pm$1,068)  & \cellcolor[rgb]{ .973,  .412,  .42}12,4989 ($\pm$1,224)  \\
			\textbf{bike}                 & \cellcolor[rgb]{ .859,  .878,  .506}5,3602 ($\pm$1,155)                           & \cellcolor[rgb]{ .996,  .843,  .506}7,4108 ($\pm$1,008) & \cellcolor[rgb]{ .988,  .686,  .475}9,2269 ($\pm$1,183)  & \cellcolor[rgb]{ .988,  .682,  .475}9,3086 ($\pm$1,761)  & \cellcolor[rgb]{ .984,  .596,  .455}10,3355 ($\pm$1,419) & \cellcolor[rgb]{ .984,  .561,  .451}10,7086 ($\pm$1,423) & \cellcolor[rgb]{ .973,  .412,  .42}12,4634 ($\pm$1,465)  \\
			\textbf{car}                  & \cellcolor[rgb]{ 1,  .922,  .518}7,7344 ($\pm$1,746)                              & \cellcolor[rgb]{ .996,  .8,  .498}8,8505 ($\pm$1,413)   & \cellcolor[rgb]{ .984,  .592,  .455}10,7763 ($\pm$1,471) & \cellcolor[rgb]{ 1,  .855,  .506}8,3613 ($\pm$1,622)     & \cellcolor[rgb]{ .988,  .651,  .467}10,2452 ($\pm$1,447) & \cellcolor[rgb]{ .984,  .576,  .451}10,9226 ($\pm$1,349) & \cellcolor[rgb]{ .973,  .412,  .42}12,4086 ($\pm$1,492)  \\
			\textbf{diabetic}             & \cellcolor[rgb]{ .494,  .773,  .486}2,6753 ($\pm$1,629)                           & \cellcolor[rgb]{ .992,  .765,  .49}8,557 ($\pm$1,17)    & \cellcolor[rgb]{ .98,  .529,  .443}11,2011 ($\pm$1,413)  & \cellcolor[rgb]{ .984,  .573,  .451}10,7022 ($\pm$1,067) & \cellcolor[rgb]{ .898,  .89,  .51}5,9215 ($\pm$1,542)    & \cellcolor[rgb]{ .949,  .906,  .514}6,3355 ($\pm$1,612)  & \cellcolor[rgb]{ .973,  .412,  .42}12,5065 ($\pm$1,242)  \\
			\textbf{fish\_toxicity}       & \cellcolor[rgb]{ .996,  .788,  .494}7,914 ($\pm$3,429)                            & \cellcolor[rgb]{ 1,  .922,  .518}6,3849 ($\pm$2,944)    & \cellcolor[rgb]{ 1,  .894,  .514}6,7043 ($\pm$2,82)      & \cellcolor[rgb]{ .996,  .82,  .498}7,5731 ($\pm$2,982)   & \cellcolor[rgb]{ .976,  .471,  .431}11,5785 ($\pm$1,459) & \cellcolor[rgb]{ .973,  .412,  .42}12,2301 ($\pm$1,382)  & \cellcolor[rgb]{ .984,  .608,  .459}10,0215 ($\pm$2,358) \\
			\textbf{forest\_fires}        & \cellcolor[rgb]{ 1,  .859,  .506}6,5161 ($\pm$3,082)                              & \cellcolor[rgb]{ .898,  .89,  .51}5,4591 ($\pm$2,668)   & \cellcolor[rgb]{ .996,  .796,  .494}7,3667 ($\pm$2,37)   & \cellcolor[rgb]{ 1,  .863,  .51}6,4796 ($\pm$3,354)      & \cellcolor[rgb]{ .98,  .529,  .443}10,8129 ($\pm$1,207)  & \cellcolor[rgb]{ .976,  .463,  .431}11,7065 ($\pm$1,325) & \cellcolor[rgb]{ .973,  .412,  .42}12,3333 ($\pm$1,923)  \\
			\textbf{housing}              & \cellcolor[rgb]{ 1,  .918,  .518}7,5903 ($\pm$2,748)                              & \cellcolor[rgb]{ 1,  .922,  .518}7,5441 ($\pm$1,736)    & \cellcolor[rgb]{ 1,  .878,  .51}7,8839 ($\pm$2,099)      & \cellcolor[rgb]{ .984,  .608,  .459}9,9409 ($\pm$2,317)  & \cellcolor[rgb]{ .973,  .412,  .42}11,4075 ($\pm$1,528)  & \cellcolor[rgb]{ .976,  .431,  .424}11,2731 ($\pm$1,506) & \cellcolor[rgb]{ .976,  .42,  .424}11,3484 ($\pm$2,096)  \\
			\textbf{iris}                 & \cellcolor[rgb]{ .973,  .412,  .42}11,3527 ($\pm$1,779)                           & \cellcolor[rgb]{ 1,  .922,  .518}6,6075 ($\pm$2,555)    & \cellcolor[rgb]{ .992,  .749,  .486}8,2473 ($\pm$1,765)  & \cellcolor[rgb]{ .992,  .745,  .486}8,2731 ($\pm$4,384)  & \cellcolor[rgb]{ .98,  .518,  .443}10,3796 ($\pm$1,294)  & \cellcolor[rgb]{ .976,  .447,  .427}11,0548 ($\pm$1,409) & \cellcolor[rgb]{ .988,  .678,  .471}8,8871 ($\pm$3,251)  \\
			\textbf{mushroom}             & \cellcolor[rgb]{ 1,  .922,  .518}6,5538 ($\pm$1,071)                              & \cellcolor[rgb]{ .992,  .718,  .478}9,0452 ($\pm$1,093) & \cellcolor[rgb]{ .98,  .51,  .439}11,5731 ($\pm$1,193)   & \cellcolor[rgb]{ .996,  .816,  .498}7,872 ($\pm$0,92)    & \cellcolor[rgb]{ .988,  .659,  .471}9,7527 ($\pm$1,083)  & \cellcolor[rgb]{ .98,  .557,  .451}10,9785 ($\pm$1,108)  & \cellcolor[rgb]{ .973,  .412,  .42}12,7097 ($\pm$1,478)  \\
			\textbf{parkinsons}           & \cellcolor[rgb]{ .875,  .882,  .51}6,472 ($\pm$2,423)                             & \cellcolor[rgb]{ .996,  .843,  .506}8,3161 ($\pm$1,644) & \cellcolor[rgb]{ 1,  .898,  .514}7,7968 ($\pm$1,719)     & \cellcolor[rgb]{ .996,  .827,  .502}8,4892 ($\pm$1,901)  & \cellcolor[rgb]{ .98,  .494,  .435}11,7516 ($\pm$1,155)  & \cellcolor[rgb]{ .973,  .412,  .42}12,5419 ($\pm$1,351)  & \cellcolor[rgb]{ .984,  .584,  .455}10,8742 ($\pm$1,317) \\
			\textbf{student\_performance} & \cellcolor[rgb]{ .514,  .78,  .486}3,4194 ($\pm$2,006)                            & \cellcolor[rgb]{ 1,  .902,  .514}6,9333 ($\pm$2,44)     & \cellcolor[rgb]{ 1,  .886,  .514}7,1032 ($\pm$1,989)     & \cellcolor[rgb]{ .98,  .537,  .443}11,0624 ($\pm$1,067)  & \cellcolor[rgb]{ .988,  .918,  .514}6,6366 ($\pm$2,023)  & \cellcolor[rgb]{ 1,  .922,  .518}6,7011 ($\pm$2,242)     & \cellcolor[rgb]{ .996,  .804,  .498}8,0323 ($\pm$1,935)  \\
			\textbf{wine\_quality}        & \cellcolor[rgb]{ 1,  .922,  .518}6,0011 ($\pm$2,404)                              & \cellcolor[rgb]{ .988,  .647,  .467}9,5935 ($\pm$1,494) & \cellcolor[rgb]{ .984,  .588,  .455}10,3387 ($\pm$1,62)  & \cellcolor[rgb]{ .98,  .525,  .443}11,1602 ($\pm$1,773)  & \cellcolor[rgb]{ .992,  .722,  .482}8,6344 ($\pm$1,18)   & \cellcolor[rgb]{ .988,  .651,  .467}9,5269 ($\pm$1,341)  & \cellcolor[rgb]{ .973,  .412,  .42}12,5903 ($\pm$1,352)  \\
			\midrule
			\textbf{avg rank}             & \cellcolor[rgb]{ 1,  .922,  .518}6,2727 ($\pm$3,004)                              & \cellcolor[rgb]{ .992,  .776,  .49}7,7855 ($\pm$2,271)  & \cellcolor[rgb]{ .988,  .639,  .467}9,1522 ($\pm$2,48)   & \cellcolor[rgb]{ .984,  .612,  .459}9,4451 ($\pm$2,75)   & \cellcolor[rgb]{ .984,  .592,  .455}9,6445 ($\pm$2,214)  & \cellcolor[rgb]{ .98,  .529,  .443}10,2877 ($\pm$2,346)  & \cellcolor[rgb]{ .973,  .412,  .42}11,4538 ($\pm$2,354)  \\
			\midrule
			\textbf{normalised avg rank}  & \cellcolor[rgb]{ 1,  .922,  .518}7                                                & \cellcolor[rgb]{ .996,  .839,  .502}8                   & \cellcolor[rgb]{ .992,  .753,  .486}9                    & \cellcolor[rgb]{ .988,  .667,  .471}10                   & \cellcolor[rgb]{ .984,  .584,  .455}11                   & \cellcolor[rgb]{ .98,  .498,  .439}12                    & \cellcolor[rgb]{ .973,  .412,  .42}13                    \\
			\cmidrule{3-3}\end{tabular}%
	}
\end{table}%

From the normalised average ranks provided in Tables \ref{tab:results:standalone:metrics:rank_a} and \ref{tab:results:standalone:metrics:rank_b}, it can be seen that the bhh\_gd configuration ranked fourth, while the bhh\_all and bhh\_mh configurations ranked sixth and eighth amongst all thirteen heuristic implementations respectively. These results show that the \acs{BHH} generally performs well, but is not able to outperform the best heuristic for each dataset.

Figure \ref{fig:results:standalone:descriptive:descriptive} provides an illustration showing a descriptive plot of the average ranks achieved over all independent runs, for each heuristic, per dataset. The heuristics are ordered according to the normalised ranks presented in Tables \ref{tab:results:standalone:metrics:rank_a} and \ref{tab:results:standalone:metrics:rank_b}.

\begin{figure}[htb]
	\centering
	\includegraphics[width=\textwidth]{standalone/figures/descriptive/descriptive.pdf}
	\caption{Descriptive plots for the average ranks of all low-level heuristics compared to three heuristic pool variants of the \acs{BHH} baseline configuration, per dataset, across all independent runs and epochs.}
	\label{fig:results:standalone:descriptive:descriptive}
\end{figure}

From Figure \ref{fig:results:standalone:descriptive:descriptive} it can be seen that the bhh\_gd heuristic achieved the lowest variance in average rank across all datasets, compared to the other heuristics. The aforementioned shows the generalisation capabilities of the \acs{BHH} to multiple problems.

Figure \ref{fig:results:standalone:descriptive:cd} provides an illustration of the overall critical difference plots that illustrate the statistically significant differences in ranked performance for each heuristic as it relates to all datasets, across all independent runs and epochs.

\begin{figure}[htb]
	\centering
	\includegraphics[width=\textwidth]{standalone/figures/cd/overall.pdf}
	\caption{Critical difference plots for the average ranks of all low-level heuristics compared to three heuristic pool variants of the baseline \acs{BHH}, across all datasets, runs and epochs.}
	\label{fig:results:standalone:descriptive:cd}
\end{figure}

Although the outcomes of the bhh\_all and bhh\_mh configurations seem to produce average performance results, it should be noted that the performance difference between all heuristics is very small. Furthermore, the best configuration of the \acs{BHH}, namely the bhh\_gd configuration, is statistically only outperformed overall by \acs{Adagrad} and \acs{Adam}, yielding statistically comparable results to \acs{RMSProp} and \acs{NAG}. It should be noted that the standalone low-level heuristics already produce good results in general across all datasets. In this particular case, producing better performance outcomes can be hard to achieve. However, as mentioned previously, the \acs{BHH} provides a generalisation capability across all datasets that is advantageous to the \acs{BHH}.

Another observation that can be made is that the gradient-based heuristics generally performed much better than the \acp{MH} on all datasets. State of the art methods for training \acp{FFNN}, such as \acs{Adam}, utilise gradient-based approaches that have been proven to work well on many occasions \cite{ref:kingma:2014}. Exploration of the heuristic space leads the \acs{BHH} to consider other heuristics during the training process, which could possibly result in worse performances at times. A suggestion to improve on these results is to include a move-acceptance strategy where heuristic progressions are discarded if they fail to produce better results.