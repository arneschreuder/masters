\section{Artifical Neural Networks}
\label{sec:anns}

\acp{ANN} can generally be described as well-organised structures of mathematical computation and are inspired by the biological brain \cite{ref:engelbrecht:2007}. \acp{ANN} have been successfully applied to a range of problem classes. \citeauthor{ref:engelbrecht:2007} \cite{ref:engelbrecht:2007} summarises some common problems that are solved using \acp{ANN}. These include classification \cite{ref:khan:2001}, pattern matching \cite{ref:cannady:1998, ref:kumar:1994}, pattern completion \cite{ref:dayhoff:2001}, optimisation \cite{ref:specht:1991} and data mining \cite{ref:singh:2009}. \acfp{AN} provide the fundamental building blocks for \acp{ANN}.

The \acs{AN} implements a non-linear mapping from some $I$-dimensional, real-numbered input space to some $T$-dimensional, real-numbered output space. The output space is usually in the ranges $[0,1]$ or $[-1,1]$, depending on the problem being solved \cite{ref:engelbrecht:2007}. In essence, the \acs{AN} implements a weighted sum of its inputs and applies some non-linear transformation as a decision making boundary.

Multiple \acp{AN} can be organised and used together forming a ``network'' of \acp{AN}, referred to as an \acs{ANN}. The architecture of the \acs{ANN} refers to the way in which \acp{AN} are organised. \acp{ANN} can be organised in layers where a single layer can contain multiple \acp{AN}. Output from one layer is propagated as input to the next layer. The topology of the \acs{ANN} refers to the way that layers of \acp{AN} are connected to each other. There are many different topologies \cite{ref:miikkulainen:2010}.

This research focuses on a particular type of \acs{ANN}, referred to as \acfp{FFNN}. \acp{FFNN} were the first and simplest type of \acp{ANN} developed \cite{ref:schmidhuber:2015} and implement an architecture consisting of input, hidden and output layers by arranging them in sequential order. Furthermore, \acp{FFNN} implement fully connected topologies, where each \acs{AN} in one layer is connected to all the \acp{AN} in the next, without any cycles \cite{ref:zell:1994}. In \acp{FFNN}, information moves forward, in one direction, from the input nodes, through the hidden nodes and finally to the output nodes.

\textit{Training} is the process whereby the weights of the \acs{FFNN} are systematically changed with the aim of improving the \textit{performance} of the \acs{FFNN}. Finding the optimal weights that produce the best performance on a given task is an optimisation problem. The optimisation algorithm used to find the optimal weights is referred to as a \textit{heuristic}. Heuristics search for possible solutions in the solution-space and make use of information from the search space to guide to process.

During the training process, the \acs{FFNN} is exposed to data while trying to produce some target outcome. The degree to which the produced outcome differs from the target outcome is referred to as \textit{loss}. Since training of \acp{FFNN} is an optimisation problem, the goal of the training process is to minimise the loss. The loss is calculated using an error function.

Supervised learning is the process of training where the data that is presented to the \acs{FFNN} during training, includes the desired solution \cite{ref:geron:2017}. The \acs{FFNN} learns the mapping function from the input to the target output \cite{ref:brownlee:2016}.
