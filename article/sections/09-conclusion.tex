\section{Conclusion}
\label{sec:conclusion}

The main area of focus for the research done in this dissertation stems from the problem statement, which identified the difficult and tedious process of selecting the best heuristic for training \acp{FFNN}. The research presented in this article identified the possibility of using a different approach, referred to as \acp{HH}, to automate the heuristic selection process.

This research set out to develop a novel high-level heuristic that utilises probability theory in an online learning setting to drive the automatic heuristic selection process. A literature study was conducted on all related topics. A novel \acs{BHH} was proposed in detail. An empirical process was designed and the detail of the implementation for the empirical process and the \acs{BHH} was provided. The experiments defined for the empirical study was executed and results were statistically analysed.

For the behavioural case study of the \acs{BHH} it was found that the \ac{BHH} is able to train a \acs{FFNN} relatively well. It was shown that the \ac{BHH} is able to learn and that the \acs{BHH} is able to exploit small performance biases as it relates to heuristic selection. This enables the \acs{BHH} to select the correct heuristics to apply to the correct entities at the correct time in the training process.

For the experimental group that compares the \acs{BHH} baseline with a number of low-level heuristics, it was found that the \textit{bhh\_gd} configuration performed the best out of the \acs{BHH} variants, achieving an overall rank of fourth amongst thirteen heuristics that were implemented and executed on fourteen datasets. The \textit{bhh\_gd} configuration produced performance results close to that of the best low-level heuristics and was only statistically outperformed by the top two low-level heuristics. The \textit{bhh\_all} configuration achieved an overall rank of sixth and the \textit{bhh\_mh} achieved an overall rank of eighth.

Although the \textit{bhh\_gd} configuration produced performance results comparable to the best low-level heuristics, the \textit{bhh\_all} and \textit{bhh\_mh} configurations produced average results. It was found that, in general, gradient-based heuristics produced the best results, as such, it is understandable that the \textit{bhh\_gd} yielded the best performance outcomes between the different \acs{BHH} variants that were implemented. Although the \acs{BHH} variants were not able to produce better results than the top low-level heuristics, the \acs{BHH} variants still effectively trained the underlying \acp{FFNN} and produced good training outcomes overall. It was shown that the \textit{bhh\_gd} configuration produced the lowest variance in rank between datasets out of all of the heuristics implemented, giving the \acs{BHH} the ability to generalise well to other problems.

Finally, it was shown that the \acs{BHH} provides a mechanism whereby prior expert knowledge can be injected, before training starts. Future research can exploit this knowledge and provide a significant bias towards heuristics that are known to perform well on particular problem types.
